{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTy2Nko7FhCtQD0eI1aQhW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuratulAin20/Deep-Learning/blob/main/LSTM%20text%20generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "-1CoeklCnlo9",
        "outputId": "08ef3187-c49c-4061-9ffc-d08ccd740dbe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-724c6f0f-aa51-407c-9df5-9224b7981120\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-724c6f0f-aa51-407c-9df5-9224b7981120\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Python Text Processing with NLTK 2.0 Cookbook.pdf to Python Text Processing with NLTK 2.0 Cookbook.pdf\n",
            "Uploaded file: Python Text Processing with NLTK 2.0 Cookbook.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the book file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# If you want to check the names of uploaded files\n",
        "for filename in uploaded.keys():\n",
        "    print(f'Uploaded file: {filename}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjetmOjrpml6",
        "outputId": "ffb974ac-0894-41ca-de83-d4a8623d4284"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.11-cp38-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading PyMuPDF-1.24.11-cp38-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "pdf_file_name = 'Python Text Processing with NLTK 2.0 Cookbook.pdf'\n",
        "\n",
        "# Open the PDF file\n",
        "document = fitz.open(pdf_file_name)\n",
        "\n",
        "# Extract text from each page\n",
        "text = \"\"\n",
        "for page in document:\n",
        "    text += page.get_text()\n",
        "\n",
        "# Close the document\n",
        "document.close()\n",
        "\n",
        "# Display the first 500 characters of the book\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gomPiLtRpcRY",
        "outputId": "6055300b-7af2-4460-9e66-a3f19c5a39fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "www.allitebooks.com\n",
            "Python Text \n",
            "Processing with  \n",
            "NLTK 2.0 Cookbook\n",
            "Over 80 practical recipes for using Python's NLTK suite of \n",
            "libraries to maximize your Natural Language Processing \n",
            "capabilities.\n",
            "Jacob Perkins\n",
            "  BIRMINGHAM - MUMBAI\n",
            "www.allitebooks.com\n",
            "Python Text Processing with NLTK 2.0  \n",
            "Cookbook\n",
            "Copyright © 2010 Packt Publishing\n",
            "All rights reserved. No part of this book may be reproduced, stored in a retrieval system, \n",
            "or transmitted in any form or by any means, without the prior written p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the extracted text to a text file\n",
        "with open('extracted_book.txt', 'w', encoding='utf-8') as text_file:\n",
        "    text_file.write(text)\n",
        "\n",
        "print(\"Text has been saved to 'extracted_book.txt'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcOA40H3p_bD",
        "outputId": "3302df66-5ac9-48a8-c289-a0cdc12065f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text has been saved to 'extracted_book.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "zqLi1Lzo9c5G",
        "outputId": "d7f36bb2-daeb-4114-dfa2-2c35e14fe692"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'www.allitebooks.com\\nPython Text \\nProcessing with  \\nNLTK 2.0 Cookbook\\nOver 80 practical recipes for using Python\\'s NLTK suite of \\nlibraries to maximize your Natural Language Processing \\ncapabilities.\\nJacob Perkins\\n  BIRMINGHAM - MUMBAI\\nwww.allitebooks.com\\nPython Text Processing with NLTK 2.0  \\nCookbook\\nCopyright © 2010 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, \\nor transmitted in any form or by any means, without the prior written permission of the \\npublisher, except in the case of brief quotations embedded in critical articles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of the \\ninformation presented. However, the information contained in this book is sold without \\nwarranty, either express or implied. Neither the author, nor Packt Publishing, and its \\ndealers and distributors will be held liable for any damages caused or alleged to be \\ncaused directly or indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the \\ncompanies and products mentioned in this book by the appropriate use of capitals. \\nHowever, Packt Publishing cannot guarantee the accuracy of this information.\\nFirst published: November 2010\\nProduction Reference: 1031110\\nPublished by Packt Publishing Ltd. \\n32 Lincoln Road \\nOlton \\nBirmingham, B27 6PA, UK.\\nISBN 978-1-849513-60-9\\nwww.packtpub.com\\nCover Image by Sujay Gawand (sujay0000@gmail.com)\\nwww.allitebooks.com\\nCredits\\nAuthor\\nJacob Perkins\\nReviewers\\nPatrick Chan\\nHerjend Teny\\nAcquisition Editor\\nSteven Wilding\\nDevelopment Editor\\nMaitreya Bhakal\\nTechnical Editors\\nBianca Sequeira\\nAditi Suvarna\\nCopy Editor\\nLaxmi Subramanian\\nIndexer\\nTejal Daruwale\\nEditorial Team Leader\\nAditya Belpathak\\nProject Team Leader\\nPriya Mukherji\\nProject Coordinator\\nShubhanjan Chatterjee\\nProofreader\\nJoanna McMahon\\nGraphics\\nNilesh Mohite\\nProduction Coordinator \\nAdline Swetha Jesuthas\\nCover Work\\nAdline Swetha Jesuthas\\nwww.allitebooks.com\\nAbout the Author\\nJacob Perkins has been an avid user of open source software since high school, when \\nhe irst built his own computer and didn\\'t want to pay for Windows. At one point he had \\nive operating systems installed, including Red Hat Linux, OpenBSD, and BeOS.\\nWhile at Washington University in St. Louis, Jacob took classes in Spanish and poetry \\nwriting, and worked on an independent study project that eventually became his Master\\'s \\nproject: WUGLE—a GUI for manipulating logical expressions. In his free time, he wrote \\nthe Gnome2 version of Seahorse (a GUI for encryption and key management), which has \\nsince been translated into over a dozen languages and is included in the default Gnome \\ndistribution.\\nAfter receiving his MS in Computer Science, Jacob tried to start a web development \\nstudio with some friends, but since no one knew anything about web development, \\nit didn\\'t work out as planned. Once he\\'d actually learned about web development, he \\nwent off and co-founded another company called Weotta, which sparked his interest in \\nMachine Learning and Natural Language Processing.\\nJacob is currently the CTO/Chief Hacker for Weotta and blogs about what he\\'s learned \\nalong the way at http://streamhacker.com/. He is also applying this knowledge to \\nproduce text processing APIs and demos at http://text-processing.com/. This book \\nis a synthesis of his knowledge on processing text using Python, NLTK, and more.\\nThanks to my parents for all their support, even when they don\\'t understand \\nwhat I\\'m doing; Grant for sparking my interest in Natural Language \\nProcessing; Les for inspiring me to program when I had no desire to; Arnie \\nfor all the algorithm discussions; and the whole Wernick family for feeding \\nme such good food whenever I come over.\\nwww.allitebooks.com\\nAbout the Reviewers\\nPatrick Chan is an engineer/programmer in the telecommunications industry. He is an \\navid fan of Linux and Python. His less geekier pursuits include Toastmasters, music, and \\nrunning.\\nHerjend Teny graduated from the University of Melbourne. He has worked mainly in \\nthe education sector and as a part of research teams. The topics that he has worked \\non mainly involve embedded programming, signal processing, simulation, and some \\nstochastic modeling. His current interests now lie in many aspects of web programming, \\nusing Django. One of the books that he has worked on is the Python Testing: Beginner\\'s \\nGuide.\\nI\\'d like to thank Patrick Chan for his help in many aspects, and his crazy and \\nodd ideas. Also to Hattie, for her tolerance in letting me do this review until \\nlate at night. Thank you!!\\nwww.allitebooks.com\\nwww.allitebooks.com\\nTable of Contents\\nPreface \\n1\\nChapter 1: Tokenizing Text and WordNet Basics \\n7\\nIntroduction \\n7\\nTokenizing text into sentences \\n8\\nTokenizing sentences into words \\n9\\nTokenizing sentences using regular expressions \\n11\\nFiltering stopwords in a tokenized sentence \\n13\\nLooking up synsets for a word in WordNet \\n14\\nLooking up lemmas and synonyms in WordNet \\n17\\nCalculating WordNet synset similarity \\n19\\nDiscovering word collocations \\n21\\nChapter 2: Replacing and Correcting Words \\n25\\nIntroduction \\n25\\nStemming words \\n25\\nLemmatizing words with WordNet \\n28\\nTranslating text with Babelish \\n30\\nReplacing words matching regular expressions \\n32\\nRemoving repeating characters \\n34\\nSpelling correction with Enchant \\n36\\nReplacing synonyms \\n39\\nReplacing negations with antonyms \\n41\\nChapter 3: Creating Custom Corpora \\n45\\nIntroduction \\n45\\nSetting up a custom corpus \\n46\\nCreating a word list corpus \\n48\\nCreating a part-of-speech tagged word corpus \\n50\\nwww.allitebooks.com\\nii\\nTable of Contents\\nCreating a chunked phrase corpus \\n54\\nCreating a categorized text corpus \\n58\\nCreating a categorized chunk corpus reader \\n61\\nLazy corpus loading \\n68\\nCreating a custom corpus view \\n70\\nCreating a MongoDB backed corpus reader \\n74\\nCorpus editing with ile locking \\n77\\nChapter 4: Part-of-Speech Tagging \\n81\\nIntroduction \\n82\\nDefault tagging \\n82\\nTraining a unigram part-of-speech tagger \\n85\\nCombining taggers with backoff tagging \\n88\\nTraining and combining Ngram taggers \\n89\\nCreating a model of likely word tags \\n92\\nTagging with regular expressions \\n94\\nAfix tagging \\n96\\nTraining a Brill tagger \\n98\\nTraining the TnT tagger \\n100\\nUsing WordNet for tagging \\n103\\nTagging proper names \\n105\\nClassiier based tagging \\n106\\nChapter 5: Extracting Chunks \\n111\\nIntroduction \\n111\\nChunking and chinking with regular expressions \\n112\\nMerging and splitting chunks with regular expressions \\n117\\nExpanding and removing chunks with regular expressions \\n121\\nPartial parsing with regular expressions \\n123\\nTraining a tagger-based chunker \\n126\\nClassiication-based chunking \\n129\\nExtracting named entities \\n133\\nExtracting proper noun chunks \\n135\\nExtracting location chunks \\n137\\nTraining a named entity chunker \\n140\\nChapter 6: Transforming Chunks and Trees \\n143\\nIntroduction \\n143\\nFiltering insigniicant words \\n144\\nCorrecting verb forms \\n146\\nSwapping verb phrases \\n149\\nSwapping noun cardinals \\n150\\nSwapping ininitive phrases \\n151\\nwww.allitebooks.com\\niii\\nTable of Contents\\nSingularizing plural nouns \\n153\\nChaining chunk transformations \\n154\\nConverting a chunk tree to text \\n155\\nFlattening a deep tree \\n157\\nCreating a shallow tree \\n161\\nConverting tree nodes \\n163\\nChapter 7: Text Classiication \\n167\\nIntroduction \\n167\\nBag of Words feature extraction \\n168\\nTraining a naive Bayes classiier \\n170\\nTraining a decision tree classiier \\n177\\nTraining a maximum entropy classiier \\n180\\nMeasuring precision and recall of a classiier \\n183\\nCalculating high information words \\n187\\nCombining classiiers with voting \\n191\\nClassifying with multiple binary classiiers \\n193\\nChapter 8: Distributed Processing and Handling Large Datasets \\n201\\nIntroduction \\n202\\nDistributed tagging with execnet \\n202\\nDistributed chunking with execnet \\n206\\nParallel list processing with execnet \\n209\\nStoring a frequency distribution in Redis \\n211\\nStoring a conditional frequency distribution in Redis \\n215\\nStoring an ordered dictionary in Redis \\n218\\nDistributed word scoring with Redis and execnet \\n221\\nChapter 9: Parsing Speciic Data \\n227\\nIntroduction \\n227\\nParsing dates and times with Dateutil \\n228\\nTime zone lookup and conversion \\n230\\nTagging temporal expressions with Timex \\n233\\nExtracting URLs from HTML with lxml \\n234\\nCleaning and stripping HTML \\n236\\nConverting HTML entities with BeautifulSoup \\n238\\nDetecting and converting character encodings \\n240\\nAppendix: Penn Treebank Part-of-Speech Tags \\n243\\nIndex \\n247\\nwww.allitebooks.com\\nPreface\\nNatural Language Processing is used everywhere—in search engines, spell checkers, mobile \\nphones, computer games, and even in your washing machine. Python\\'s Natural Language \\nToolkit (NLTK) suite of libraries has rapidly emerged as one of the most eficient tools for \\nNatural Language Processing. You want to employ nothing less than the best techniques in \\nNatural Language Processing—and this book is your answer.\\nPython Text Processing with NLTK 2.0 Cookbook is your handy and illustrative guide, which \\nwill walk you through all the Natural Language Processing techniques in a step-by-step \\nmanner. It will demystify the advanced features of text analysis and text mining using the \\ncomprehensive NLTK suite.\\nThis book cuts short the preamble and lets you dive right into the science of text processing \\nwith a practical hands-on approach.\\nGet started off with learning tokenization of text. Receive an overview of WordNet and how \\nto use it. Learn the basics as well as advanced features of stemming and lemmatization. \\nDiscover various ways to replace words with simpler and more common (read: more searched) \\nvariants. Create your own corpora and learn to create custom corpus readers for data stored \\nin MongoDB. Use and manipulate POS taggers. Transform and normalize parsed chunks to \\nproduce a canonical form without changing their meaning. Dig into feature extraction and text \\nclassiication. Learn how to easily handle huge amounts of data without any loss in eficiency \\nor speed.\\nThis book will teach you all that and beyond, in a hands-on learn-by-doing manner. Make \\nyourself an expert in using the NLTK for Natural Language Processing with this handy \\ncompanion.\\nPreface\\n2\\nWhat this book covers\\nChapter 1, Tokenizing Text and WordNet Basics, covers the basics of tokenizing text \\nand using WordNet.\\nChapter 2, Replacing and Correcting Words, discusses various word replacement and \\ncorrection techniques. The recipes cover the gamut of linguistic compression, spelling \\ncorrection, and text normalization.\\nChapter 3, Creating Custom Corpora, covers how to use corpus readers and create \\ncustom corpora. At the same time, it explains how to use the existing corpus data that  \\ncomes with NLTK.\\nChapter 4, Part-of-Speech Tagging, explains the process of converting a sentence, \\nin the form of a list of words, into a list of tuples. It also explains taggers, which  \\nare trainable.\\nChapter 5, Extracting Chunks, explains the process of extracting short phrases from a \\npart-of-speech tagged sentence. It uses Penn Treebank corpus for basic training and testing \\nchunk extraction, and the CoNLL 2000 corpus as it has a simpler and more lexible format  \\nthat supports multiple chunk types.\\nChapter 6, Transforming Chunks and Trees, shows you how to do various transforms on both \\nchunks and trees. The functions detailed in these recipes modify data, as opposed to learning \\nfrom it.\\nChapter 7, Text Classiication, describes a way to categorize documents or pieces of text and, \\nby examining the word usage in a piece of text, classiiers decide what class label should be \\nassigned to it.\\nChapter 8, Distributed Processing and Handling Large Datasets, discusses how to use \\nexecnet to do parallel and distributed processing with NLTK. It also explains how to use the \\nRedis data structure server/database to store frequency distributions.\\nChapter 9, Parsing Speciic Data, covers parsing speciic kinds of data, focusing primarily on \\ndates, times, and HTML.\\nAppendix, Penn Treebank Part-of-Speech Tags, lists a table of all the part-of-speech tags that \\noccur in the treebank corpus distributed with NLTK.\\nPreface\\n3\\nWhat you need for this book\\nIn the course of this book, you will need the following software utilities to try out various code \\nexamples listed:\\n• \\nNLTK\\n• \\nMongoDB\\n• \\nPyMongo\\n• \\nRedis\\n• \\nredis-py\\n• \\nexecnet\\n• \\nEnchant\\n• \\nPyEnchant\\n• \\nPyYAML\\n• \\ndateutil\\n• \\nchardet\\n• \\nBeautifulSoup\\n• \\nlxml\\n• \\nSimpleParse\\n• \\nmxBase\\n• \\nlockile\\nWho this book is for\\nThis book is for Python programmers who want to quickly get to grips with using the  \\nNLTK for Natural Language Processing. Familiarity with basic text processing concepts  \\nis required. Programmers experienced in the NLTK will ind it useful. Students of linguistics  \\nwill ind it invaluable.\\nConventions\\nIn this book, you will ind a number of styles of text that distinguish between different kinds  \\nof information. Here are some examples of these styles, and an explanation of their meaning.\\nCode words in text are shown as follows: \"Now we want to split para into sentences. First we \\nneed to import the sentence tokenization function, and then we can call it with the paragraph \\nas an argument.\"\\nPreface\\n4\\nA block of code is set as follows:\\n >>> para = \"Hello World. It\\'s good to see you. Thanks for buying this \\nbook.\"\\n >>> from nltk.tokenize import sent_tokenize\\n >>> sent_tokenize(para)\\nNew terms and important words are shown in bold.\\nWarnings or important notes appear in a box like this.\\nTips and tricks appear like this.\\nReader feedback\\nFeedback from our readers is always welcome. Let us know what you think about this  \\nbook—what you liked or may have disliked. Reader feedback is important for us to develop \\ntitles that you really get the most out of.\\nTo send us general feedback, simply send an e-mail to feedback@packtpub.com, and \\nmention the book title via the subject of your message.\\nIf there is a book that you need and would like to see us publish, please send us a note in  \\nthe SUGGEST A TITLE form on www.packtpub.com or e-mail suggest@packtpub.com.\\nIf there is a topic that you have expertise in and you are interested in either writing or \\ncontributing to a book, see our author guide on www.packtpub.com/authors.\\nCustomer support\\nNow that you are the proud owner of a Packt book, we have a number of things to help you  \\nto get the most from your purchase.\\nDownloading the example code for this book\\nYou can download the example code iles for all Packt books you have \\npurchased from your account at http://www.PacktPub.com. If you \\npurchased this book elsewhere, you can visit http://www.PacktPub.\\ncom/support and register to have the iles e-mailed directly to you.\\nPreface\\n5\\nErrata\\nAlthough we have taken every care to ensure the accuracy of our content, mistakes do \\nhappen. If you ind a mistake in one of our books—maybe a mistake in the text or the code—\\nwe would be grateful if you would report this to us. By doing so, you can save other readers \\nfrom frustration and help us improve subsequent versions of this book. If you ind any errata, \\nplease report them by visiting http://www.packtpub.com/support, selecting your book, \\nclicking on the errata submission form link, and entering the details of your errata. Once \\nyour errata are veriied, your submission will be accepted and the errata will be uploaded on \\nour website, or added to any list of existing errata, under the Errata section of that title. Any \\nexisting errata can be viewed by selecting your title from http://www.packtpub.com/\\nsupport.\\nPiracy\\nPiracy of copyright material on the Internet is an ongoing problem across all media. At Packt, \\nwe take the protection of our copyright and licenses very seriously. If you come across any \\nillegal copies of our works, in any form, on the Internet, please provide us with the location \\naddress or website name immediately so that we can pursue a remedy.\\nPlease contact us at copyright@packtpub.com with a link to the suspected \\npirated material.\\nWe appreciate your help in protecting our authors, and our ability to bring you valuable \\ncontent.\\nQuestions\\nYou can contact us at questions@packtpub.com if you are having a problem with any \\naspect of the book, and we will do our best to address it.\\n1\\nTokenizing Text and \\nWordNet Basics\\nIn this chapter, we will cover:\\n \\nf\\nTokenizing text into sentences\\n \\nf\\nTokenizing sentences into words\\n \\nf\\nTokenizing sentences using regular expressions\\n \\nf\\nFiltering stopwords in a tokenized sentence\\n \\nf\\nLooking up synsets for a word in WordNet\\n \\nf\\nLooking up lemmas and synonyms in WordNet\\n \\nf\\nCalculating WordNet synset similarity\\n \\nf\\nDiscovering word collocations\\nIntroduction\\nNLTK is the Natural Language Toolkit, a comprehensive Python library for natural language \\nprocessing and text analytics. Originally designed for teaching, it has been adopted in the \\nindustry for research and development due to its usefulness and breadth of coverage.\\nThis chapter will cover the basics of tokenizing text and using WordNet. Tokenization is a \\nmethod of breaking up a piece of text into many pieces, and is an essential irst step for \\nrecipes in later chapters.\\nTokenizing Text and WordNet Basics\\n8\\nWordNet is a dictionary designed for programmatic access by natural language processing \\nsystems. NLTK includes a WordNet corpus reader, which we will use to access and explore \\nWordNet. We\\'ll be using WordNet again in later chapters, so it\\'s important to familiarize \\nyourself with the basics irst.\\nTokenizing text into sentences\\nTokenization is the process of splitting a string into a list of pieces, or tokens. We\\'ll start by \\nsplitting a paragraph into a list of sentences.\\nGetting ready\\nInstallation instructions for NLTK are available at http://www.nltk.org/download and \\nthe latest version as of this writing is 2.0b9. NLTK requires Python 2.4 or higher, but is not \\ncompatible with Python 3.0. The recommended Python version is 2.6.\\nOnce you\\'ve installed NLTK, you\\'ll also need to install the data by following the instructions \\nat http://www.nltk.org/data. We recommend installing everything, as we\\'ll be using \\na number of corpora and pickled objects. The data is installed in a data directory, which on \\nMac and Linux/Unix is usually /usr/share/nltk_data, or on Windows is C:\\\\nltk_data. \\nMake sure that tokenizers/punkt.zip is in the data directory and has been unpacked so \\nthat there\\'s a ile at tokenizers/punkt/english.pickle.\\nFinally, to run the code examples, you\\'ll need to start a Python console. Instructions on  \\nhow to do so are available at http://www.nltk.org/getting-started. For Mac \\nwith Linux/Unix users, you can open a terminal and type python.\\nHow to do it...\\nOnce NLTK is installed and you have a Python console running, we can start by creating a \\nparagraph of text:\\n>>> para = \"Hello World. It\\'s good to see you. Thanks for buying this \\nbook.\"\\nNow we want to split para into sentences. First we need to import the sentence tokenization \\nfunction, and then we can call it with the paragraph as an argument.\\n>>> from nltk.tokenize import sent_tokenize\\n>>> sent_tokenize(para)\\n[\\'Hello World.\\', \"It\\'s good to see you.\", \\'Thanks for buying this \\nbook.\\']\\nSo now we have a list of sentences that we can use for further processing.\\nChapter 1\\n9\\nHow it works...\\nsent_tokenize uses an instance of PunktSentenceTokenizer from the nltk.\\ntokenize.punkt module. This instance has already been trained on and works well for \\nmany European languages. So it knows what punctuation and characters mark the end of a \\nsentence and the beginning of a new sentence.\\nThere\\'s more...\\nThe instance used in sent_tokenize() is actually loaded on demand from a pickle \\nile. So if you\\'re going to be tokenizing a lot of sentences, it\\'s more eficient to load the \\nPunktSentenceTokenizer once, and call its tokenize() method instead.\\n>>> import nltk.data\\n>>> tokenizer = nltk.data.load(\\'tokenizers/punkt/english.pickle\\')\\n>>> tokenizer.tokenize(para)\\n[\\'Hello World.\\', \"It\\'s good to see you.\", \\'Thanks for buying this \\nbook.\\']\\nOther languages\\nIf you want to tokenize sentences in languages other than English, you can load one of the \\nother pickle iles in tokenizers/punkt and use it just like the English sentence tokenizer. \\nHere\\'s an example for Spanish:\\n>>> spanish_tokenizer = nltk.data.load(\\'tokenizers/punkt/spanish.\\npickle\\')\\n>>> spanish_tokenizer.tokenize(\\'Hola amigo. Estoy bien.\\')\\nSee also\\nIn the next recipe, we\\'ll learn how to split sentences into individual words. After that, we\\'ll \\ncover how to use regular expressions for tokenizing text.\\nTokenizing sentences into words\\nIn this recipe, we\\'ll split a sentence into individual words. The simple task of creating a list of \\nwords from a string is an essential part of all text processing.\\nwww.allitebooks.com\\nTokenizing Text and WordNet Basics\\n10\\nHow to do it...\\nBasic word tokenization is very simple: use the word_tokenize() function:\\n>>> from nltk.tokenize import word_tokenize\\n>>> word_tokenize(\\'Hello World.\\')\\n[\\'Hello\\', \\'World\\', \\'.\\']\\nHow it works...\\nword_tokenize() is a wrapper function that calls tokenize() on an instance of the \\nTreebankWordTokenizer. It\\'s equivalent to the following:\\n>>> from nltk.tokenize import TreebankWordTokenizer\\n>>> tokenizer = TreebankWordTokenizer()\\n>>> tokenizer.tokenize(\\'Hello World.\\')\\n[\\'Hello\\', \\'World\\', \\'.\\']\\nIt works by separating words using spaces and punctuation. And as you can see, it does not \\ndiscard the punctuation, allowing you to decide what to do with it.\\nThere\\'s more...\\nIgnoring the obviously named WhitespaceTokenizer and SpaceTokenizer, there are two \\nother word tokenizers worth looking at: PunktWordTokenizer and WordPunctTokenizer. \\nThese differ from the TreebankWordTokenizer by how they handle punctuation and \\ncontractions, but they all inherit from TokenizerI. The inheritance tree looks like this:\\nChapter 1\\n11\\nContractions\\nTreebankWordTokenizer uses conventions found in the Penn Treebank corpus, which we\\'ll \\nbe using for training in Chapter 4, Part-of-Speech Tagging and Chapter 5, Extracting Chunks. \\nOne of these conventions is to separate contractions. For example:\\n>>> word_tokenize(\"can\\'t\")\\n[\\'ca\\', \"n\\'t\"]\\nIf you ind this convention unacceptable, then read on for alternatives, and see the next recipe \\nfor tokenizing with regular expressions.\\nPunktWordTokenizer\\nAn alternative word tokenizer is the PunktWordTokenizer. It splits on punctuation, but \\nkeeps it with the word instead of creating separate tokens.\\n>>> from nltk.tokenize import PunktWordTokenizer\\n>>> tokenizer = PunktWordTokenizer()\\n>>> tokenizer.tokenize(\"Can\\'t is a contraction.\")\\n[\\'Can\\', \"\\'t\", \\'is\\', \\'a\\', \\'contraction.\\']\\nWordPunctTokenizer\\nAnother alternative word tokenizer is WordPunctTokenizer. It splits all punctuations into \\nseparate tokens.\\n>>> from nltk.tokenize import WordPunctTokenizer\\n>>> tokenizer = WordPunctTokenizer()\\n>>> tokenizer.tokenize(\"Can\\'t is a contraction.\")\\n[\\'Can\\', \"\\'\", \\'t\\', \\'is\\', \\'a\\', \\'contraction\\', \\'.\\']\\nSee also\\nFor more control over word tokenization, you\\'ll want to read the next recipe to learn how to use \\nregular expressions and the RegexpTokenizer for tokenization.\\nTokenizing sentences using regular \\nexpressions\\nRegular expression can be used if you want complete control over how to tokenize text. As \\nregular expressions can get complicated very quickly, we only recommend using them if the \\nword tokenizers covered in the previous recipe are unacceptable.\\nTokenizing Text and WordNet Basics\\n12\\nGetting ready\\nFirst you need to decide how you want to tokenize a piece of text, as this will determine how \\nyou construct your regular expression. The choices are:\\n \\nf\\nMatch on the tokens\\n \\nf\\nMatch on the separators, or gaps\\nWe\\'ll start with an example of the irst, matching alphanumeric tokens plus single quotes so \\nthat we don\\'t split up contractions.\\nHow to do it...\\nWe\\'ll create an instance of the RegexpTokenizer, giving it a regular expression string to \\nuse for matching tokens.\\n>>> from nltk.tokenize import RegexpTokenizer\\n>>> tokenizer = RegexpTokenizer(\"[\\\\w\\']+\")\\n>>> tokenizer.tokenize(\"Can\\'t is a contraction.\")\\n[\"Can\\'t\", \\'is\\', \\'a\\', \\'contraction\\']\\nThere\\'s also a simple helper function you can use in case you don\\'t want to instantiate  \\nthe class.\\n>>> from nltk.tokenize import regexp_tokenize\\n>>> regexp_tokenize(\"Can\\'t is a contraction.\", \"[\\\\w\\']+\")\\n[\"Can\\'t\", \\'is\\', \\'a\\', \\'contraction\\']\\nNow we inally have something that can treat contractions as whole words, instead of splitting \\nthem into tokens.\\nHow it works...\\nThe RegexpTokenizer works by compiling your pattern, then calling re.findall() on \\nyour text. You could do all this yourself using the re module, but the RegexpTokenizer \\nimplements the TokenizerI interface, just like all the word tokenizers from the previous \\nrecipe. This means it can be used by other parts of the NLTK package, such as corpus \\nreaders, which we\\'ll cover in detail in Chapter 3, Creating Custom Corpora. Many corpus \\nreaders need a way to tokenize the text they\\'re reading, and can take optional keyword \\narguments specifying an instance of a TokenizerI subclass. This way, you have the ability to \\nprovide your own tokenizer instance if the default tokenizer is unsuitable.\\nChapter 1\\n13\\nThere\\'s more...\\nRegexpTokenizer can also work by matching the gaps, instead of the tokens. Instead \\nof using re.findall(), the RegexpTokenizer will use re.split(). This is how the \\nBlanklineTokenizer in nltk.tokenize is implemented.\\nSimple whitespace tokenizer\\nHere\\'s a simple example of using the RegexpTokenizer to tokenize on whitespace:\\n>>> tokenizer = RegexpTokenizer(\\'\\\\s+\\', gaps=True)\\n>>> tokenizer.tokenize(\"Can\\'t is a contraction.\")\\n [\"Can\\'t\", \\'is\\', \\'a\\', \\'contraction.\\']\\nNotice that punctuation still remains in the tokens.\\nSee also\\nFor simpler word tokenization, see the previous recipe.\\nFiltering stopwords in a tokenized sentence\\nStopwords are common words that generally do not contribute to the meaning of a sentence, \\nat least for the purposes of information retrieval and natural language processing. Most \\nsearch engines will ilter stopwords out of search queries and documents in order to save \\nspace in their index.\\nGetting ready\\nNLTK comes with a stopwords corpus that contains word lists for many languages. Be sure to \\nunzip the dataile so NLTK can ind these word lists in nltk_data/corpora/stopwords/.\\nHow to do it...\\nWe\\'re going to create a set of all English stopwords, then use it to ilter stopwords from a \\nsentence.\\n>>> from nltk.corpus import stopwords\\n>>> english_stops = set(stopwords.words(\\'english\\'))\\n>>> words = [\"Can\\'t\", \\'is\\', \\'a\\', \\'contraction\\']\\n>>> [word for word in words if word not in english_stops]\\n[\"Can\\'t\", \\'contraction\\']\\nTokenizing Text and WordNet Basics\\n14\\nHow it works...\\nThe stopwords corpus is an instance of nltk.corpus.reader.WordListCorpusReader. \\nAs such, it has a words() method that can take a single argument for the ile ID, which in this \\ncase is \\'english\\', referring to a ile containing a list of English stopwords. You could also \\ncall stopwords.words() with no argument to get a list of all stopwords in every language \\navailable.\\nThere\\'s more...\\nYou can see the list of all English stopwords using stopwords.words(\\'english\\') or by \\nexamining the word list ile at nltk_data/corpora/stopwords/english. There are also \\nstopword lists for many other languages. You can see the complete list of languages using the \\nfileids() method:\\n>>> stopwords.fileids()\\n[\\'danish\\', \\'dutch\\', \\'english\\', \\'finnish\\', \\'french\\', \\'german\\', \\n\\'hungarian\\', \\'italian\\', \\'norwegian\\', \\'portuguese\\', \\'russian\\', \\n\\'spanish\\', \\'swedish\\', \\'turkish\\']\\nAny of these fileids can be used as an argument to the words() method to get a list of \\nstopwords for that language.\\nSee also\\nIf you\\'d like to create your own stopwords corpus, see the Creating a word list corpus recipe \\nin Chapter 3, Creating Custom Corpora, to learn how to use the WordListCorpusReader. \\nWe\\'ll also be using stopwords in the Discovering word collocations recipe, later in this chapter.\\nLooking up synsets for a word in WordNet\\nWordNet is a lexical database for the English language. In other words, it\\'s a dictionary \\ndesigned speciically for natural language processing.\\nNLTK comes with a simple interface for looking up words in WordNet. What you get is a list of \\nsynset instances, which are groupings of synonymous words that express the same concept. \\nMany words have only one synset, but some have several. We\\'ll now explore a single synset, \\nand in the next recipe, we\\'ll look at several in more detail.\\nChapter 1\\n15\\nGetting ready\\nBe sure you\\'ve unzipped the wordnet corpus in nltk_data/corpora/wordnet. This will \\nallow the WordNetCorpusReader to access it.\\nHow to do it...\\nNow we\\'re going to lookup the synset for cookbook, and explore some of the properties and \\nmethods of a synset.\\n>>> from nltk.corpus import wordnet\\n>>> syn = wordnet.synsets(\\'cookbook\\')[0]\\n>>> syn.name\\n\\'cookbook.n.01\\'\\n>>> syn.definition\\n\\'a book of recipes and cooking directions\\'\\nHow it works...\\nYou can look up any word in WordNet using wordnet.synsets(word) to get a list of \\nsynsets. The list may be empty if the word is not found. The list may also have quite a few \\nelements, as some words can have many possible meanings and therefore many synsets.\\nThere\\'s more...\\nEach synset in the list has a number of attributes you can use to learn more about it.  \\nThe name attribute will give you a unique name for the synset, which you can use to get \\nthe synset directly.\\n>>> wordnet.synset(\\'cookbook.n.01\\')\\nSynset(\\'cookbook.n.01\\')\\nThe definition attribute should be self-explanatory. Some synsets also have an examples \\nattribute, which contains a list of phrases that use the word in context.\\n>>> wordnet.synsets(\\'cooking\\')[0].examples\\n[\\'cooking can be a great art\\', \\'people are needed who have experience \\nin cookery\\', \\'he left the preparation of meals to his wife\\']\\nHypernyms\\nSynsets are organized in a kind of inheritance tree. More abstract terms are known as \\nhypernyms and more speciic terms are hyponyms. This tree can be traced all the way up \\nto a root hypernym.\\nTokenizing Text and WordNet Basics\\n16\\nHypernyms provide a way to categorize and group words based on their similarity to each \\nother. The synset similarity recipe details the functions used to calculate similarity based on \\nthe distance between two words in the hypernym tree.\\n>>> syn.hypernyms()\\n[Synset(\\'reference_book.n.01\\')]\\n>>> syn.hypernyms()[0].hyponyms()\\n[Synset(\\'encyclopedia.n.01\\'), Synset(\\'directory.n.01\\'), \\nSynset(\\'source_book.n.01\\'), Synset(\\'handbook.n.01\\'), \\nSynset(\\'instruction_book.n.01\\'), Synset(\\'cookbook.n.01\\'), \\nSynset(\\'annual.n.02\\'), Synset(\\'atlas.n.02\\'), Synset(\\'wordbook.n.01\\')]\\n>>> syn.root_hypernyms()\\n[Synset(\\'entity.n.01\\')]\\nAs you can see, reference book is a hypernym of cookbook, but cookbook is only one of \\nmany hyponyms of reference book. All these types of books have the same root hypernym, \\nentity, one of the most abstract terms in the English language. You can trace the entire \\npath from entity down to cookbook using the hypernym_paths() method.\\n>>> syn.hypernym_paths()\\n[[Synset(\\'entity.n.01\\'), Synset(\\'physical_entity.n.01\\'), \\nSynset(\\'object.n.01\\'), Synset(\\'whole.n.02\\'), Synset(\\'artifact.n.01\\'), \\nSynset(\\'creation.n.02\\'), Synset(\\'product.n.02\\'), Synset(\\'work.n.02\\'), \\nSynset(\\'publication.n.01\\'), Synset(\\'book.n.01\\'), Synset(\\'reference_\\nbook.n.01\\'), Synset(\\'cookbook.n.01\\')]]\\nThis method returns a list of lists, where each list starts at the root hypernym and ends with \\nthe original Synset. Most of the time you\\'ll only get one nested list of synsets.\\nPart-of-speech (POS)\\nYou can also look up a simpliied part-of-speech tag.\\n>>> syn.pos\\n\\'n\\'\\nThere are four common POS found in WordNet.\\nPart-of-speech\\nTag\\nNoun\\nn\\nAdjective\\na\\nAdverb\\nr\\nVerb\\nv\\nThese POS tags can be used for looking up speciic synsets for a word. For example, the \\nword great can be used as a noun or an adjective. In WordNet, great has one noun synset \\nand six adjective synsets.\\nChapter 1\\n17\\n>>> len(wordnet.synsets(\\'great\\'))\\n7\\n>>> len(wordnet.synsets(\\'great\\', pos=\\'n\\'))\\n1\\n>>> len(wordnet.synsets(\\'great\\', pos=\\'a\\'))\\n6\\nThese POS tags will be referenced more in the Using WordNet for Tagging recipe of \\nChapter 4, Part-of-Speech Tagging.\\nSee also\\nIn the next two recipes, we\\'ll explore lemmas and how to calculate synset similarity. In \\nChapter 2, Replacing and Correcting Words, we\\'ll use WordNet for lemmatization, synonym \\nreplacement, and then explore the use of antonyms.\\nLooking up lemmas and synonyms \\nin WordNet\\nBuilding on the previous recipe, we can also look up lemmas in WordNet to ind synonyms of a \\nword. A lemma (in linguistics) is the canonical form, or morphological form, of a word.\\nHow to do it...\\nIn the following block of code, we\\'ll ind that there are two lemmas for the cookbook synset \\nby using the lemmas attribute:\\n>>> from nltk.corpus import wordnet\\n>>> syn = wordnet.synsets(\\'cookbook\\')[0]\\n>>> lemmas = syn.lemmas\\n>>> len(lemmas)\\n2\\n>>> lemmas[0].name\\n\\'cookbook\\'\\n>>> lemmas[1].name\\n\\'cookery_book\\'\\n>>> lemmas[0].synset == lemmas[1].synset\\nTrue\\nTokenizing Text and WordNet Basics\\n18\\nHow it works...\\nAs you can see, cookery_book and cookbook are two distinct lemmas in the same \\nsynset. In fact, a lemma can only belong to a single synset. In this way, a synset represents \\na group of lemmas that all have the same meaning, while a lemma represents a distinct  \\nword form.\\nThere\\'s more...\\nSince lemmas in a synset all have the same meaning, they can be treated as synonyms. So if \\nyou wanted to get all synonyms for a synset, you could do:\\n>>> [lemma.name for lemma in syn.lemmas]\\n[\\'cookbook\\', \\'cookery_book\\']\\nAll possible synonyms\\nAs mentioned before, many words have multiple synsets because the word can have \\ndifferent meanings depending on the context. But let\\'s say you didn\\'t care about the context, \\nand wanted to get all possible synonyms for a word.\\n>>> synonyms = []\\n>>> for syn in wordnet.synsets(\\'book\\'):\\n...     for lemma in syn.lemmas:\\n...         synonyms.append(lemma.name)\\n>>> len(synonyms)\\n38\\nAs you can see, there appears to be 38 possible synonyms for the word book. But in fact, \\nsome are verb forms, and many are just different usages of book. Instead, if we take the set \\nof synonyms, there are fewer unique words.\\n>>> len(set(synonyms))\\n25\\nAntonyms\\nSome lemmas also have antonyms. The word good, for example, has 27 synsets, ive of \\nwhich have lemmas with antonyms.\\n>>> gn2 = wordnet.synset(\\'good.n.02\\')\\n>>> gn2.definition\\n\\'moral excellence or admirableness\\'\\n>>> evil = gn2.lemmas[0].antonyms()[0]\\n>>> evil.name\\n\\'evil\\'\\n>>> evil.synset.definition\\nChapter 1\\n19\\n\\'the quality of being morally wrong in principle or practice\\'\\n>>> ga1 = wordnet.synset(\\'good.a.01\\')\\n>>> ga1.definition\\n\\'having desirable or positive qualities especially those suitable for \\na thing specified\\'\\n>>> bad = ga1.lemmas[0].antonyms()[0]\\n>>> bad.name\\n\\'bad\\'\\n>>> bad.synset.definition\\n\\'having undesirable or negative qualities\\'\\nThe antonyms() method returns a list of lemmas. In the irst case here, we see that the \\nsecond synset for good as a noun is deined as moral excellence, and its irst antonym \\nis evil, deined as morally wrong. In the second case, when good is used as an adjective \\nto describe positive qualities, the irst antonym is bad, which describes negative qualities.\\nSee also\\nIn the next recipe, we\\'ll learn how to calculate synset similarity. Then in Chapter 2, Replacing \\nand Correcting Words, we\\'ll revisit lemmas for lemmatization, synonym replacement, and \\nantonym replacement.\\nCalculating WordNet synset similarity\\nSynsets are organized in a hypernym tree. This tree can be used for reasoning about the \\nsimilarity between the synsets it contains. Two synsets are more similar, the closer they are  \\nin the tree.\\nHow to do it...\\nIf you were to look at all the hyponyms of reference book (which is the hypernym of \\ncookbook) you\\'d see that one of them is instruction_book. These seem intuitively very \\nsimilar to cookbook, so let\\'s see what WordNet similarity has to say about it.\\n>>> from nltk.corpus import wordnet\\n>>> cb = wordnet.synset(\\'cookbook.n.01\\')\\n>>> ib = wordnet.synset(\\'instruction_book.n.01\\')\\n>>> cb.wup_similarity(ib)\\n0.91666666666666663\\nSo they are over 91% similar!\\nwww.allitebooks.com\\nTokenizing Text and WordNet Basics\\n20\\nHow it works...\\nwup_similarity is short for Wu-Palmer Similarity, which is a scoring method based on \\nhow similar the word senses are and where the synsets occur relative to each other in the \\nhypernym tree. One of the core metrics used to calculate similarity is the shortest path \\ndistance between the two synsets and their common hypernym.\\n>>> ref = cb.hypernyms()[0]\\n>>> cb.shortest_path_distance(ref)\\n1\\n>>> ib.shortest_path_distance(ref)\\n1\\n>>> cb.shortest_path_distance(ib)\\n2\\nSo cookbook and instruction book must be very similar, because they are only one step \\naway from the same hypernym, reference book, and therefore only two steps away from \\neach other.\\nThere\\'s more...\\nLet\\'s look at two dissimilar words to see what kind of score we get. We\\'ll compare dog with \\ncookbook, two seemingly very different words.\\n>>> dog = wordnet.synsets(\\'dog\\')[0]\\n>>> dog.wup_similarity(cb)\\n0.38095238095238093\\nWow, dog and cookbook are apparently 38% similar! This is because they share common \\nhypernyms farther up the tree.\\n>>> dog.common_hypernyms(cb)\\n[Synset(\\'object.n.01\\'), Synset(\\'whole.n.02\\'), Synset(\\'physical_\\nentity.n.01\\'), Synset(\\'entity.n.01\\')]\\nComparing verbs\\nThe previous comparisons were all between nouns, but the same can be done for verbs  \\nas well.\\n>>> cook = wordnet.synset(\\'cook.v.01\\')\\n>>> bake = wordnet.synset(\\'bake.v.02\\')\\n>>> cook.wup_similarity(bake)\\n0.75\\nChapter 1\\n21\\nThe previous synsets were obviously handpicked for demonstration, and the reason is that \\nthe hypernym tree for verbs has a lot more breadth and a lot less depth. While most nouns \\ncan be traced up to object, thereby providing a basis for similarity, many verbs do not share \\ncommon hypernyms, making WordNet unable to calculate similarity. For example, if you were \\nto use the synset for bake.v.01 here, instead of bake.v.02, the return value would be \\nNone. This is because the root hypernyms of the two synsets are different, with no overlapping \\npaths. For this reason, you also cannot calculate similarity between words with different parts \\nof speech.\\nPath and LCH similarity\\nTwo other similarity comparisons are the path similarity and Leacock Chodorow (LCH) \\nsimilarity.\\n>>> cb.path_similarity(ib)\\n0.33333333333333331\\n>>> cb.path_similarity(dog)\\n0.071428571428571425\\n>>> cb.lch_similarity(ib)\\n2.5389738710582761\\n>>> cb.lch_similarity(dog)\\n0.99852883011112725\\nAs you can see, the number ranges are very different for these scoring methods, which is why \\nwe prefer the wup_similarity() method.\\nSee also\\nThe recipe on Looking up synsets for a word in WordNet, discussed earlier in this chapter, has \\nmore details about hypernyms and the hypernym tree.\\nDiscovering word collocations\\nCollocations are two or more words that tend to appear frequently together, such as \"United \\nStates\". Of course, there are many other words that can come after \"United\", for example \\n\"United Kingdom\", \"United Airlines\", and so on. As with many aspects of natural language \\nprocessing, context is very important, and for collocations, context is everything!\\nIn the case of collocations, the context will be a document in the form of a list of words. \\nDiscovering collocations in this list of words means that we\\'ll ind common phrases that occur \\nfrequently throughout the text. For fun, we\\'ll start with the script for Monty Python and the \\nHoly Grail.\\nTokenizing Text and WordNet Basics\\n22\\nGetting ready\\nThe script for Monty Python and the Holy Grail is found in the webtext corpus, so be sure \\nthat it\\'s unzipped in nltk_data/corpora/webtext/.\\nHow to do it...\\nWe\\'re going to create a list of all lowercased words in the text, and then produce a \\nBigramCollocationFinder, which we can use to ind bigrams, which are pairs of words. \\nThese bigrams are found using association measurement functions found in the nltk.\\nmetrics package.\\n>>> from nltk.corpus import webtext\\n>>> from nltk.collocations import BigramCollocationFinder\\n>>> from nltk.metrics import BigramAssocMeasures\\n>>> words = [w.lower() for w in webtext.words(\\'grail.txt\\')]\\n>>> bcf = BigramCollocationFinder.from_words(words)\\n>>> bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\\n[(\"\\'\", \\'s\\'), (\\'arthur\\', \\':\\'), (\\'#\\', \\'1\\'), (\"\\'\", \\'t\\')]\\nWell that\\'s not very useful! Let\\'s reine it a bit by adding a word ilter to remove punctuation \\nand stopwords.\\n>>> from nltk.corpus import stopwords\\n>>> stopset = set(stopwords.words(\\'english\\'))\\n>>> filter_stops = lambda w: len(w) < 3 or w in stopset\\n>>> bcf.apply_word_filter(filter_stops)\\n>>> bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\\n[(\\'black\\', \\'knight\\'), (\\'clop\\', \\'clop\\'), (\\'head\\', \\'knight\\'), (\\'mumble\\', \\n\\'mumble\\')]\\nMuch better—we can clearly see four of the most common bigrams in Monty Python and the \\nHoly Grail. If you\\'d like to see more than four, simply increase the number to whatever you \\nwant, and the collocation inder will do its best.\\nHow it works...\\nThe BigramCollocationFinder constructs two frequency distributions: one for each \\nword, and another for bigrams. A frequency distribution, or FreqDist in NLTK, is basically \\nan enhanced dictionary where the keys are what\\'s being counted, and the values are the \\ncounts. Any iltering functions that are applied, reduce the size of these two FreqDists by \\neliminating any words that don\\'t pass the ilter. By using a iltering function to eliminate all \\nwords that are one or two characters, and all English stopwords, we can get a much cleaner \\nresult. After iltering, the collocation inder is ready to accept a generic scoring function for \\ninding collocations. Additional scoring functions are covered in the Scoring functions section \\nfurther in this chapter.\\nChapter 1\\n23\\nThere\\'s more...\\nIn addition to BigramCollocationFinder, there\\'s also TrigramCollocationFinder, \\nfor inding triples instead of pairs. This time, we\\'ll look for trigrams in Australian singles ads.\\n>>> from nltk.collocations import TrigramCollocationFinder\\n>>> from nltk.metrics import TrigramAssocMeasures\\n>>> words = [w.lower() for w in webtext.words(\\'singles.txt\\')]\\n>>> tcf = TrigramCollocationFinder.from_words(words)\\n>>> tcf.apply_word_filter(filter_stops)\\n>>> tcf.apply_freq_filter(3)\\n>>> tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)\\n[(\\'long\\', \\'term\\', \\'relationship\\')]\\nNow, we don\\'t know whether people are looking for a long-term relationship or not, but clearly \\nit\\'s an important topic. In addition to the stopword ilter, we also applied a frequency ilter \\nwhich removed any trigrams that occurred less than three times. This is why only one result \\nwas returned when we asked for four—because there was only one result that occurred more \\nthan twice.\\nScoring functions\\nThere are many more scoring functions available besides likelihood_ratio(). But other \\nthan raw_freq(), you may need a bit of a statistics background to understand how they \\nwork. Consult the NLTK API documentation for NgramAssocMeasures in the nltk.metrics \\npackage, to see all the possible scoring functions.\\nScoring ngrams\\nIn addition to the nbest() method, there are two other ways to get ngrams (a generic term \\nfor describing bigrams and trigrams) from a collocation inder.\\n1. above_score(score_fn, min_score) can be used to get all ngrams with scores \\nthat are at least min_score. The min_score that you choose will depend heavily on \\nthe score_fn you use.\\n2. score_ngrams(score_fn) will return a list with tuple pairs of (ngram, score). \\nThis can be used to inform your choice for min_score in the previous step.\\nSee also\\nThe nltk.metrics module will be used again in Chapter 7, Text Classiication.\\n2\\nReplacing and \\nCorrecting Words\\nIn this chapter, we will cover:\\n \\nf\\nStemming words\\n \\nf\\nLemmatizing words with WordNet\\n \\nf\\nTranslating text with Babelish\\n \\nf\\nReplacing words matching regular expressions\\n \\nf\\nRemoving repeating characters\\n \\nf\\nSpelling correction with Enchant\\n \\nf\\nReplacing synonyms\\n \\nf\\nReplacing negations with antonyms\\nIntroduction\\nIn this chapter, we will go over various word replacement and correction techniques. The \\nrecipes cover the gamut of linguistic compression, spelling correction, and text normalization. \\nAll of these methods can be very useful for pre-processing text before search indexing, \\ndocument classiication, and text analysis.\\nStemming words\\nStemming is a technique for removing afixes from a word, ending up with the stem. For \\nexample, the stem of \"cooking\" is \"cook\", and a good stemming algorithm knows that the \\n\"ing\" sufix can be removed. Stemming is most commonly used by search engines for indexing \\nwords. Instead of storing all forms of a word, a search engine can store only the stems, greatly \\nreducing the size of index while increasing retrieval accuracy.\\nReplacing and Correcting Words\\n26\\nOne of the most common stemming algorithms is the Porter Stemming Algorithm, by Martin \\nPorter. It is designed to remove and replace well known sufixes of English words, and its \\nusage in NLTK will be covered next.\\nThe resulting stem is not always a valid word. For example, the \\nstem of \"cookery\" is \"cookeri\". This is a feature, not a bug.\\nHow to do it...\\nNLTK comes with an implementation of the Porter Stemming Algorithm, which is very easy  \\nto use. Simply instantiate the PorterStemmer class and call the stem() method with the \\nword you want to stem.\\n>>> from nltk.stem import PorterStemmer\\n>>> stemmer = PorterStemmer()\\n>>> stemmer.stem(\\'cooking\\')\\n\\'cook\\'\\n>>> stemmer.stem(\\'cookery\\')\\n\\'cookeri\\'\\nHow it works...\\nThe PorterStemmer knows a number of regular word forms and sufixes, and uses \\nthat knowledge to transform your input word to a inal stem through a series of steps. The \\nresulting stem is often a shorter word, or at least a common form of the word, that has the \\nsame root meaning.\\nThere\\'s more...\\nThere are other stemming algorithms out there besides the Porter Stemming Algorithm, such \\nas the Lancaster Stemming Algorithm, developed at Lancaster University. NLTK includes \\nit as the LancasterStemmer class. At the time of writing, there is no deinitive research \\ndemonstrating the superiority of one algorithm over the other. However, Porter Stemming  \\nis generally the default choice.\\nChapter 2\\n27\\nAll the stemmers covered next inherit from the StemmerI interface, which deines the \\nstem() method. The following is an inheritance diagram showing this:\\nLancasterStemmer\\nThe LancasterStemmer functions just like the PorterStemmer, but can produce slightly \\ndifferent results. It is known to be slightly more aggressive than the PorterStemmer.\\n>>> from nltk.stem import LancasterStemmer\\n>>> stemmer = LancasterStemmer()\\n>>> stemmer.stem(\\'cooking\\')\\n\\'cook\\'\\n>>> stemmer.stem(\\'cookery\\')\\n\\'cookery\\'\\nRegexpStemmer\\nYou can also construct your own stemmer using the RegexpStemmer. It takes a single regular \\nexpression (either compiled or as a string) and will remove any preix or sufix that matches.\\n>>> from nltk.stem import RegexpStemmer\\n>>> stemmer = RegexpStemmer(\\'ing\\')\\n>>> stemmer.stem(\\'cooking\\')\\n\\'cook\\'\\n>>> stemmer.stem(\\'cookery\\')\\n\\'cookery\\'\\n>>> stemmer.stem(\\'ingleside\\')\\n\\'leside\\'\\nA RegexpStemmer should only be used in very speciic cases that are not covered by the \\nPorterStemmer or LancasterStemmer.\\nReplacing and Correcting Words\\n28\\nSnowballStemmer\\nNew in NLTK 2.0b9 is the SnowballStemmer, which supports 13 non-English languages. \\nTo use it, you create an instance with the name of the language you are using, and then call \\nthe stem() method. Here is a list of all the supported languages, and an example using the \\nSpanish SnowballStemmer:\\n>>> from nltk.stem import SnowballStemmer\\n>>> SnowballStemmer.languages\\n(\\'danish\\', \\'dutch\\', \\'finnish\\', \\'french\\', \\'german\\', \\'hungarian\\', \\n\\'italian\\', \\'norwegian\\', \\'portuguese\\', \\'romanian\\', \\'russian\\', \\n\\'spanish\\', \\'swedish\\')\\n>>> spanish_stemmer = SnowballStemmer(\\'spanish\\')\\n>>> spanish_stemmer.stem(\\'hola\\')\\nu\\'hol\\'\\nSee also\\nIn the next recipe, we will cover lemmatization, which is quite similar to stemming, but  \\nsubtly different.\\nLemmatizing words with WordNet\\nLemmatization is very similar to stemming, but is more akin to synonym replacement. A \\nlemma is a root word, as opposed to the root stem. So unlike stemming, you are always \\nleft with a valid word which means the same thing. But the word you end up with can be \\ncompletely different. A few examples will explain lemmatization...\\nGetting ready\\nBe sure you have unzipped the wordnet corpus in nltk_data/corpora/wordnet. This will \\nallow the WordNetLemmatizer to access WordNet. You should also be somewhat familiar \\nwith the part-of-speech tags covered in the Looking up synsets for a word in WordNet recipe of \\nChapter 1, Tokenizing Text and WordNet Basics.\\nHow to do it...\\nWe will use the WordNetLemmatizer to ind lemmas:\\n>>> from nltk.stem import WordNetLemmatizer\\n>>> lemmatizer = WordNetLemmatizer()\\n>>> lemmatizer.lemmatize(\\'cooking\\')\\n\\'cooking\\'\\nChapter 2\\n29\\n>>> lemmatizer.lemmatize(\\'cooking\\', pos=\\'v\\')\\n\\'cook\\'\\n>>> lemmatizer.lemmatize(\\'cookbooks\\')\\n\\'cookbook\\'\\nHow it works...\\nThe WordNetLemmatizer is a thin wrapper around the WordNet corpus, and uses the \\nmorphy() function of the WordNetCorpusReader to ind a lemma. If no lemma is found, \\nthe word is returned as it is. Unlike with stemming, knowing the part of speech of the word is \\nimportant. As demonstrated previously, \"cooking\" does not have a lemma unless you specify \\nthat the part of speech (pos) is a verb. This is because the default part of speech is a noun, \\nand since \"cooking\" is not a noun, no lemma is found. \"Cookbooks\", on the other hand, is a \\nnoun, and its lemma is the singular form, \"cookbook\".\\nThere\\'s more...\\nHere\\'s an example that illustrates one of the major differences between stemming  \\nand lemmatization:\\n>>> from nltk.stem import PorterStemmer\\n>>> stemmer = PorterStemmer()\\n>>> stemmer.stem(\\'believes\\')\\n\\'believ\\'\\n>>> lemmatizer.lemmatize(\\'believes\\')\\n\\'belief\\'\\nInstead of just chopping off the \"es\" like the PorterStemmer, the WordNetLemmatizer \\ninds a valid root word. Where a stemmer only looks at the form of the word, the lemmatizer \\nlooks at the meaning of the word. And by returning a lemma, you will always get a valid word.\\nCombining stemming with lemmatization\\nStemming and lemmatization can be combined to compress words more than either process \\ncan by itself. These cases are somewhat rare, but they do exist:\\n>>> stemmer.stem(\\'buses\\')\\n\\'buse\\'\\n>>> lemmatizer.lemmatize(\\'buses\\')\\n\\'bus\\'\\n>>> stemmer.stem(\\'bus\\')\\n\\'bu\\'\\nwww.allitebooks.com\\nReplacing and Correcting Words\\n30\\nIn this example, stemming saves one character, lemmatizing saves two characters, and \\nstemming the lemma saves a total of three characters out of ive characters. That is nearly a \\n60% compression rate! This level of word compression over many thousands of words, while \\nunlikely to always produce such high gains, can still make a huge difference.\\nSee also\\nIn the previous recipe, we covered stemming basics and WordNet was introduced in the \\nLooking up synsets for a word in WordNet and Looking up lemmas and synonyms in WordNet \\nrecipes of Chapter 1, Tokenizing Text and WordNet Basics. Looking forward, we will cover the \\nUsing WordNet for Tagging recipe in Chapter 4, Part-of-Speech Tagging.\\nTranslating text with Babelish\\nBabelish is an online language translation API provided by Yahoo. With it, you can translate \\ntext in a source language to a target language. NLTK comes with a simple interface for \\nusing it.\\nGetting ready\\nBe sure you are connected to the internet irst. The babelfish.translate() function \\nrequires access to Yahoo\\'s online API in order to work.\\nHow to do it...\\nTo translate your text, you irst need to know two things:\\n1. The language of your text or source language.\\n2. The language you want to translate to or target language.\\nLanguage detection is outside the scope of this recipe, so we will assume you already know \\nthe source and target languages.\\n>>> from nltk.misc import babelfish\\n>>> babelfish.translate(\\'cookbook\\', \\'english\\', \\'spanish\\')\\n\\'libro de cocina\\'\\n>>> babelfish.translate(\\'libro de cocina\\', \\'spanish\\', \\'english\\')\\n\\'kitchen book\\'\\n>>> babelfish.translate(\\'cookbook\\', \\'english\\', \\'german\\')\\n\\'Kochbuch\\'\\n>>> babelfish.translate(\\'kochbuch\\', \\'german\\', \\'english\\')\\n\\'cook book\\'\\nChapter 2\\n31\\nYou cannot translate using the same language for both source and target. \\nAttempting to do so will raise a BabelfishChangedError.\\nHow it works...\\nThe translate() function is a small function that sends a urllib request to \\nhttp://babelfish.yahoo.com/translate_txt, and then searches the \\nresponse for the translated text.\\nIf Yahoo, for whatever reason, had changed their HTML response \\nto the point that translate() cannot identify the translated \\ntext, a BabelfishChangedError will be raised. This is unlikely \\nto happen, but if it does, you may need to upgrade to a newer \\nversion of NLTK and/or report the error.\\nThere\\'s more...\\nThere is also a fun function called babelize() that translates back and forth between the \\nsource and target language until there are no more changes.\\n>>> for text in babelfish.babelize(\\'cookbook\\', \\'english\\', \\'spanish\\'):\\n...  print text\\ncookbook\\nlibro de cocina\\nkitchen book\\nlibro de la cocina\\nbook of the kitchen\\nAvailable languages\\nYou can see all the languages available for translation by examining the available_\\nlanguages attribute.\\n>>> babelfish.available_languages\\n[\\'Portuguese\\', \\'Chinese\\', \\'German\\', \\'Japanese\\', \\'French\\', \\'Spanish\\', \\n\\'Russian\\', \\'Greek\\', \\'English\\', \\'Korean\\', \\'Italian\\']\\nThe lowercased version of each of these languages can be used as a source or target \\nlanguage for translation.\\nReplacing and Correcting Words\\n32\\nReplacing words matching regular  \\nexpressions\\nNow we are going to get into the process of replacing words. Where stemming and \\nlemmatization are a kind of linguistic compression, and word replacement can be thought \\nof as error correction, or text normalization.\\nFor this recipe, we will be replacing words based on regular expressions, with a focus on \\nexpanding contractions. Remember when we were tokenizing words in Chapter 1, Tokenizing \\nText and WordNet Basics and it was clear that most tokenizers had trouble with contractions? \\nThis recipe aims to ix that by replacing contractions with their expanded forms, such as by \\nreplacing \"can\\'t\" with \"cannot\", or \"would\\'ve\" with \"would have\".\\nGetting ready\\nUnderstanding how this recipe works will require a basic knowledge of regular expressions and \\nthe re module. The key things to know are matching patterns and the re.subn() function.\\nHow to do it...\\nFirst, we need to deine a number of replacement patterns. This will be a list of tuple pairs, \\nwhere the irst element is the pattern to match on, and the second element is the replacement.\\nNext, we will create a RegexpReplacer class that will compile the patterns, and provide a \\nreplace() method to substitute all found patterns with their replacements.\\nThe following code can be found in the replacers.py module and is meant to be imported, \\nnot typed into the console:\\nimport re\\nreplacement_patterns = [\\n  (r\\'won\\\\\\'t\\', \\'will not\\'),\\n  (r\\'can\\\\\\'t\\', \\'cannot\\'),\\n  (r\\'i\\\\\\'m\\', \\'i am\\'),\\n  (r\\'ain\\\\\\'t\\', \\'is not\\'),\\n  (r\\'(\\\\w+)\\\\\\'ll\\', \\'\\\\g<1> will\\'),\\n  (r\\'(\\\\w+)n\\\\\\'t\\', \\'\\\\g<1> not\\'),\\n  (r\\'(\\\\w+)\\\\\\'ve\\', \\'\\\\g<1> have\\'),\\n  (r\\'(\\\\w+)\\\\\\'s\\', \\'\\\\g<1> is\\'),\\n  (r\\'(\\\\w+)\\\\\\'re\\', \\'\\\\g<1> are\\'),\\n  (r\\'(\\\\w+)\\\\\\'d\\', \\'\\\\g<1> would\\')\\n]\\nclass RegexpReplacer(object):\\nChapter 2\\n33\\n  def __init__(self, patterns=replacement_patterns):\\n    self.patterns = [(re.compile(regex), repl) for (regex, repl) in  \\n      patterns]\\n  def replace(self, text):\\n    s = text\\n    for (pattern, repl) in self.patterns:\\n      (s, count) = re.subn(pattern, repl, s)\\n    return s\\nHow it works...\\nHere is a simple usage example:\\n>>> from replacers import RegexpReplacer\\n>>> replacer = RegexpReplacer()\\n>>> replacer.replace(\"can\\'t is a contraction\")\\n\\'cannot is a contraction\\'\\n>>> replacer.replace(\"I should\\'ve done that thing I didn\\'t do\")\\n\\'I should have done that thing I did not do\\'\\nRegexpReplacer.replace() works by replacing every instance of a replacement pattern \\nwith its corresponding substitution pattern. In replacement_patterns, we have deined \\ntuples such as (r\\'(\\\\w+)\\\\\\'ve\\', \\'\\\\g<1> have\\'). The irst element matches a group of \\nASCII characters followed by \\'ve. By grouping the characters before the \\'ve in parenthesis, \\na match group is found and can be used in the substitution pattern with the \\\\g<1> reference. \\nSo we keep everything before \\'ve, then replace \\'ve with the word have. This is how \\n\"should\\'ve\" can become \"should have\".\\nThere\\'s more...\\nThis replacement technique can work with any kind of regular expression, not just \\ncontractions. So you could replace any occurrence of \"&\" with \"and\", or eliminate all \\noccurrences of \"-\" by replacing it with the empty string. The RegexpReplacer can \\ntake any list of replacement patterns for whatever purpose.\\nReplacement before tokenization\\nLet us try using the RegexpReplacer as a preliminary step before tokenization:\\n>>> from nltk.tokenize import word_tokenize\\n>>> from replacers import RegexpReplacer\\n>>> replacer = RegexpReplacer()\\n>>> word_tokenize(\"can\\'t is a contraction\")\\n[\\'ca\\', \"n\\'t\", \\'is\\', \\'a\\', \\'contraction\\']\\n>>> word_tokenize(replacer.replace(\"can\\'t is a contraction\"))\\n[\\'can\\', \\'not\\', \\'is\\', \\'a\\', \\'contraction\\']\\nReplacing and Correcting Words\\n34\\nMuch better! By eliminating the contractions in the irst place, the tokenizer will produce \\ncleaner results. Cleaning up text before processing is a common pattern in natural  \\nlanguage processing.\\nSee also\\nFor more information on tokenization, see the irst three recipes in Chapter 1, Tokenizing \\nText and WordNet Basics. For more replacement techniques, continue reading the rest of \\nthis chapter.\\nRemoving repeating characters\\nIn everyday language, people are often not strictly grammatical. They will write things like \\n\"I looooooove it\" in order to emphasize the word \"love\". But computers don\\'t know that \\n\"looooooove\" is a variation of \"love\" unless they are told. This recipe presents a method for \\nremoving those annoying repeating characters in order to end up with a \"proper\" English word.\\nGetting ready\\nAs in the previous recipe, we will be making use of the re module, and more speciically, \\nbackreferences. A backreference is a way to refer to a previously matched group in a regular \\nexpression. This is what will allow us to match and remove repeating characters.\\nHow to do it...\\nWe will create a class that has the same form as the RegexpReplacer from the previous \\nrecipe. It will have a replace() method that takes a single word and returns a more correct \\nversion of that word, with dubious repeating characters removed. The following code can be \\nfound in replacers.py and is meant to be imported:\\nimport re\\nclass RepeatReplacer(object):\\n  def __init__(self):\\n    self.repeat_regexp = re.compile(r\\'(\\\\w*)(\\\\w)\\\\2(\\\\w*)\\')\\n    self.repl = r\\'\\\\1\\\\2\\\\3\\'\\n  def replace(self, word):\\n    repl_word = self.repeat_regexp.sub(self.repl, word)\\n    if repl_word != word:\\n      return self.replace(repl_word)\\n    else:\\n      return repl_word\\nChapter 2\\n35\\nAnd now some example use cases:\\n>>> from replacers import RepeatReplacer\\n>>> replacer = RepeatReplacer()\\n>>> replacer.replace(\\'looooove\\')\\n\\'love\\'\\n>>> replacer.replace(\\'oooooh\\')\\n\\'oh\\'\\n>>> replacer.replace(\\'goose\\')\\n\\'gose\\'\\nHow it works...\\nRepeatReplacer starts by compiling a regular expression for matching and deining a \\nreplacement string with backreferences. The repeat_regexp matches three groups:\\n1. Zero or more starting characters (\\\\w*).\\n2. A single character (\\\\w), followed by another instance of that character \\\\2.\\n3. Zero or more ending characters (\\\\w*).\\nThe replacement string is then used to keep all the matched groups, while discarding the \\nbackreference to the second group. So the word \"looooove\" gets split into (l)(o)o(ooove) \\nand then recombined as \"loooove\", discarding the second \"o\". This continues until only one \"o\" \\nremains, when repeat_regexp no longer matches the string, and no more characters \\nare removed.\\nThere\\'s more...\\nIn the preceding examples, you can see that the RepeatReplacer is a bit too greedy and \\nends up changing \"goose\" into \"gose\". To correct this issue, we can augment the replace() \\nfunction with a WordNet lookup. If WordNet recognizes the word, then we can stop replacing \\ncharacters. Here is the WordNet augmented version:\\nimport re\\nfrom nltk.corpus import wordnet\\nclass RepeatReplacer(object):\\n  def __init__(self):\\n    self.repeat_regexp = re.compile(r\\'(\\\\w*)(\\\\w)\\\\2(\\\\w*)\\')\\n    self.repl = r\\'\\\\1\\\\2\\\\3\\'\\n  def replace(self, word):\\n    if wordnet.synsets(word):\\n      return word\\n    repl_word = self.repeat_regexp.sub(self.repl, word)\\n    if repl_word != word:\\n      return self.replace(repl_word)\\n    else:\\n      return repl_word\\nReplacing and Correcting Words\\n36\\nNow, \"goose\" will be found in WordNet, and no character replacement will take place. And \\n\"oooooh\" will become \"ooh\" instead of \"oh\", because \"ooh\" is actually a word in WordNet, \\ndeined as an expression of admiration or pleasure.\\nSee also\\nRead the next recipe to learn how to correct misspellings. And for more on WordNet, refer to \\nthe WordNet recipes in Chapter 1, Tokenizing Text and WordNet Basics. We will also be using \\nWordNet for antonym replacement later in this chapter.\\nSpelling correction with Enchant\\nReplacing repeating characters is actually an extreme form of spelling correction. In this \\nrecipe, we will take on the less extreme case of correcting minor spelling issues using \\nEnchant—a spelling correction API.\\nGetting ready\\nYou will need to install Enchant, and a dictionary for it to use. Enchant is an offshoot  \\nof the \"Abiword\" open source word processor, and more information can be found at  \\nhttp://www.abisource.com/projects/enchant/.\\nFor dictionaries, aspell is a good open source spellchecker and dictionary that can be found \\nat http://aspell.net/.\\nFinally, you will need the pyenchant library, which can be found at http://www.rfk.id.au/\\nsoftware/pyenchant/. You should be able to install it with the easy_install command \\nthat comes with python-setuptools, such as by doing sudo easy_install pyenchant \\non Linux or Unix.\\nHow to do it...\\nWe will create a new class called SpellingReplacer in replacers.py, and this time \\nthe replace() method will check Enchant to see whether the word is valid or not. If not, we \\nwill look up suggested alternatives and return the best match using nltk.metrics.edit_\\ndistance():\\nimport enchant\\nfrom nltk.metrics import edit_distance\\nclass SpellingReplacer(object):\\n  def __init__(self, dict_name=\\'en\\', max_dist=2):\\n    self.spell_dict = enchant.Dict(dict_name)\\n    self.max_dist = 2\\nChapter 2\\n37\\n  def replace(self, word):\\n    if self.spell_dict.check(word):\\n      return word\\n    suggestions = self.spell_dict.suggest(word)\\n    if suggestions and edit_distance(word, suggestions[0]) <=  \\n      self.max_dist:\\n      return suggestions[0]\\n    else:\\n      return word\\nThe preceding class can be used to correct English spellings as follows:\\n>>> from replacers import SpellingReplacer\\n>>> replacer = SpellingReplacer()\\n>>> replacer.replace(\\'cookbok\\')\\n\\'cookbook\\'\\nHow it works...\\nSpellingReplacer starts by creating a reference to an enchant dictionary. Then, in the \\nreplace() method, it irst checks whether the given word is present in the dictionary or \\nnot. If it is, no spelling correction is necessary, and the word is returned. But if the word is \\nnot found, it looks up a list of suggestions and returns the irst suggestion, as long as its edit \\ndistance is less than or equal to max_dist. The edit distance is the number of character \\nchanges necessary to transform the given word into the suggested word. max_dist then acts \\nas a constraint on the Enchant suggest() function to ensure that no unlikely replacement \\nwords are returned. Here is an example showing all the suggestions for \"languege\", a \\nmisspelling of \"language\":\\n>>> import enchant\\n>>> d = enchant.Dict(\\'en\\')\\n>>> d.suggest(\\'languege\\')\\n[\\'language\\', \\'languisher\\', \\'languish\\', \\'languor\\', \\'languid\\']\\nExcept for the correct suggestion, \"language\", all the other words have an edit distance of \\nthree or greater.\\nThere\\'s more...\\nYou can use language dictionaries other than \\'en\\', such as \\'en_GB\\', assuming the \\ndictionary has already been installed. To check which other languages are available, use \\nenchant.list_languages():\\n>>> enchant.list_languages()\\n[\\'en_AU\\', \\'en_GB\\', \\'en_US\\', \\'en_ZA\\', \\'en_CA\\', \\'en\\']\\nReplacing and Correcting Words\\n38\\nIf you try to use a dictionary that doesn\\'t exist, you will get enchant.\\nDictNotFoundError. You can irst check whether the dictionary exists \\nusing enchant.dict_exists(), which will return True if the named \\ndictionary exists, or False otherwise.\\nen_GB dictionary\\nAlways be sure to use the correct dictionary for whichever language you are doing spelling \\ncorrection on. \\'en_US\\' can give you different results than \\'en_GB\\', such as for the word \\n\"theater\". \"Theater\" is the American English spelling, whereas the British English spelling  \\nis \"Theatre\":\\n>>> import enchant\\n>>> dUS = enchant.Dict(\\'en_US\\')\\n>>> dUS.check(\\'theater\\')\\nTrue\\n>>> dGB = enchant.Dict(\\'en_GB\\')\\n>>> dGB.check(\\'theater\\')\\nFalse\\n>>> from replacers import SpellingReplacer\\n>>> us_replacer = SpellingReplacer(\\'en_US\\')\\n>>> us_replacer.replace(\\'theater\\')\\n\\'theater\\'\\n>>> gb_replacer = SpellingReplacer(\\'en_GB\\')\\n>>> gb_replacer.replace(\\'theater\\')\\n\\'theatre\\'\\nPersonal word lists\\nEnchant also supports personal word lists. These can be combined with an existing \\ndictionary, allowing you to augment the dictionary with your own words. So let us say you had \\na ile named mywords.txt that had nltk on one line. You could then create a dictionary \\naugmented with your personal word list as follows:\\n>>> d = enchant.Dict(\\'en_US\\')\\n>>> d.check(\\'nltk\\')\\nFalse\\n>>> d = enchant.DictWithPWL(\\'en_US\\', \\'mywords.txt\\')\\n>>> d.check(\\'nltk\\')\\nTrue\\nChapter 2\\n39\\nTo use an augmented dictionary with our SpellingReplacer, we can create a subclass in \\nreplacers.py that takes an existing spelling dictionary.\\nclass CustomSpellingReplacer(SpellingReplacer):\\n  def __init__(self, spell_dict, max_dist=2):\\n    self.spell_dict = spell_dict\\n    self.max_dist = max_dist\\nThis CustomSpellingReplacer will not replace any words that you put into mywords.txt.\\n>>> from replacers import CustomSpellingReplacer\\n>>> d = enchant.DictWithPWL(\\'en_US\\', \\'mywords.txt\\')\\n>>> replacer = CustomSpellingReplacer(d)\\n>>> replacer.replace(\\'nltk\\')\\n\\'nltk\\'\\nSee also\\nThe previous recipe covered an extreme form of spelling correction by replacing repeating \\ncharacters. You could also do spelling correction by simple word replacement as discussed  \\nin the next recipe.\\nReplacing synonyms\\nIt is often useful to reduce the vocabulary of a text by replacing words with common \\nsynonyms. By compressing the vocabulary without losing meaning, you can save memory in \\ncases such as frequency analysis and text indexing. Vocabulary reduction can also increase \\nthe occurrence of signiicant collocations, which was covered in the Discovering word \\ncollocations recipe of Chapter 1, Tokenizing Text and WordNet Basics.\\nGetting ready\\nYou will need to have a deined mapping of a word to its synonym. This is a simple controlled \\nvocabulary. We will start by hardcoding the synonyms as a Python dictionary, then explore \\nother options for storing synonym maps.\\nHow to do it...\\nWe\\'ll irst create a WordReplacer class in replacers.py that takes a word replacement \\nmapping:\\nclass WordReplacer(object):\\n  def __init__(self, word_map):\\n    self.word_map = word_map\\n  def replace(self, word):\\n    return self.word_map.get(word, word)\\nwww.allitebooks.com\\nReplacing and Correcting Words\\n40\\nThen we can demonstrate its usage for simple word replacement:\\n>>> from replacers import wordReplacer\\n>>> replacer = WordReplacer({\\'bday\\': \\'birthday\\'})\\n>>> replacer.replace(\\'bday\\')\\n\\'birthday\\'\\n>>> replacer.replace(\\'happy\\')\\n\\'happy\\'\\nHow it works...\\nWordReplacer is simply a class wrapper around a Python dictionary. The replace() \\nmethod looks up the given word in its word_map and returns the replacement synonym \\nif it exists. Otherwise, the given word is returned as is.\\nIf you were only using the word_map dictionary, you would have no need for the \\nWordReplacer class, and could instead call word_map.get() directly. But WordReplacer \\ncan act as a base class for other classes that construct the word_map from various ile \\nformats. Read on for more information.\\nThere\\'s more...\\nHardcoding synonyms as a Python dictionary is not a good long-term solution. Two better \\nalternatives are to store the synonyms in a CSV ile or in a YAML ile. Choose whichever format \\nis easiest for whoever will be maintaining your synonym vocabulary. Both of the classes \\noutlined in the following section inherit the replace() method from WordReplacer.\\nCSV synonym replacement\\nThe CsvWordReplacer class extends WordReplacer in replacers.py in order to \\nconstruct the word_map from a CSV ile:\\nimport csv\\nclass CsvWordReplacer(WordReplacer):\\n  def __init__(self, fname):\\n    word_map = {}\\n    for line in csv.reader(open(fname)):\\n      word, syn = line\\n      word_map[word] = syn\\n    super(CsvWordReplacer, self).__init__(word_map)\\nChapter 2\\n41\\nYour CSV ile should be two columns, where the irst column is the word, and the second \\ncolumn is the synonym meant to replace it. If this ile is called synonyms.csv and the irst \\nline is bday, birthday, then you can do:\\n>>> from replacers import CsvWordReplacer\\n>>> replacer = CsvWordReplacer(\\'synonyms.csv\\')\\n>>> replacer.replace(\\'bday\\')\\n\\'birthday\\'\\n>>> replacer.replace(\\'happy\\')\\n\\'happy\\'\\nYAML synonym replacement\\nIf you have PyYAML installed, you can create a YamlWordReplacer in replacers.py. \\nDownload and installation instructions for PyYAML are located at http://pyyaml.org/\\nwiki/PyYAML.\\nimport yaml\\nclass YamlWordReplacer(WordReplacer):\\n  def __init__(self, fname):\\n    word_map = yaml.load(open(fname))\\n    super(YamlWordReplacer, self).__init__(word_map)\\nYour YAML ile should be a simple mapping of \"word: synonym\", such as bday: birthday. \\nNote that the YAML syntax is very particular, and the space after the colon is required. If the \\nile is named synonyms.yaml, you can do:\\n>>> from replacers import YamlWordReplacer\\n>>> replacer = YamlWordReplacer(\\'synonyms.yaml\\')\\n>>> replacer.replace(\\'bday\\')\\n\\'birthday\\'\\n>>> replacer.replace(\\'happy\\')\\n\\'happy\\'\\nSee also\\nYou can use the WordReplacer to do any kind of word replacement, even spelling correction \\nfor more complicated words that can\\'t be automatically corrected, as we did in the previous \\nrecipe. In the next recipe, we will cover antonym replacement.\\nReplacing negations with antonyms\\nThe opposite of synonym replacement is antonym replacement. An antonym is the opposite \\nmeaning of a word. This time, instead of creating custom word mappings, we can use  \\nWordNet to replace words with unambiguous antonyms. Refer to the Looking up lemmas \\nand synonyms in WordNet recipe in Chapter 1, Tokenizing Text and WordNet Basics for more \\ndetails on antonym lookups.\\nReplacing and Correcting Words\\n42\\nHow to do it...\\nLet us say you have a sentence such as \"let\\'s not uglify our code\". With antonym replacement, \\nyou can replace \"not uglify\" with \"beautify\", resulting in the sentence \"let\\'s beautify our code\". \\nTo do this, we will need to create an AntonymReplacer in replacers.py as follows:\\nfrom nltk.corpus import wordnet\\nclass AntonymReplacer(object):\\n  def replace(self, word, pos=None):\\n    antonyms = set()\\n    for syn in wordnet.synsets(word, pos=pos):\\n      for lemma in syn.lemmas:\\n        for antonym in lemma.antonyms():\\n          antonyms.add(antonym.name)\\n    if len(antonyms) == 1:\\n      return antonyms.pop()\\n    else:\\n      return None\\n  def replace_negations(self, sent):\\n    i, l = 0, len(sent)\\n    words = []\\n    while i < l:\\n      word = sent[i]\\n      if word == \\'not\\' and i+1 < l:\\n        ant = self.replace(sent[i+1])\\n        if ant:\\n          words.append(ant)\\n          i += 2\\n          continue\\n      words.append(word)\\n      i += 1\\n    return words\\nNow we can tokenize the original sentence into [\"let\\'s\", \\'not\\', \\'uglify\\', \\'our\\', \\n\\'code\\'], and pass this to the replace_negations() function. Here are some examples:\\n>>> from replacers import AntonymReplacer\\n>>> replacer = AntonymReplacer()\\n>>> replacer.replace(\\'good\\')\\n>>> replacer.replace(\\'uglify\\')\\n\\'beautify\\'\\n>>> sent = [\"let\\'s\", \\'not\\', \\'uglify\\', \\'our\\', \\'code\\']\\n>>> replacer.replace_negations(sent)\\n[\"let\\'s\", \\'beautify\\', \\'our\\', \\'code\\']\\nChapter 2\\n43\\nHow it works...\\nThe AntonymReplacer has two methods: replace() and replace_negations(). The \\nreplace() method takes a single word and an optional part of speech tag, then looks up \\nthe synsets for the word in WordNet. Going through all the synsets and every lemma of each \\nsynset, it creates a set of all antonyms found. If only one antonym is found, then it is an \\nunambiguous replacement. If there is more than one antonym found, which can happen quite \\noften, then we don\\'t know for sure which antonym is correct. In the case of multiple antonyms \\n(or no antonyms), replace() returns None since it cannot make a decision.\\nIn replace_negations(), we look through a tokenized sentence for the word \"not\". If \\n\"not\" is found, then we try to ind an antonym for the next word using replace(). If we ind \\nan antonym, then it is appended to the list of words, replacing \"not\" and the original word. \\nAll other words are appended as it is, resulting in a tokenized sentence with unambiguous \\nnegations replaced by their antonyms.\\nThere\\'s more...\\nSince unambiguous antonyms aren\\'t very common in WordNet, you may want to create a \\ncustom antonym mapping the same way we did for synonyms. This AntonymWordReplacer \\ncould be constructed by inheriting from both WordReplacer and AntonymReplacer:\\nclass AntonymWordReplacer(WordReplacer, AntonymReplacer):\\n  pass\\nThe order of inheritance is very important, as we want the initialization and replace() \\nfunction of WordReplacer combined with the replace_negations() function from \\nAntonymReplacer. The result is a replacer that can do the following:\\n>>> from replacers import AntonymWordReplacer\\n>>> replacer = AntonymWordReplacer({\\'evil\\': \\'good\\'})\\n>>> replacer.replace_negations([\\'good\\', \\'is\\', \\'not\\', \\'evil\\'])\\n[\\'good\\', \\'is\\', \\'good\\']\\nOf course, you could also inherit from CsvWordReplacer or YamlWordReplacer instead \\nof WordReplacer if you want to load the antonym word mappings from a ile.\\nSee also\\nThe previous recipe covers the WordReplacer from the perspective of synonym replacement. \\nAnd in Chapter 1, Tokenizing Text and WordNet Basics Wordnet usage is covered in detail \\nin the Looking up synsets for a word in Wordnet and Looking up lemmas and synonyms in \\nWordnet recipes.\\n3\\nCreating Custom \\nCorpora\\nIn this chapter, we will cover:\\n \\nf\\nSetting up a custom corpus\\n \\nf\\nCreating a word list corpus\\n \\nf\\nCreating a part-of-speech tagged word corpus\\n \\nf\\nCreating a chunked phrase corpus\\n \\nf\\nCreating a categorized text corpus\\n \\nf\\nCreating a categorized chunk corpus reader\\n \\nf\\nLazy corpus loading\\n \\nf\\nCreating a custom corpus view\\n \\nf\\nCreating a MongoDB backed corpus reader\\n \\nf\\nCorpus editing with ile locking\\nIntroduction\\nIn this chapter, we\\'ll cover how to use corpus readers and create custom corpora. At the same \\ntime, you\\'ll learn how to use the existing corpus data that comes with NLTK. This information \\nis essential for future chapters when we\\'ll need to access the corpora as training data. We\\'ll \\nalso cover creating custom corpus readers, which can be used when your corpus is not in a \\nile format that NLTK already recognizes, or if your corpus is not in iles at all, but instead is \\nlocated in a database such as MongoDB.\\nCreating Custom Corpora\\n46\\nSetting up a custom corpus\\nA corpus is a collection of text documents, and corpora is the plural of corpus. So a custom \\ncorpus is really just a bunch of text iles in a directory, often alongside many other directories \\nof text iles.\\nGetting ready\\nYou should already have the NLTK data package installed, following the instructions at \\nhttp://www.nltk.org/data. We\\'ll assume that the data is installed to C:\\\\nltk_data \\non Windows, and /usr/share/nltk_data on Linux, Unix, or Mac OS X.\\nHow to do it...\\nNLTK deines a list of data directories, or paths, in nltk.data.path. Our custom corpora \\nmust be within one of these paths so it can be found by NLTK. So as not to conlict with the \\noficial data package, we\\'ll create a custom nltk_data directory in our home directory. \\nHere\\'s some Python code to create this directory and verify that it is in the list of known  \\npaths speciied by nltk.data.path:\\n>>> import os, os.path\\n>>> path = os.path.expanduser(\\'~/nltk_data\\')\\n>>> if not os.path.exists(path):\\n...    os.mkdir(path)\\n>>> os.path.exists(path)\\nTrue\\n>>> import nltk.data\\n>>> path in nltk.data.path\\nTrue\\nIf the last line, path in nltk.data.path, is True, then you should now have a nltk_\\ndata directory in your home directory. The path should be %UserProfile%\\\\nltk_data on \\nWindows, or ~/nltk_data on Unix, Linux, or Mac OS X. For simplicity, I\\'ll refer to the directory \\nas ~/nltk_data.\\nIf the last line does not return True, try creating the nltk_data directory \\nmanually in your home directory, then verify that the absolute path is in \\nnltk.data.path. It\\'s essential to ensure that this directory exists and is \\nin nltk.data.path before continuing. Once you have your nltk_data \\ndirectory, the convention is that corpora reside in a corpora subdirectory. \\nCreate this corpora directory within the nltk_data directory, so that \\nthe path is ~/nltk_data/corpora. Finally, we\\'ll create a subdirectory in \\ncorpora to hold our custom corpus. Let\\'s call it cookbook, giving us the full \\npath of ~/nltk_data/corpora/cookbook.\\nChapter 3\\n47\\nNow we can create a simple word list ile and make sure it loads. In Chapter 2, Replacing and \\nCorrecting Words, Spelling correction with Enchant recipe, we created a word list ile called \\nmywords.txt. Put this ile into ~/nltk_data/corpora/cookbook/. Now we can use \\nnltk.data.load() to load the ile.\\n>>> import nltk.data\\n>>> nltk.data.load(\\'corpora/cookbook/mywords.txt\\', format=\\'raw\\')\\n\\'nltk\\\\n\\'\\nWe need to specify format=\\'raw\\' since nltk.data.load() doesn\\'t \\nknow how to interpret .txt iles. As we\\'ll see, it does know how to interpret a \\nnumber of other ile formats.\\nHow it works...\\nThe nltk.data.load() function recognizes a number of formats, such as \\'raw\\', \\n\\'pickle\\', and \\'yaml\\'. If no format is speciied, then it tries to guess the format based \\non the ile\\'s extension. In the previous case, we have a .txt ile, which is not a recognized \\nextension, so we have to specify the \\'raw\\' format. But if we used a ile that ended in .yaml, \\nthen we would not need to specify the format.\\nFilenames passed in to nltk.data.load() can be absolute or relative paths. Relative \\npaths must be relative to one of the paths speciied in nltk.data.path. The ile is found \\nusing nltk.data.find(path), which searches all known paths combined with the relative \\npath. Absolute paths do not require a search, and are used as is.\\nThere\\'s more...\\nFor most corpora access, you won\\'t actually need to use nltk.data.load, as that will \\nbe handled by the CorpusReader classes covered in the following recipes. But it\\'s a good \\nfunction to be familiar with for loading .pickle iles and .yaml iles, plus it introduces the \\nidea of putting all of your data iles into a path known by NLTK.\\nLoading a YAML ile\\nIf you put the synonyms.yaml ile from the Chapter 2, Replacing and Correcting Words, \\nReplacing synonyms recipe, into ~/nltk_data/corpora/cookbook (next to mywords.\\ntxt), you can use nltk.data.load() to load it without specifying a format.\\n>>> import nltk.data\\n>>> nltk.data.load(\\'corpora/cookbook/synonyms.yaml\\')\\n{\\'bday\\': \\'birthday\\'}\\nThis assumes that PyYAML is installed. If not, you can ind download and installation \\ninstructions at http://pyyaml.org/wiki/PyYAML.\\nCreating Custom Corpora\\n48\\nSee also\\nIn the next recipes, we\\'ll cover various corpus readers, and then in the Lazy corpus loading \\nrecipe, we\\'ll use the LazyCorpusLoader, which expects corpus data to be in a corpora \\nsubdirectory of one of the paths speciied by nltk.data.path.\\nCreating a word list corpus\\nThe WordListCorpusReader is one of the simplest CorpusReader classes. It provides \\naccess to a ile containing a list of words, one word per line. In fact, you\\'ve already used it \\nwhen we used the stopwords corpus in the Filtering stopwords in a tokenized sentence and \\nDiscovering word collocations recipes in Chapter 1, Tokenizing Text and WordNet Basics.\\nGetting ready\\nWe need to start by creating a word list ile. This could be a single column CSV ile, or just a \\nnormal text ile with one word per line. Let\\'s create a ile named wordlist that looks like this:\\nnltk\\ncorpus\\ncorpora\\nwordnet\\nHow to do it...\\nNow we can instantiate a WordListCorpusReader that will produce a list of words from our \\nile. It takes two arguments: the directory path containing the iles, and a list of ilenames. If \\nyou open the Python console in the same directory as the iles, then \\'.\\' can be used as the \\ndirectory path. Otherwise, you must use a directory path such as: \\'nltk_data/corpora/\\ncookbook\\'.\\n>>> from nltk.corpus.reader import WordListCorpusReader\\n>>> reader = WordListCorpusReader(\\'.\\', [\\'wordlist\\'])\\n>>> reader.words()\\n[\\'nltk\\', \\'corpus\\', \\'corpora\\', \\'wordnet\\']\\n>>> reader.fileids()\\n[\\'wordlist\\']\\nChapter 3\\n49\\nHow it works...\\nWordListCorpusReader inherits from CorpusReader, which is a common base class for \\nall corpus readers. CorpusReader does all the work of identifying which iles to read, while \\nWordListCorpus reads the iles and tokenizes each line to produce a list of words. Here\\'s \\nan inheritance diagram:\\nWhen you call the words() function, it calls nltk.tokenize.line_tokenize() on the \\nraw ile data, which you can access using the raw() function.\\n>>> reader.raw()\\n\\'nltk\\\\ncorpus\\\\ncorpora\\\\nwordnet\\\\n\\'\\n>>> from nltk.tokenize import line_tokenize\\n>>> line_tokenize(reader.raw())\\n[\\'nltk\\', \\'corpus\\', \\'corpora\\', \\'wordnet\\']\\nThere\\'s more...\\nThe stopwords corpus is a good example of a multi-ile WordListCorpusReader. In \\nChapter 1, Tokenizing Text and WordNet Basics, in the Filtering stopwords in a tokenized \\nsentence recipe, we saw that it had one word list ile for each language, and you could access \\nthe words for that language by calling stopwords.words(fileid). If you want to create \\nyour own multi-ile word list corpus, this is a great example to follow.\\nNames corpus\\nAnother word list corpus that comes with NLTK is the names corpus. It contains two iles: \\nfemale.txt and male.txt, each containing a list of a few thousand common irst names \\norganized by gender.\\n>>> from nltk.corpus import names\\n>>> names.fileids()\\n[\\'female.txt\\', \\'male.txt\\']\\n>>> len(names.words(\\'female.txt\\'))\\n5001\\nwww.allitebooks.com\\nCreating Custom Corpora\\n50\\n>>> len(names.words(\\'male.txt\\'))\\n2943\\nEnglish words\\nNLTK also comes with a large list of English words. There\\'s one ile with 850 basic words, \\nand another list with over 200,000 known English words.\\n>>> from nltk.corpus import words\\n>>> words.fileids()\\n[\\'en\\', \\'en-basic\\']\\n>>> len(words.words(\\'en-basic\\'))\\n850\\n>>> len(words.words(\\'en\\'))\\n234936\\nSee also\\nIn Chapter 1, Tokenizing Text and WordNet Basics, the Filtering stopwords in a tokenized \\nsentence recipe, has more details on using the stopwords corpus. In the following recipes, \\nwe\\'ll cover more advanced corpus ile formats and corpus reader classes.\\nCreating a part-of-speech tagged word \\ncorpus\\nPart-of-speech tagging is the process of identifying the part-of-speech tag for a word. Most of \\nthe time, a tagger must irst be trained on a training corpus. How to train and use a tagger is \\ncovered in detail in Chapter 4, Part-of-Speech Tagging, but irst we must know how to create \\nand use a training corpus of part-of-speech tagged words.\\nGetting ready\\nThe simplest format for a tagged corpus is of the form \"word/tag\". Following is an excerpt from \\nthe brown corpus:\\nThe/at-tl expense/nn and/cc time/nn involved/vbn are/ber astronomical/\\njj ./.\\nEach word has a tag denoting its part-of-speech. For example, nn refers to a noun, while a tag \\nthat starts with vb is a verb.\\nChapter 3\\n51\\nHow to do it...\\nIf you were to put the previous excerpt into a ile called brown.pos, you could then create a \\nTaggedCorpusReader and do the following:\\n>>> from nltk.corpus.reader import TaggedCorpusReader\\n>>> reader = TaggedCorpusReader(\\'.\\', r\\'.*\\\\.pos\\')\\n>>> reader.words()\\n[\\'The\\', \\'expense\\', \\'and\\', \\'time\\', \\'involved\\', \\'are\\', ...]\\n>>> reader.tagged_words()\\n[(\\'The\\', \\'AT-TL\\'), (\\'expense\\', \\'NN\\'), (\\'and\\', \\'CC\\'), …]\\n>>> reader.sents()\\n[[\\'The\\', \\'expense\\', \\'and\\', \\'time\\', \\'involved\\', \\'are\\', \\'astronomical\\', \\n\\'.\\']]\\n>>> reader.tagged_sents()\\n[[(\\'The\\', \\'AT-TL\\'), (\\'expense\\', \\'NN\\'), (\\'and\\', \\'CC\\'), (\\'time\\', \\'NN\\'), \\n(\\'involved\\', \\'VBN\\'), (\\'are\\', \\'BER\\'), (\\'astronomical\\', \\'JJ\\'), (\\'.\\', \\n\\'.\\')]]\\n>>> reader.paras()\\n[[[\\'The\\', \\'expense\\', \\'and\\', \\'time\\', \\'involved\\', \\'are\\', \\'astronomical\\', \\n\\'.\\']]]\\n>>> reader.tagged_paras()\\n[[[(\\'The\\', \\'AT-TL\\'), (\\'expense\\', \\'NN\\'), (\\'and\\', \\'CC\\'), (\\'time\\', \\'NN\\'), \\n(\\'involved\\', \\'VBN\\'), (\\'are\\', \\'BER\\'), (\\'astronomical\\', \\'JJ\\'), (\\'.\\', \\n\\'.\\')]]]\\nHow it works...\\nThis time, instead of naming the ile explicitly, we use a regular expression, r\\'.*\\\\.pos\\', \\nto match all iles whose name ends with .pos. We could have done the same thing as we \\ndid with the WordListCorpusReader, and pass [\\'brown.pos\\'] as the second \\nargument, but this way you can see how to include multiple iles in a corpus without  \\nnaming each one explicitly.\\nCreating Custom Corpora\\n52\\nTaggedCorpusReader provides a number of methods for extracting text from a corpus. First, \\nyou can get a list of all words, or a list of tagged tokens. A tagged token is simply a tuple of \\n(word, tag). Next, you can get a list of every sentence, and also every tagged sentence, \\nwhere the sentence is itself a list of words or tagged tokens. Finally, you can get a list of \\nparagraphs, where each paragraph is a list of sentences, and each sentence is a list of words \\nor tagged tokens. Here\\'s an inheritance diagram listing all the major methods:\\nThere\\'s more...\\nThe functions demonstrated in the previous diagram all depend on tokenizers for splitting \\nthe text. TaggedCorpusReader tries to have good defaults, but you can customize them by \\npassing in your own tokenizers at initialization time.\\nCustomizing the word tokenizer\\nThe default word tokenizer is an instance of nltk.tokenize.WhitespaceTokenizer. If \\nyou want to use a different tokenizer, you can pass that in as word_tokenizer.\\n>>> from nltk.tokenize import SpaceTokenizer\\n>>> reader = TaggedCorpusReader(\\'.\\', r\\'.*\\\\.pos\\', word_\\ntokenizer=SpaceTokenizer())\\n>>> reader.words()\\n[\\'The\\', \\'expense\\', \\'and\\', \\'time\\', \\'involved\\', \\'are\\', ...]\\nChapter 3\\n53\\nCustomizing the sentence tokenizer\\nThe default sentence tokenizer is an instance of nltk.tokenize.RegexpTokenize \\nwith \\'\\\\n\\' to identify the gaps. It assumes that each sentence is on a line all by itself, and \\nindividual sentences do not have line breaks. To customize this, you can pass in your own \\ntokenizer as sent_tokenizer.\\n>>> from nltk.tokenize import LineTokenizer\\n>>> reader = TaggedCorpusReader(\\'.\\', r\\'.*\\\\.pos\\', sent_\\ntokenizer=LineTokenizer())\\n>>> reader.sents()\\n[[\\'The\\', \\'expense\\', \\'and\\', \\'time\\', \\'involved\\', \\'are\\', \\'astronomical\\', \\n\\'.\\']]\\nCustomizing the paragraph block reader\\nParagraphs are assumed to be split by blank lines. This is done with the default para_\\nblock_reader, which is nltk.corpus.reader.util.read_blankline_block. There \\nare a number of other block reader functions in nltk.corpus.reader.util, whose \\npurpose is to read blocks of text from a stream. Their usage will be covered in more detail in \\nthe later recipe, Creating a custom corpus view, where we\\'ll create a custom corpus reader.\\nCustomizing the tag separator\\nIf you don\\'t want to use \\'/\\' as the word/tag separator, you can pass an alternative string to \\nTaggedCorpusReader for sep. The default is sep=\\'/\\', but if you want to split words and \\ntags with \\'|\\', such as \\'word|tag\\', then you should pass in sep=\\'|\\'.\\nSimplifying tags with a tag mapping function\\nIf you\\'d like to somehow transform the part-of-speech tags, you can pass in a tag_mapping_\\nfunction at initialization, then call one of the tagged_* functions with simplify_\\ntags=True. Here\\'s an example where we lowercase each tag:\\n>>> reader = TaggedCorpusReader(\\'.\\', r\\'.*\\\\.pos\\', tag_mapping_\\nfunction=lambda t: t.lower())\\n>>> reader.tagged_words(simplify_tags=True)\\n[(\\'The\\', \\'at-tl\\'), (\\'expense\\', \\'nn\\'), (\\'and\\', \\'cc\\'), …]\\nCalling tagged_words() without simplify_tags=True would produce the same result as \\nif you did not pass in a tag_mapping_function.\\nThere are also a number of tag simpliication functions deined in nltk.tag.simplify. \\nThese can be useful for reducing the number of different part-of-speech tags.\\n>>> from nltk.tag import simplify\\n>>> reader = TaggedCorpusReader(\\'.\\', r\\'.*\\\\.pos\\', tag_mapping_\\nfunction=simplify.simplify_brown_tag)\\n>>> reader.tagged_words(simplify_tags=True)\\nCreating Custom Corpora\\n54\\n[(\\'The\\', \\'DET\\'), (\\'expense\\', \\'N\\'), (\\'and\\', \\'CNJ\\'), ...]\\n>>> reader = TaggedCorpusReader(\\'.\\', r\\'.*\\\\.pos\\', tag_mapping_\\nfunction=simplify.simplify_tag)\\n>>> reader.tagged_words(simplify_tags=True)\\n[(\\'The\\', \\'A\\'), (\\'expense\\', \\'N\\'), (\\'and\\', \\'C\\'), ...]\\nSee also\\nChapter 4, Part-of-Speech Tagging will cover part-of-speech tags and tagging in much more \\ndetail. And for more on tokenizers, see the irst three recipes of Chapter 1, Tokenizing Text \\nand WordNet Basics.\\nIn the next recipe, we\\'ll create a chunked phrase corpus, where each phrase is also \\npart-of-speech tagged.\\nCreating a chunked phrase corpus\\nA chunk is a short phrase within a sentence. If you remember sentence diagrams from grade \\nschool, they were a tree-like representation of phrases within a sentence. This is exactly what \\nchunks are: sub-trees within a sentence tree, and they will be covered in much more detail in \\nChapter 5, Extracting Chunks. Following is a sample sentence tree with three noun phrase \\n(NP) chunks shown as sub-trees.\\nThis recipe will cover how to create a corpus with sentences that contain chunks.\\nGetting ready\\nHere is an excerpt from the tagged treebank corpus. It has part-of-speech tags, as in \\nthe previous recipe, but it also has square brackets for denoting chunks. This is the same \\nsentence as in the previous tree diagram, but in text form:\\n[Earlier/JJR staff-reduction/NN moves/NNS] have/VBP trimmed/VBN about/\\nIN [300/CD jobs/NNS] ,/, [the/DT spokesman/NN] said/VBD ./.\\nIn this format, every chunk is a noun phrase. Words that are not within brackets are part of \\nthe sentence tree, but are not part of any noun phrase sub-tree.\\nChapter 3\\n55\\nHow to do it...\\nPut this excerpt into a ile called treebank.chunk, and then do the following:\\n>>> from nltk.corpus.reader import ChunkedCorpusReader\\n>>> reader = ChunkedCorpusReader(\\'.\\', r\\'.*\\\\.chunk\\')\\n>>> reader.chunked_words()\\n[Tree(\\'NP\\', [(\\'Earlier\\', \\'JJR\\'), (\\'staff-reduction\\', \\'NN\\'), (\\'moves\\', \\n\\'NNS\\')]), (\\'have\\', \\'VBP\\'), ...]\\n>>> reader.chunked_sents()\\n[Tree(\\'S\\', [Tree(\\'NP\\', [(\\'Earlier\\', \\'JJR\\'), (\\'staff-reduction\\', \\'NN\\'), \\n(\\'moves\\', \\'NNS\\')]), (\\'have\\', \\'VBP\\'), (\\'trimmed\\', \\'VBN\\'), (\\'about\\', \\n\\'IN\\'), Tree(\\'NP\\', [(\\'300\\', \\'CD\\'), (\\'jobs\\', \\'NNS\\')]), (\\',\\', \\',\\'), \\nTree(\\'NP\\', [(\\'the\\', \\'DT\\'), (\\'spokesman\\', \\'NN\\')]), (\\'said\\', \\'VBD\\'), \\n(\\'.\\', \\'.\\')])]\\n>>> reader.chunked_paras()\\n[[Tree(\\'S\\', [Tree(\\'NP\\', [(\\'Earlier\\', \\'JJR\\'), (\\'staff-reduction\\', \\n\\'NN\\'), (\\'moves\\', \\'NNS\\')]), (\\'have\\', \\'VBP\\'), (\\'trimmed\\', \\'VBN\\'), \\n(\\'about\\', \\'IN\\'), Tree(\\'NP\\', [(\\'300\\', \\'CD\\'), (\\'jobs\\', \\'NNS\\')]), (\\',\\', \\n\\',\\'), Tree(\\'NP\\', [(\\'the\\', \\'DT\\'), (\\'spokesman\\', \\'NN\\')]), (\\'said\\', \\n\\'VBD\\'), (\\'.\\', \\'.\\')])]]\\nThe ChunkedCorpusReader provides the same methods as the TaggedCorpusReader \\nfor getting tagged tokens, along with three new methods for getting chunks. Each chunk is \\nrepresented as an instance of nltk.tree.Tree. Sentence level trees look like Tree(\\'S\\', \\n[...]) while noun phrase trees look like Tree(\\'NP\\', [...]). In chunked_sents(), \\nyou get a list of sentence trees, with each noun-phrase as a sub-tree of the sentence. In \\nchunked_words(), you get a list of noun phrase trees alongside tagged tokens of words that \\nwere not in a chunk. Here\\'s an inheritance diagram listing the major methods:\\nCreating Custom Corpora\\n56\\nYou can draw a Tree by calling the draw() method. Using the corpus reader \\ndeined earlier, you could do reader.chunked_sents()[0].draw() to \\nget the same sentence tree diagram shown at the beginning of this recipe.\\nHow it works...\\nChunkedCorpusReader is similar to the TaggedCorpusReader from the last recipe. \\nIt has the same default sent_tokenizer and para_block_reader, but instead of a \\nword_tokenizer, it uses a str2chunktree() function. The default is nltk.chunk.\\nutil.tagstr2tree(), which parses a sentence string containing bracketed chunks into a \\nsentence tree, with each chunk as a noun phrase sub-tree. Words are split by whitespace, and \\nthe default word/tag separator is \\'/\\'. If you want to customize the chunk parsing, then you \\ncan pass in your own function for str2chunktree().\\nThere\\'s more...\\nAn alternative format for denoting chunks is called IOB tags. IOB tags are similar to part-of-\\nspeech tags, but provide a way to denote the inside, outside, and beginning of a chunk. They \\nalso have the beneit of allowing multiple different chunk phrase types, not just noun phrases. \\nHere is an excerpt from the conll2000 corpus. Each word is on its own line with a part-of-\\nspeech tag followed by an IOB tag.\\nMr. NNP B-NP\\nMeador NNP I-NP\\nhad VBD B-VP\\nbeen VBN I-VP\\nexecutive JJ B-NP\\nvice NN I-NP\\npresident NN I-NP\\nof IN B-PP\\nBalcor NNP B-NP\\n. . O\\nB-NP denotes the beginning of a noun phrase, while I-NP denotes that the word is inside of \\nthe current noun phrase. B-VP and I-VP denote the beginning and inside of a verb phrase. O \\nends the sentence.\\nTo read a corpus using the IOB format, you must use the ConllChunkCorpusReader. Each \\nsentence is separated by a blank line, but there is no separation for paragraphs. This means \\nthat the para_* methods are not available. If you put the previous IOB example text into a ile \\nnamed conll.iob, you can create and use a ConllChunkCorpusReader with the code we \\nare about to see. The third argument to ConllChunkCorpusReader should be a tuple or list \\nspecifying the types of chunks in the ile, which in this case is (\\'NP\\', \\'VP\\', \\'PP\\').\\nChapter 3\\n57\\n>>> from nltk.corpus.reader import ConllChunkCorpusReader\\n>>> conllreader = ConllChunkCorpusReader(\\'.\\', r\\'.*\\\\.iob\\', (\\'NP\\', \\n\\'VP\\', \\'PP\\'))\\n>>> conllreader.chunked_words()\\n[Tree(\\'NP\\', [(\\'Mr.\\', \\'NNP\\'), (\\'Meador\\', \\'NNP\\')]), Tree(\\'VP\\', \\n[(\\'had\\', \\'VBD\\'), (\\'been\\', \\'VBN\\')]), ...]\\n>>> conllreader.chunked_sents()\\n[Tree(\\'S\\', [Tree(\\'NP\\', [(\\'Mr.\\', \\'NNP\\'), (\\'Meador\\', \\'NNP\\')]), \\nTree(\\'VP\\', [(\\'had\\', \\'VBD\\'), (\\'been\\', \\'VBN\\')]), Tree(\\'NP\\', \\n[(\\'executive\\', \\'JJ\\'), (\\'vice\\', \\'NN\\'), (\\'president\\', \\'NN\\')]), \\nTree(\\'PP\\', [(\\'of\\', \\'IN\\')]), Tree(\\'NP\\', [(\\'Balcor\\', \\'NNP\\')]), (\\'.\\', \\n\\'.\\')])]\\n>>> conllreader.iob_words()\\n[(\\'Mr.\\', \\'NNP\\', \\'B-NP\\'), (\\'Meador\\', \\'NNP\\', \\'I-NP\\'), ...]\\n>>> conllreader.iob_sents()\\n[[(\\'Mr.\\', \\'NNP\\', \\'B-NP\\'), (\\'Meador\\', \\'NNP\\', \\'I-NP\\'), (\\'had\\', \\n\\'VBD\\', \\'B-VP\\'), (\\'been\\', \\'VBN\\', \\'I-VP\\'), (\\'executive\\', \\'JJ\\', \\'B-\\nNP\\'), (\\'vice\\', \\'NN\\', \\'I-NP\\'), (\\'president\\', \\'NN\\', \\'I-NP\\'), (\\'of\\', \\n\\'IN\\', \\'B-PP\\'), (\\'Balcor\\', \\'NNP\\', \\'B-NP\\'), (\\'.\\', \\'.\\', \\'O\\')]]\\nThe previous code also shows the iob_words() and iob_sents() methods, which \\nreturn lists of three tuples of (word, pos, iob). The inheritance diagram for \\nConllChunkCorpusReader looks like the following, with most of the methods implemented \\nby its superclass, ConllCorpusReader:\\nCreating Custom Corpora\\n58\\nTree leaves\\nWhen it comes to chunk trees, the leaves of a tree are the tagged tokens. So if you want to get \\na list of all the tagged tokens in a tree, call the leaves() method.\\n>>> reader.chunked_words()[0].leaves()\\n[(\\'Earlier\\', \\'JJR\\'), (\\'staff-reduction\\', \\'NN\\'), (\\'moves\\', \\'NNS\\')]\\n>>> reader.chunked_sents()[0].leaves()\\n[(\\'Earlier\\', \\'JJR\\'), (\\'staff-reduction\\', \\'NN\\'), (\\'moves\\', \\'NNS\\'), \\n(\\'have\\', \\'VBP\\'), (\\'trimmed\\', \\'VBN\\'), (\\'about\\', \\'IN\\'), (\\'300\\', \\n\\'CD\\'), (\\'jobs\\', \\'NNS\\'), (\\',\\', \\',\\'), (\\'the\\', \\'DT\\'), (\\'spokesman\\', \\n\\'NN\\'), (\\'said\\', \\'VBD\\'), (\\'.\\', \\'.\\')]\\n>>> reader.chunked_paras()[0][0].leaves()\\n[(\\'Earlier\\', \\'JJR\\'), (\\'staff-reduction\\', \\'NN\\'), (\\'moves\\', \\'NNS\\'), \\n(\\'have\\', \\'VBP\\'), (\\'trimmed\\', \\'VBN\\'), (\\'about\\', \\'IN\\'), (\\'300\\', \\n\\'CD\\'), (\\'jobs\\', \\'NNS\\'), (\\',\\', \\',\\'), (\\'the\\', \\'DT\\'), (\\'spokesman\\', \\n\\'NN\\'), (\\'said\\', \\'VBD\\'), (\\'.\\', \\'.\\')]\\nTreebank chunk corpus\\nThe nltk.corpus.treebank_chunk corpus uses ChunkedCorpusReader to provide \\npart-of-speech tagged words and noun phrase chunks of Wall Street Journal headlines.  \\nNLTK comes with a 5% sample from the Penn Treebank Project. You can ind out more at \\nhttp://www.cis.upenn.edu/~treebank/home.html.\\nCoNLL2000 corpus\\nCoNLL stands for the Conference on Computational Natural Language Learning. For the \\nyear 2000 conference, a shared task was undertaken to produce a corpus of chunks based \\non the Wall Street Journal corpus. In addition to noun phrases (NP), it also contains verb \\nphrases (VP) and prepositional phrases (PP). This chunked corpus is available as nltk.\\ncorpus.conll2000, which is an instance of ConllChunkCorpusReader. You can read \\nmore at http://www.cnts.ua.ac.be/conll2000/chunking/.\\nSee also\\nChapter 5, Extracting Chunks will cover chunk extraction in detail. Also see the previous \\nrecipe for details on getting tagged tokens from a corpus reader.\\nCreating a categorized text corpus\\nIf you have a large corpus of text, you may want to categorize it into separate sections. The \\nbrown corpus, for example, has a number of different categories.\\n>>> from nltk.corpus import brown\\n>>> brown.categories()\\nChapter 3\\n59\\n[\\'adventure\\', \\'belles_lettres\\', \\'editorial\\', \\'fiction\\', \\n\\'government\\', \\'hobbies\\', \\'humor\\', \\'learned\\', \\'lore\\', \\'mystery\\', \\n\\'news\\', \\'religion\\', \\'reviews\\', \\'romance\\', \\'science_fiction\\']\\nIn this recipe, we\\'ll learn how to create our own categorized text corpus.\\nGetting ready\\nThe easiest way to categorize a corpus is to have one ile for each category. Following are two \\nexcerpts from the movie_reviews corpus:\\nmovie_pos.txt\\nthe thin red line is flawed but it provokes .\\nmovie_neg.txt\\na big-budget and glossy production can not make up for a lack of \\nspontaneity that permeates their tv show .\\nWith these two iles, we\\'ll have two categories: pos and neg.\\nHow to do it...\\nWe\\'ll use the CategorizedPlaintextCorpusReader, which inherits from both \\nPlaintextCorpusReader and CategorizedCorpusReader. These two superclasses \\nrequire three arguments: the root directory, the fileids, and a category speciication.\\n>>> from nltk.corpus.reader import \\nCategorizedPlaintextCorpusReader\\n>>> reader = CategorizedPlaintextCorpusReader(\\'.\\', r\\'movie_.*\\\\.\\ntxt\\', cat_pattern=r\\'movie_(\\\\w+)\\\\.txt\\')\\n>>> reader.categories()\\n[\\'neg\\', \\'pos\\']\\n>>> reader.fileids(categories=[\\'neg\\'])\\n[\\'movie_neg.txt\\']\\n>>> reader.fileids(categories=[\\'pos\\'])\\n[\\'movie_pos.txt\\']\\nwww.allitebooks.com\\nCreating Custom Corpora\\n60\\nHow it works...\\nThe irst two arguments to CategorizedPlaintextCorpusReader are the root \\ndirectory and fileids, which are passed on to the PlaintextCorpusReader to read \\nin the iles. The cat_pattern keyword argument is a regular expression for extracting the \\ncategory names from the fileids. In our case, the category is the part of the fileid after \\nmovie_ and before .txt. The category must be surrounded by grouping parenthesis.\\ncat_pattern is passed to CategorizedCorpusReader, which overrides the common \\ncorpus reader functions such as fileids(), words(), sents(), and paras() to accept \\na categories keyword argument. This way, you could get all the pos sentences by calling \\nreader.sents(categories=[\\'pos\\']). CategorizedCorpusReader also provides \\nthe categories() function, which returns a list of all known categories in the corpus.\\nCategorizedPlaintextCorpusReader is an example of using multiple-inheritance to join \\nmethods from multiple superclasses, as shown in the following diagram:\\nThere\\'s more...\\nInstead of cat_pattern, you could pass in a cat_map, which is a dictionary mapping \\na fileid to a list of category labels.\\n>>> reader = CategorizedPlaintextCorpusReader(\\'.\\', r\\'movie_.*\\\\.\\ntxt\\', cat_map={\\'movie_pos.txt\\': [\\'pos\\'], \\'movie_neg.txt\\': \\n[\\'neg\\']})\\n>>> reader.categories()\\n[\\'neg\\', \\'pos\\']\\nChapter 3\\n61\\nCategory ile\\nA third way of specifying categories is to use the cat_file keyword argument to specify a \\nilename containing a mapping of fileid to category. For example, the brown corpus has a \\nile called cats.txt that looks like this:\\nca44 news\\ncb01 editorial\\nThe reuters corpus has iles in multiple categories, and its cats.txt looks like this:\\ntest/14840 rubber coffee lumber palm-oil veg-oil\\ntest/14841 wheat grain\\nCategorized tagged corpus reader\\nThe brown corpus reader is actually an instance of CategorizedTaggedCorpusReader, \\nwhich inherits from CategorizedCorpusReader and TaggedCorpusReader. \\nJust like in CategorizedPlaintextCorpusReader, it overrides all the methods of \\nTaggedCorpusReader to allow a categories argument, so you can call brown.\\ntagged_sents(categories=[\\'news\\']) to get all the tagged sentences from \\nthe news category. You can use the CategorizedTaggedCorpusReader just like \\nCategorizedPlaintextCorpusReader for your own categorized and tagged text corpora.\\nCategorized corpora\\nThe movie_reviews corpus reader is an instance of \\nCategorizedPlaintextCorpusReader, as is the reuters corpus reader. But where the \\nmovie_reviews corpus only has two categories (neg and pos), reuters has 90 categories. \\nThese corpora are often used for training and evaluating classiiers, which will be covered in \\nChapter 7, Text Classiication.\\nSee also\\nIn the next recipe, we\\'ll create a subclass of CategorizedCorpusReader and \\nChunkedCorpusReader for reading a categorized chunk corpus. Also see Chapter 7, \\nText Classiication in which we use categorized text for classiication.\\nCreating a categorized chunk corpus reader\\nNLTK provides a CategorizedPlaintextCorpusReader and \\nCategorizedTaggedCorpusReader, but there\\'s no categorized corpus reader for chunked \\ncorpora. So in this recipe, we\\'re going to make one.\\nCreating Custom Corpora\\n62\\nGetting ready\\nRefer to the earlier recipe, Creating a chunked phrase corpus, for an explanation \\nof ChunkedCorpusReader, and to the previous recipe for details on \\nCategorizedPlaintextCorpusReader and CategorizedTaggedCorpusReader, \\nboth of which inherit from CategorizedCorpusReader.\\nHow to do it...\\nWe\\'ll create a class called CategorizedChunkedCorpusReader that inherits from both \\nCategorizedCorpusReader and ChunkedCorpusReader. It is heavily based on the \\nCategorizedTaggedCorpusReader, and also provides three additional methods for \\ngetting categorized chunks. The following code is found in catchunked.py:\\nfrom nltk.corpus.reader import CategorizedCorpusReader, \\nChunkedCorpusReader\\nclass CategorizedChunkedCorpusReader(CategorizedCorpusReader, \\nChunkedCorpusReader):\\n  def __init__(self, *args, **kwargs):\\n    CategorizedCorpusReader.__init__(self, kwargs)\\n    ChunkedCorpusReader.__init__(self, *args, **kwargs)\\n  def _resolve(self, fileids, categories):\\n    if fileids is not None and categories is not None:\\n      raise ValueError(\\'Specify fileids or categories, not both\\')\\n    if categories is not None:\\n      return self.fileids(categories)\\n    else:\\n      return fileids\\nAll of the following methods call the corresponding function in ChunkedCorpusReader with \\nthe value returned from _resolve(). We\\'ll start with the plain text methods.\\n  def raw(self, fileids=None, categories=None):\\n    return ChunkedCorpusReader.raw(self, self._resolve(fileids, \\ncategories))\\n  \\n  def words(self, fileids=None, categories=None):\\n    return ChunkedCorpusReader.words(self, self._resolve(fileids, \\ncategories))\\n  \\n  def sents(self, fileids=None, categories=None):\\n    return ChunkedCorpusReader.sents(self, self._resolve(fileids, \\ncategories))\\n  \\n  def paras(self, fileids=None, categories=None):\\nChapter 3\\n63\\n    return ChunkedCorpusReader.paras(self, self._resolve(fileids, \\ncategories))\\nNext comes the tagged text methods.\\n  def tagged_words(self, fileids=None, categories=None, simplify_\\ntags=False):\\n    return ChunkedCorpusReader.tagged_words(\\n      self, self._resolve(fileids, categories), simplify_tags)\\n  \\n  def tagged_sents(self, fileids=None, categories=None, simplify_\\ntags=False):\\n    return ChunkedCorpusReader.tagged_sents(\\n      self, self._resolve(fileids, categories), simplify_tags)\\n    \\n  def tagged_paras(self, fileids=None, categories=None, simplify_\\ntags=False):\\n    return ChunkedCorpusReader.tagged_paras(\\n      self, self._resolve(fileids, categories), simplify_tags)\\nAnd inally, the chunked methods, which is what we\\'ve really been after.\\n  def chunked_words(self, fileids=None, categories=None):\\n    return ChunkedCorpusReader.chunked_words(\\n      self, self._resolve(fileids, categories))\\n  \\n  def chunked_sents(self, fileids=None, categories=None):\\n    return ChunkedCorpusReader.chunked_sents(\\n      self, self._resolve(fileids, categories))\\n  \\n  def chunked_paras(self, fileids=None, categories=None):\\n    return ChunkedCorpusReader.chunked_paras(\\n      self, self._resolve(fileids, categories))\\nAll these methods together give us a complete CategorizedChunkedCorpusReader.\\nCreating Custom Corpora\\n64\\nHow it works...\\nCategorizedChunkedCorpusReader overrides all the ChunkedCorpusReader \\nmethods to take a categories argument for locating fileids. These fileids \\nare found with the internal _resolve() function. This _resolve() function makes \\nuse of CategorizedCorpusReader.fileids() to return fileids for a given list \\nof categories. If no categories are given, _resolve() just returns the given \\nfileids, which could be None, in which case all iles are read. The initialization of both \\nCategorizedCorpusReader and ChunkedCorpusReader is what makes this all possible. \\nIf you look at the code for CategorizedTaggedCorpusReader, you\\'ll see it\\'s very similar. \\nThe inheritance diagram looks like this:\\nHere\\'s some example code for using the treebank corpus. All we\\'re doing is making \\ncategories out of the fileids, but the point is that you could use the same techniques \\nto create your own categorized chunk corpus.\\n>>> import nltk.data\\n>>> from catchunked import CategorizedChunkedCorpusReader\\n>>> path = nltk.data.find(\\'corpora/treebank/tagged\\')\\n>>> reader = CategorizedChunkedCorpusReader(path, r\\'wsj_.*\\\\.pos\\', \\ncat_pattern=r\\'wsj_(.*)\\\\.pos\\')\\n>>> len(reader.categories()) == len(reader.fileids())\\nTrue\\n>>> len(reader.chunked_sents(categories=[\\'0001\\']))\\n16\\nChapter 3\\n65\\nWe use nltk.data.find() to search the data directories to get a \\nFileSystemPathPointer to the treebank corpus. All the treebank tagged iles start \\nwith wsj_ followed by a number, and end with .pos. The previous code turns that ile number \\ninto a category.\\nThere\\'s more...\\nAs covered in the Creating a chunked phrase corpus recipe, there\\'s an alternative format and \\nreader for a chunk corpus using IOB tags. To have a categorized corpus of IOB chunks, we \\nhave to make a new corpus reader.\\nCategorized Conll chunk corpus reader\\nHere\\'s a subclass of CategorizedCorpusReader and ConllChunkReader \\ncalled CategorizedConllChunkCorpusReader. It overrides all methods of \\nConllCorpusReader that take a fileids argument, so the methods can also take \\na categories argument. The ConllChunkCorpusReader is just a small subclass \\nof ConllCorpusReader that handles initialization; most of the work is done in \\nConllCorpusReader. This code can also be found in catchunked.py.\\nfrom nltk.corpus.reader import CategorizedCorpusReader, \\nConllCorpusReader, ConllChunkCorpusReader\\nclass CategorizedConllChunkCorpusReader(CategorizedCorpusReader, \\nConllChunkCorpusReader):\\n  def __init__(self, *args, **kwargs):\\n    CategorizedCorpusReader.__init__(self, kwargs)\\n    ConllChunkCorpusReader.__init__(self, *args, **kwargs)\\n  \\n  def _resolve(self, fileids, categories):\\n    if fileids is not None and categories is not None:\\n      raise ValueError(\\'Specify fileids or categories, not both\\')\\n    if categories is not None:\\n      return self.fileids(categories)\\n    else:\\n      return fileids\\nAll the following methods call the corresponding method of ConllCorpusReader with the \\nvalue returned from _resolve(). We\\'ll start with the plain text methods.\\n  def raw(self, fileids=None, categories=None):\\n    return ConllCorpusReader.raw(self, self._resolve(fileids, \\ncategories))\\n  \\n  def words(self, fileids=None, categories=None):\\nCreating Custom Corpora\\n66\\n    return ConllCorpusReader.words(self, self._resolve(fileids, \\ncategories))\\n  \\n  def sents(self, fileids=None, categories=None):\\n    return ConllCorpusReader.sents(self, self._resolve(fileids, \\ncategories))\\nThe ConllCorpusReader does not recognize paragraphs, so there are no *_paras() \\nmethods. Next are the tagged and chunked methods.\\n  def tagged_words(self, fileids=None, categories=None):\\n    return ConllCorpusReader.tagged_words(self, self._\\nresolve(fileids, categories))\\n  \\n  def tagged_sents(self, fileids=None, categories=None):\\n    return ConllCorpusReader.tagged_sents(self, self._\\nresolve(fileids, categories))\\n  \\n  def chunked_words(self, fileids=None, categories=None, chunk_\\ntypes=None):\\n    return ConllCorpusReader.chunked_words(\\n      self, self._resolve(fileids, categories), chunk_types)\\n  \\n  def chunked_sents(self, fileids=None, categories=None, chunk_\\ntypes=None):\\n    return ConllCorpusReader.chunked_sents(\\n      self, self._resolve(fileids, categories), chunk_types)\\nFor completeness, we must override the following methods of the ConllCorpusReader:\\n  def parsed_sents(self, fileids=None, categories=None, pos_in_\\ntree=None):\\n    return ConllCorpusReader.parsed_sents(\\n      self, self._resolve(fileids, categories), pos_in_tree)\\n  \\n  def srl_spans(self, fileids=None, categories=None):\\n    return ConllCorpusReader.srl_spans(self, self._\\nresolve(fileids, categories))\\n  \\n  def srl_instances(self, fileids=None, categories=None, pos_in_\\ntree=None, flatten=True):\\n    return ConllCorpusReader.srl_instances(\\n      self, self._resolve(fileids, categories), pos_in_tree, \\nflatten)\\n  \\nChapter 3\\n67\\n  def iob_words(self, fileids=None, categories=None):\\n    return ConllCorpusReader.iob_words(self, self._\\nresolve(fileids, categories))\\n  \\n  def iob_sents(self, fileids=None, categories=None):\\n    return ConllCorpusReader.iob_sents(self, self._\\nresolve(fileids, categories))\\nThe inheritance diagram for this class is as follows:\\nFollowing is some example code using the conll2000 corpus. Like with treebank, we\\'re \\nusing the fileids for categories. The ConllChunkCorpusReader requires a third \\nargument to specify the chunk_types. These chunk_types are used to parse the IOB \\ntags. As you learned in the Creating a chunked phrase corpus recipe, the conll2000 corpus \\nrecognizes three chunk types:\\n \\nf\\nNP for noun phrases\\n \\nf\\nVP for verb phrases\\n \\nf\\nPP for prepositional phrases\\n>>> import nltk.data\\n>>> from catchunked import CategorizedConllChunkCorpusReader\\nCreating Custom Corpora\\n68\\n>>> path = nltk.data.find(\\'corpora/conll2000\\')\\n>>> reader = CategorizedConllChunkCorpusReader(path, r\\'.*\\\\.txt\\', \\n(\\'NP\\',\\'VP\\',\\'PP\\'), cat_pattern=r\\'(.*)\\\\.txt\\')\\n>>> reader.categories()\\n[\\'test\\', \\'train\\']\\n>>> reader.fileids()\\n[\\'test.txt\\', \\'train.txt\\']\\n>>> len(reader.chunked_sents(categories=[\\'test\\']))\\n2012\\nSee also\\nIn the Creating a chunked phrase corpus recipe in this chapter, we covered both \\nthe ChunkedCorpusReader and ConllChunkCorpusReader. And in the \\nprevious recipe, we covered CategorizedPlaintextCorpusReader and \\nCategorizedTaggedCorpusReader, which share the same superclass used by \\nCategorizedChunkedCorpusReader and CategorizedConllChunkReader—\\nCategorizedCorpusReader.\\nLazy corpus loading\\nLoading a corpus reader can be an expensive operation due to the number of iles, ile sizes, \\nand various initialization tasks. And while you\\'ll often want to specify a corpus reader in a \\ncommon module, you don\\'t always need to access it right away. To speed up module import \\ntime when a corpus reader is deined, NLTK provides a LazyCorpusLoader class that can \\ntransform itself into your actual corpus reader as soon as you need it. This way, you can deine \\na corpus reader in a common module without it slowing down module loading.\\nHow to do it...\\nLazyCorpusLoader requires two arguments: the name of the corpus and the corpus \\nreader class, plus any other arguments needed to initialize the corpus reader class.\\nThe name argument speciies the root directory name of the corpus, which must be within a \\ncorpora subdirectory of one of the paths in nltk.data.path. See the irst recipe of this \\nchapter, Setting up a custom corpus, for more details on nltk.data.path.\\nFor example, if you have a custom corpora named cookbook in your local nltk_data \\ndirectory, its path would be ~/nltk_data/corpora/cookbook. You\\'d then pass \\n\\'cookbook\\' to LazyCorpusLoader as the name, and LazyCorpusLoader will look in \\n~/nltk_data/corpora for a directory named \\'cookbook\\'.\\nChapter 3\\n69\\nThe second argument to LazyCorpusLoader is reader_cls, which should be the name \\nof a subclass of CorpusReader, such as WordListCorpusReader. You will also need \\nto pass in any other arguments required by the reader_cls for initialization. This will be \\ndemonstrated as follows, using the same wordlist ile we created in the earlier recipe, \\nCreating a word list corpus. The third argument to LazyCorpusLoader is the list of \\nilenames and fileids that will be passed in to WordListCorpusReader at initialization.\\n>>> from nltk.corpus.util import LazyCorpusLoader\\n>>> from nltk.corpus.reader import WordListCorpusReader\\n>>> reader = LazyCorpusLoader(\\'cookbook\\', WordListCorpusReader, \\n[\\'wordlist\\'])\\n>>> isinstance(reader, LazyCorpusLoader)\\nTrue\\n>>> reader.fileids()\\n[\\'wordlist\\']\\n>>> isinstance(reader, LazyCorpusLoader)\\nFalse\\n>>> isinstance(reader, WordListCorpusReader)\\nTrue\\nHow it works...\\nLazyCorpusLoader stores all the arguments given, but otherwise does nothing until you try \\nto access an attribute or method. This way initialization is very fast, eliminating the overhead \\nof loading the corpus reader immediately. As soon as you do access an attribute or method, it \\ndoes the following:\\n1. Calls nltk.data.find(\\'corpora/%s\\' % name) to ind the corpus data \\nroot directory.\\n2. Instantiate the corpus reader class with the root directory and any other arguments.\\n3. Transforms itself into the corpus reader class.\\nSo in the previous example code, before we call reader.fileids(), reader is \\nan instance of LazyCorpusLoader, but after the call, reader is an instance of \\nWordListCorpusReader.\\nThere\\'s more...\\nAll of the corpora included with NLTK and deined in nltk.corpus are initially an instance of \\nLazyCorpusLoader. Here\\'s some code from nltk.corpus deining the treebank corpora.\\ntreebank = LazyCorpusLoader(\\n    \\'treebank/combined\\', BracketParseCorpusReader, r\\'wsj_.*\\\\.mrg\\',\\nCreating Custom Corpora\\n70\\n    tag_mapping_function=simplify_wsj_tag)\\ntreebank_chunk = LazyCorpusLoader(\\n    \\'treebank/tagged\\', ChunkedCorpusReader, r\\'wsj_.*\\\\.pos\\',\\n    sent_tokenizer=RegexpTokenizer(r\\'(?<=/\\\\.)\\\\s*(?![^\\\\[]*\\\\])\\', \\ngaps=True),\\n    para_block_reader=tagged_treebank_para_block_reader)\\ntreebank_raw = LazyCorpusLoader(\\n    \\'treebank/raw\\', PlaintextCorpusReader, r\\'wsj_.*\\')\\nAs you can see, any number of additional arguments can be passed through by \\nLazyCorpusLoader to its reader_cls.\\nCreating a custom corpus view\\nA corpus view is a class wrapper around a corpus ile that reads in blocks of tokens as \\nneeded. Its purpose is to provide a view into a ile without reading the whole ile at once (since \\ncorpus iles can often be quite large). If the corpus readers included by NLTK already meet \\nall your needs, then you do not have to know anything about corpus views. But, if you have a \\ncustom ile format that needs special handling, this recipe will show you how to create and \\nuse a custom corpus view. The main corpus view class is StreamBackedCorpusView, which \\nopens a single ile as a stream, and maintains an internal cache of blocks it has read.\\nBlocks of tokens are read in with a block reader function. A block can be any piece of text, \\nsuch as a paragraph or a line, and tokens are parts of a block, such as individual words. \\nIn the Creating a part-of-speech tagged word corpus recipe, we discussed the default \\npara_block_reader function of the TaggedCorpusReader, which reads lines from \\na ile until it inds a blank line, then returns those lines as a single paragraph token. The \\nactual block reader function is: nltk.corpus.reader.util.read_blankline_block. \\nTaggedCorpusReader passes this block reader function into a TaggedCorpusView \\nwhenever it needs to read blocks from a ile. TaggedCorpusView is a subclass of \\nStreamBackedCorpusView that knows to split paragraphs of \"word/tag\" into (word, \\ntag) tuples.\\nHow to do it...\\nWe\\'ll start with the simple case of a plain text ile with a heading that should be ignored by the \\ncorpus reader. Let\\'s make a ile called heading_text.txt that looks like this:\\nA simple heading\\nHere is the actual text for the corpus.\\nParagraphs are split by blanklines.\\nThis is the 3rd paragraph.\\nChapter 3\\n71\\nNormally we\\'d use the PlaintextCorpusReader but, by default, it will treat A simple \\nheading as the irst paragraph. To ignore this heading, we need to subclass the \\nPlaintextCorpusReader so we can override its CorpusView class variable with our \\nown StreamBackedCorpusView subclass. This code is found in corpus.py.\\nfrom nltk.corpus.reader import PlaintextCorpusReader\\nfrom nltk.corpus.reader.util import StreamBackedCorpusView\\nclass IgnoreHeadingCorpusView(StreamBackedCorpusView):\\n  def __init__(self, *args, **kwargs):\\n    StreamBackedCorpusView.__init__(self, *args, **kwargs)\\n    # open self._stream\\n    self._open()\\n    # skip the heading block\\n    self.read_block(self._stream)\\n    # reset the start position to the current position in the \\nstream\\n    self._filepos = [self._stream.tell()]\\nclass IgnoreHeadingCorpusReader(PlaintextCorpusReader):\\n  CorpusView = IgnoreHeadingCorpusView\\nTo demonstrate that this works as expected, here\\'s the code showing that the default  \\nPlaintextCorpusReader inds four paragraphs, while our IgnoreHeadingCorpusReader \\nonly has three paragraphs.\\n>>> from nltk.corpus.reader import PlaintextCorpusReader\\n>>> plain = PlaintextCorpusReader(\\'.\\', [\\'heading_text.txt\\'])\\n>>> len(plain.paras())\\n4\\n>>> from corpus import IgnoreHeadingCorpusReader\\n>>> reader = IgnoreHeadingCorpusReader(\\'.\\', [\\'heading_text.txt\\'])\\n>>> len(reader.paras())\\n3\\nHow it works...\\nThe PlaintextCorpusReader by design has a CorpusView class variable that can be \\noverridden by subclasses. So we do just that, and make our IgnoreHeadingCorpusView \\nthe CorpusView.\\nCreating Custom Corpora\\n72\\nMost corpus readers do not have a CorpusView class variable because they \\nrequire very speciic corpus views.\\nThe IgnoreHeadingCorpusView is a subclass of StreamBackedCorpusView that does \\nthe following on initialization:\\n1. Open the ile using self._open(). This function is deined by \\nStreamBackedCorpusView, and sets the internal instance variable \\nself._stream to the opened ile.\\n2. Read one block with read_blankline_block(), which will read the heading \\nas a paragraph, and move the stream\\'s ile position forward to the next block.\\n3. Reset the start ile position to the current position of self._stream. self._\\nfilepos is an internal index of where each block is in the ile.\\nHere\\'s a diagram illustrating the relationships between the classes:\\nThere\\'s more...\\nCorpus views can get a lot fancier and more complicated, but the core concept is the same: \\nread blocks from a stream to return a list of tokens. There are a number of block readers \\nprovided in nltk.corpus.reader.util, but you can always create your own. If you \\ndo want to deine your own block reader function, then you have two choices on how to \\nimplement it:\\n1. Deine it as a separate function and pass it in to StreamBackedCorpusView as \\nblock_reader. This is a good option if your block reader is fairly simple, reusable, \\nand doesn\\'t require any outside variables or coniguration.\\nChapter 3\\n73\\n2. Subclass StreamBackedCorpusView and override the read_block() method. \\nThis is what many custom corpus views do because the block reading is highly \\nspecialized and requires additional functions and coniguration, usually provided by \\nthe corpus reader when the corpus view is initialized.\\nBlock reader functions\\nFollowing is a survey of most of the included block readers in nltk.corpus.reader.util. \\nUnless otherwise noted, each block reader function takes a single argument: the stream to \\nread from.\\n \\nf\\nread_whitespace_block() will read 20 lines from the stream, splitting each line \\ninto tokens by whitespace.\\n \\nf\\nread_wordpunct_block() reads 20 lines from the stream, splitting each line \\nusing nltk.tokenize.wordpunct_tokenize().\\n \\nf\\nread_line_block() reads 20 lines from the stream and returns them as a list, \\nwith each line as a token.\\n \\nf\\nread_blankline_block() will read lines from the stream until it inds a blank \\nline. It will then return a single token of all lines found combined into a single string.\\n \\nf\\nread_regexp_block() takes two additional arguments, which must be regular \\nexpressions that can be passed to re.match(): a start_re and end_re. start_\\nre matches the starting line of a block, and end_re matches the ending line of the \\nblock. end_re defaults to None, in which case the block will end as soon as a new \\nstart_re match is found. The return value is a single token of all lines in the block \\njoined into a single string.\\nPickle corpus view\\nIf you want to have a corpus of pickled objects, you can use the PickleCorpusView, a \\nsubclass of StreamBackedCorpusView found in nltk.corpus.reader.util. A ile \\nconsists of blocks of pickled objects, and can be created with the PickleCorpusView.\\nwrite() class method, which takes a sequence of objects and an output ile, then pickles \\neach object using pickle.dump() and writes it to the ile. It overrides the read_block() \\nmethod to return a list of unpickled objects from the stream, using pickle.load().\\nCreating Custom Corpora\\n74\\nConcatenated corpus view\\nAlso found in nltk.corpus.reader.util is the ConcatenatedCorpusView. This class \\nis useful if you have multiple iles that you want a corpus reader to treat as a single ile. A \\nConcatenatedCorpusView is created by giving it a list of corpus_views, which are then \\niterated over as if they were a single view.\\nSee also\\nThe concept of block readers was introduced in the Creating a part-of-speech tagged word \\ncorpus recipe in this chapter.\\nCreating a MongoDB backed corpus reader\\nAll the corpus readers we\\'ve dealt with so far have been ile-based. That is in part due to the \\ndesign of the CorpusReader base class, and also the assumption that most corpus data will \\nbe in text iles. But sometimes you\\'ll have a bunch of data stored in a database that you want \\nto access and use just like a text ile corpus. In this recipe, we\\'ll cover the case where you \\nhave documents in MongoDB, and you want to use a particular ield of each document as your \\nblock of text.\\nGetting ready\\nMongoDB is a document-oriented database that has become a popular alternative to \\nrelational databases such as MySQL. The installation and setup of MongoDB is outside the \\nscope of this book, but you can ind instructions at http://www.mongodb.org/display/\\nDOCS/Quickstart.\\nYou\\'ll also need to install PyMongo, a Python driver for MongoDB. You should be able to do \\nthis with either easy_install or pip, by doing sudo easy_install pymongo or sudo \\npip install pymongo.\\nThe code in the How to do it... section assumes that your database is on localhost \\nport 27017, which is the MongoDB default coniguration, and that you\\'ll be using the \\ntest database with a collection named corpus that contains documents with a text \\nield. Explanations for these arguments are available in the PyMongo documentation at \\nhttp://api.mongodb.org/python/.\\nChapter 3\\n75\\nHow to do it...\\nSince the CorpusReader class assumes you have a ile-based corpus, we can\\'t directly \\nsubclass it. Instead, we\\'re going to emulate both the StreamBackedCorpusView and \\nPlaintextCorpusReader. StreamBackedCorpusView is a subclass of nltk.util.\\nAbstractLazySequence, so we\\'ll subclass AbstractLazySequence to create a \\nMongoDB view, and then create a new class that will use the view to provide functionality \\nsimilar to the PlaintextCorpusReader. This code is found in mongoreader.py.\\nimport pymongo\\nfrom nltk.data import LazyLoader\\nfrom nltk.tokenize import TreebankWordTokenizer\\nfrom nltk.util import AbstractLazySequence, LazyMap, \\nLazyConcatenation\\nclass MongoDBLazySequence(AbstractLazySequence):\\n  def __init__(self, host=\\'localhost\\', port=27017, db=\\'test\\', \\ncollection=\\'corpus\\', field=\\'text\\'):\\n    self.conn = pymongo.Connection(host, port)\\n    self.collection = self.conn[db][collection]\\n    self.field = field\\n  def __len__(self):\\n    return self.collection.count()\\n  def iterate_from(self, start):\\n    f = lambda d: d.get(self.field, \\'\\')\\n    return iter(LazyMap(f, self.collection.find(fields=[self.\\nfield], skip=start)))\\nclass MongoDBCorpusReader(object):\\n  def __init__(self, word_tokenizer=TreebankWordTokenizer(),\\n         sent_tokenizer=LazyLoader(\\'tokenizers/punkt/english.\\npickle\\'),\\n         **kwargs):\\n    self._seq = MongoDBLazySequence(**kwargs)\\n    self._word_tokenize = word_tokenizer.tokenize\\n    self._sent_tokenize = sent_tokenizer.tokenize\\n  def text(self):\\n    return self._seq\\n  def words(self):\\n    return LazyConcatenation(LazyMap(self._word_tokenize, self.\\ntext()))\\n  def sents(self):\\n    return LazyConcatenation(LazyMap(self._sent_tokenize, self.\\ntext()))\\nCreating Custom Corpora\\n76\\nHow it works...\\nAbstractLazySequence is an abstract class that provides read-only, on-demand \\niteration. Subclasses must implement the __len__() and iterate_from(start) \\nmethods, while it provides the rest of the list and iterator emulation methods. By creating \\nthe MongoDBLazySequence subclass as our view, we can iterate over documents in the \\nMongoDB collection on-demand, without keeping all the documents in memory. LazyMap \\nis a lazy version of Python\\'s built-in map() function, and is used in iterate_from() to \\ntransform the document into the speciic ield that we\\'re interested in. It\\'s also a subclass of \\nAbstractLazySequence.\\nThe MongoDBCorpusReader creates an internal instance of MongoDBLazySequence for \\niteration, then deines the word and sentence tokenization methods. The text() method \\nsimply returns the instance of MongoDBLazySequence, which results in a lazily evaluated list \\nof each text ield. The words() method uses LazyMap and LazyConcatenation to return \\na lazily evaluated list of all words, while the sents() method does the same for sentences. \\nThe sent_tokenizer is loaded on demand with LazyLoader, which is a wrapper around \\nnltk.data.load(), analogous to LazyCorpusLoader. LazyConcatentation is a \\nsubclass of AbstractLazySequence too, and produces a lat list from a given list of lists \\n(each list may also be lazy). In our case, we\\'re concatenating the results of LazyMap to ensure \\nwe don\\'t return nested lists.\\nThere\\'s more...\\nAll of the parameters are conigurable. For example, if you had a db named website, with a \\ncollection named comments, whose documents had a field called comment, you could \\ncreate a MongoDBCorpusReader as follows:\\n>>> reader = MongoDBCorpusReader(db=\\'website\\', \\ncollection=\\'comments\\', field=\\'comment\\')\\nYou can also pass in custom instances for word_tokenizer and sent_tokenizer, as \\nlong as the objects implement the nltk.tokenize.TokenizerI interface by providing a \\ntokenize(text) method.\\nSee also\\nCorpus views were covered in the previous recipe, and tokenization was covered in Chapter 1, \\nTokenizing Text and WordNet Basics.\\nChapter 3\\n77\\nCorpus editing with ile locking\\nCorpus readers and views are all read-only, but there may be times when you want to add to \\nor edit the corpus iles. However, modifying a corpus ile while other processes are using it, \\nsuch as through a corpus reader, can lead to dangerous undeined behavior. This is where ile \\nlocking comes in handy.\\nGetting ready\\nYou must install the lockfile library using sudo easy_install lockfile or sudo pip \\ninstall lockfile. This library provides cross-platform ile locking, and so will work on \\nWindows, Unix/Linux, Mac OX, and more. You can ind detailed documentation on lockfile \\nat http://packages.python.org/lockfile/.\\nFor the following code to work, you must also have Python 2.6. Versions 2.4 and earlier do not \\nsupport the with keyword.\\nHow to do it...\\nHere are two ile editing functions: append_line() and remove_line(). Both try to \\nacquire an exclusive lock on the ile before updating it. An exclusive lock means that these \\nfunctions will wait until no other process is reading from or writing to the ile. Once the lock \\nis acquired, any other process that tries to access the ile will have to wait until the lock is \\nreleased. This way, modifying the ile will be safe and not cause any undeined behavior in \\nother processes. These functions can be found in corpus.py.\\nimport lockfile, tempfile, shutil\\ndef append_line(fname, line):\\n  with lockfile.FileLock(fname):\\n    fp = open(fname, \\'a+\\')\\n    fp.write(line)\\n    fp.write(\\'\\\\n\\')\\n    fp.close()\\ndef remove_line(fname, line):\\n  with lockfile.FileLock(fname):\\n    tmp = tempfile.TemporaryFile()\\n    fp = open(fname, \\'r+\\')\\n    # write all lines from orig file, except if matches given line\\n    for l in fp:\\n      if l.strip() != line:\\n        tmp.write(l)\\n    \\nCreating Custom Corpora\\n78\\n    # reset file pointers so entire files are copied\\n    fp.seek(0)\\n    tmp.seek(0)\\n    # copy tmp into fp, then truncate to remove trailing line(s)\\n    shutil.copyfileobj(tmp, fp)\\n    fp.truncate()\\n    fp.close()\\n    tmp.close()\\nThe lock acquiring and releasing happens transparently when you do with lockfile.\\nFileLock(fname).\\nInstead of using with lockfile.FileLock(fname), you can also get a \\nlock by calling lock = lockfile.FileLock(fname), then call lock.\\nacquire() to acquire the lock, and lock.release() to release the lock. \\nThis alternative usage is compatible with Python 2.4.\\nHow it works...\\nYou can use these functions as follows:\\n>>> from corpus import append_line, remove_line\\n>>> append_line(\\'test.txt\\', \\'foo\\')\\n>>> remove_line(\\'test.txt\\', \\'foo\\')\\nIn append_line(), a lock is acquired, the ile is opened in append mode, the text is written \\nalong with an end-of-line character, and then the ile is closed, releasing the lock.\\nA lock acquired by lockfile only protects the ile from other processes that \\nalso use lockfile. In other words, just because your Python process has \\na lock with lockfile, doesn\\'t mean a non-Python process can\\'t modify the \\nile. For this reason, it\\'s best to only use lockfile with iles that will not \\nbe edited by any non-Python processes, or Python processes that do not use \\nlockfile.\\nChapter 3\\n79\\nThe remove_line() function is a bit more complicated. Because we\\'re removing a line and \\nnot a speciic section of the ile, we need to iterate over the ile to ind each instance of the \\nline to remove. The easiest way to do this while writing the changes back to the ile, is to use \\na TemporaryFile to hold the changes, then copy that ile back into the original ile using \\nshutil.copyfileobj().\\nThese functions are best suited for a word list corpus, or some other corpus type with \\npresumably unique lines, that may be edited by multiple people at about the same time,  \\nsuch as through a web interface. Using these functions with a more document-oriented \\ncorpus such as brown, treebank, or conll2000, is probably a bad idea.\\n4\\nPart-of-Speech \\nTagging\\nIn this chapter, we will cover:\\n \\nf\\nDefault tagging\\n \\nf\\nTraining a unigram part-of-speech tagger\\n \\nf\\nCombining taggers with backoff tagging\\n \\nf\\nTraining and combining Ngram taggers\\n \\nf\\nCreating a model of likely word tags\\n \\nf\\nTagging with regular expressions\\n \\nf\\nAfix tagging\\n \\nf\\nTraining a Brill tagger\\n \\nf\\nTraining the TnT tagger\\n \\nf\\nUsing WordNet for tagging\\n \\nf\\nTagging proper names\\n \\nf\\nClassiier-based tagging\\nPart-of-Speech Tagging\\n82\\nIntroduction\\nPart-of-speech tagging is the process of converting a sentence, in the form of a list of \\nwords, into a list of tuples, where each tuple is of the form (word, tag). The tag is a \\npart-of-speech tag and signiies whether the word is a noun, adjective, verb, and so on.\\nMost of the taggers we will cover are trainable. They use a list of tagged sentences \\nas their training data, such as what you get from the tagged_sents() function of a \\nTaggedCorpusReader (see the Creating a part-of-speech tagged word corpus recipe in \\nChapter 3, Creating Custom Corpora for more details). With these training sentences, the \\ntagger generates an internal model that will tell them how to tag a word. Other taggers use \\nexternal data sources or match word patterns to choose a tag for a word.\\nAll taggers in NLTK are in the nltk.tag package and inherit from the TaggerI base \\nclass. TaggerI requires all subclasses to implement a tag() method, which takes a list \\nof words as input, and returns a list of tagged words as output. TaggerI also provides an \\nevaluate() method for evaluating the accuracy of the tagger (covered at the end of the \\nDefault tagging recipe). Many taggers can also be combined into a backoff chain, so that if \\none tagger cannot tag a word, the next tagger is used, and so on.\\nPart-of-speech tagging is a necessary step before chunking, which is covered in Chapter 5, \\nExtracting Chunks. Without the part-of-speech tags, a chunker cannot know how to extract \\nphrases from a sentence. But with part-of-speech tags, you can tell a chunker how to identify \\nphrases based on tag patterns.\\nDefault tagging\\nDefault tagging provides a baseline for part-of-speech tagging. It simply assigns the same \\npart-of-speech tag to every token. We do this using the DefaultTagger.\\nGetting ready\\nWe are going to use the treebank corpus for most of this chapter because it\\'s a common \\nstandard and is quick to load and test. But everything we do should apply equally well to \\nbrown, conll2000, and any other part-of-speech tagged corpus.\\nHow to do it...\\nThe DefaultTagger takes a single argument—the tag you want to apply. We will give it \\'NN\\', \\nwhich is the tag for a singular noun.\\n>>> from nltk.tag import DefaultTagger\\n>>> tagger = DefaultTagger(\\'NN\\')\\nChapter 4\\n83\\n>>> tagger.tag([\\'Hello\\', \\'World\\'])\\n[(\\'Hello\\', \\'NN\\'), (\\'World\\', \\'NN\\')]\\nEvery tagger has a tag() method that takes a list of tokens, where each token is a single \\nword. This list of tokens is usually a list of words produced by a word tokenizer (see Chapter 1, \\nTokenizing Text and WordNet Basics for more on tokenization). As you can see, tag() returns \\na list of tagged tokens, where a tagged token is a tuple of (word, tag).\\nHow it works...\\nDefaultTagger is a subclass of SequentialBackoffTagger. Every subclass of \\nSequentialBackoffTagger must implement the choose_tag() method, which takes \\nthree arguments:\\n1. The list of tokens.\\n2. The index of the current token whose tag we want to choose.\\n3. The history, which is a list of the previous tags.\\nSequentialBackoffTagger implements the tag() method, which calls the \\nchoose_tag() of the subclass for each index in the tokens list, while accumulating a \\nhistory of the previously tagged tokens. This history is the reason for the Sequential in \\nSequentialBackoffTagger. We will get to the Backoff portion of the name in the \\nCombining taggers with backoff tagging recipe. The following is a diagram showing the \\ninheritance tree:\\nThe choose_tag() method of DefaultTagger is very simple—it returns the tag we gave it \\nat initialization time. It does not care about the current token or the history.\\nPart-of-Speech Tagging\\n84\\nThere\\'s more...\\nThere are a lot of different tags you could give to the DefaultTagger. You can ind a \\ncomplete list of possible tags for the treebank corpus at http://www.ling.upenn.\\nedu/courses/Fall_2003/ling001/penn_treebank_pos.html. These tags are also \\ndocumented in Appendix, Penn Treebank Part-of-Speech Tags.\\nEvaluating accuracy\\nTo know how accurate a tagger is, you can use the evaluate() method, which takes a list \\nof tagged tokens as a gold standard to evaluate the tagger. Using our default tagger created \\nearlier, we can evaluate it against a subset of the treebank corpus tagged sentences.\\n>>> from nltk.corpus import treebank\\n>>> test_sents = treebank.tagged_sents()[3000:]\\n>>> tagger.evaluate(test_sents)\\n0.14331966328512843\\nSo by just choosing \\'NN\\' for every tag, we can achieve 14% accuracy testing on ¼th of the \\ntreebank corpus. We will be reusing these same test_sents for evaluating more taggers \\nin upcoming recipes.\\nBatch tagging sentences\\nTaggerI also implements a batch_tag() method that can be used to tag a list of \\nsentences, instead of a single sentence. Here\\'s an example of tagging two simple sentences:\\n>>> tagger.batch_tag([[\\'Hello\\', \\'world\\', \\'.\\'], [\\'How\\', \\'are\\', \\'you\\', \\n\\'?\\']])\\n[[(\\'Hello\\', \\'NN\\'), (\\'world\\', \\'NN\\'), (\\'.\\', \\'NN\\')], [(\\'How\\', \\'NN\\'), \\n(\\'are\\', \\'NN\\'), (\\'you\\', \\'NN\\'), (\\'?\\', \\'NN\\')]]\\nThe result is a list of two tagged sentences, and of course every tag is NN because we are \\nusing the DefaultTagger. The batch_tag() method can be quite useful if you have many \\nsentences you wish to tag all at once.\\nUntagging a tagged sentence\\nTagged sentences can be untagged using nltk.tag.untag(). Calling this function with a \\ntagged sentence will return a list of words without the tags.\\n>>> from nltk.tag import untag\\n>>> untag([(\\'Hello\\', \\'NN\\'), (\\'World\\', \\'NN\\')])\\n[\\'Hello\\', \\'World\\']\\nChapter 4\\n85\\nSee also\\nFor more on tokenization, see Chapter 1, Tokenizing Text and WordNet Basics. And to learn \\nmore about tagged sentences, see the Creating a part-of-speech tagged word corpus recipe \\nin Chapter 3, Creating Custom Corpora. For a complete list of part-of-speech tags found in \\nthe treebank corpus, see Appendix, Penn Treebank Part-of-Speech Tags.\\nTraining a unigram part-of-speech tagger\\nA unigram generally refers to a single token. Therefore, a unigram tagger only uses a single \\nword as its context for determining the part-of-speech tag.\\nThe UnigramTagger inherits from NgramTagger, which is a subclass of ContextTagger, \\nwhich inherits from SequentialBackoffTagger. In other words, the UnigramTagger is a \\ncontext-based tagger whose context is a single word, or unigram.\\nHow to do it...\\nUnigramTagger can be trained by giving it a list of tagged sentences at initialization.\\n>>> from nltk.tag import UnigramTagger\\n>>> from nltk.corpus import treebank\\n>>> train_sents = treebank.tagged_sents()[:3000]\\n>>> tagger = UnigramTagger(train_sents)\\n>>> treebank.sents()[0]\\n[\\'Pierre\\', \\'Vinken\\', \\',\\', \\'61\\', \\'years\\', \\'old\\', \\',\\', \\'will\\', \\'join\\', \\n\\'the\\', \\'board\\', \\'as\\', \\'a\\', \\'nonexecutive\\', \\'director\\', \\'Nov.\\', \\'29\\', \\n\\'.\\']\\n>>> tagger.tag(treebank.sents()[0])\\n[(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\'), (\\',\\', \\',\\'), (\\'61\\', \\'CD\\'), \\n(\\'years\\', \\'NNS\\'), (\\'old\\', \\'JJ\\'), (\\',\\', \\',\\'), (\\'will\\', \\'MD\\'), (\\'join\\', \\n\\'VB\\'), (\\'the\\', \\'DT\\'), (\\'board\\', \\'NN\\'), (\\'as\\', \\'IN\\'), (\\'a\\', \\'DT\\'), \\n(\\'nonexecutive\\', \\'JJ\\'), (\\'director\\', \\'NN\\'), (\\'Nov.\\', \\'NNP\\'), (\\'29\\', \\n\\'CD\\'), (\\'.\\', \\'.\\')]\\nWe use the irst 3,000 tagged sentences of the treebank corpus as the training set to \\ninitialize the UnigramTagger. Then we see the irst sentence as a list of words, and can \\nsee how it is transformed by the tag() function into a list of tagged tokens.\\nPart-of-Speech Tagging\\n86\\nHow it works...\\nThe UnigramTagger builds a context model from the list of tagged sentences. Because \\nUnigramTagger inherits from ContextTagger, instead of providing a choose_tag() \\nmethod, it must implement a context() method, which takes the same three arguments as \\nchoose_tag(). The result of context() is, in this case, the word token. The context token \\nis used to create the model, and also to look up the best tag once the model is created. Here\\'s \\nan inheritance diagram showing each class, starting at SequentialBackoffTagger:\\nLet\\'s see how accurate the UnigramTagger is on the test sentences (see the previous \\nrecipe for how test_sents is created).\\n>>> tagger.evaluate(test_sents)\\n0.85763004532700193\\nIt has almost 86% accuracy for a tagger that only uses single word lookup to determine  \\nthe part-of-speech tag. All accuracy gains from here on will be much smaller.\\nThere\\'s more...\\nThe model building is actually implemented in ContextTagger. Given the list of tagged \\nsentences, it calculates the frequency that a tag has occurred for each context. The tag  \\nwith the highest frequency for a context is stored in the model.\\nChapter 4\\n87\\nOverriding the context model\\nAll taggers that inherit from ContextTagger can take a pre-built model instead of training \\ntheir own. This model is simply a Python dict mapping a context key to a tag. The context \\nkeys will depend on what the ContextTagger subclass returns from its context() \\nmethod. For UnigramTagger, context keys are individual words. But for other NgramTagger \\nsubclasses, the context keys will be tuples.\\nHere\\'s an example where we pass a very simple model to the UnigramTagger instead of a \\ntraining set:\\n>>> tagger = UnigramTagger(model={\\'Pierre\\': \\'NN\\'})\\n>>> tagger.tag(treebank.sents()[0])\\n[(\\'Pierre\\', \\'NN\\'), (\\'Vinken\\', None), (\\',\\', None), (\\'61\\', None), \\n(\\'years\\', None), (\\'old\\', None), (\\',\\', None), (\\'will\\', None), (\\'join\\', \\nNone), (\\'the\\', None), (\\'board\\', None), (\\'as\\', None), (\\'a\\', None), \\n(\\'nonexecutive\\', None),(\\'director\\', None), (\\'Nov.\\', None), (\\'29\\', \\nNone), (\\'.\\', None)]\\nSince the model only contained the context key, \\'Pierre\\', only the irst word got a tag. \\nEvery other word got None as the tag since the context word was not in the model. So unless \\nyou know exactly what you are doing, let the tagger train its own model instead of passing in \\nyour own.\\nOne good case for passing a self-created model to the UnigramTagger is for when you \\nhave a dictionary of words and tags, and you know that every word should always map to its \\ntag. Then, you can put this UnigramTagger as your irst backoff tagger (covered in the next \\nrecipe), to look up tags for unambiguous words.\\nMinimum frequency cutoff\\nThe ContextTagger uses frequency of occurrence to decide which tag is most likely for a \\ngiven context. By default, it will do this even if the context word and tag occurs only once. If \\nyou would like to set a minimum frequency threshold, then you can pass a cutoff value to \\nthe UnigramTagger.\\n>>> tagger = UnigramTagger(train_sents, cutoff=3)\\n>>> tagger.evaluate(test_sents)\\n0.775350744657889\\nIn this case, using cutoff=3 has decreased accuracy, but there may be times when a cutoff \\nis a good idea.\\nPart-of-Speech Tagging\\n88\\nSee also\\nIn the next recipe, we will cover backoff tagging to combine taggers. And in the Creating a \\nmodel of likely word tags recipe, we will learn how to statistically determine tags for very \\ncommon words.\\nCombining taggers with backoff tagging\\nBackoff tagging is one of the core features of SequentialBackoffTagger. It allows you \\nto chain taggers together so that if one tagger doesn\\'t know how to tag a word, it can pass the \\nword on to the next backoff tagger. If that one can\\'t do it, it can pass the word on to the next \\nbackoff tagger, and so on until there are no backoff taggers left to check.\\nHow to do it...\\nEvery subclass of SequentialBackoffTagger can take a backoff keyword argument \\nwhose value is another instance of a SequentialBackoffTagger. So we will use the \\nDefaultTagger from the Default tagging recipe as the backoff to the UnigramTagger \\nfrom the Training a unigram part-of-speech tagger recipe. Refer to both recipes for details on \\ntrain_sents and test_sents.\\n>>> tagger1 = DefaultTagger(\\'NN\\')\\n>>> tagger2 = UnigramTagger(train_sents, backoff=tagger1)\\n>>> tagger2.evaluate(test_sents)\\n0.87459529462551266\\nBy using a default tag of NN whenever the UnigramTagger is unable to tag a word, we have \\nincreased the accuracy by almost 2%!\\nHow it works...\\nWhen a SequentialBackoffTagger is initialized, it creates an internal list of backoff \\ntaggers with itself as the irst element. If a backoff tagger is given, then the backoff tagger\\'s \\ninternal list of taggers is appended. Here\\'s some code to illustrate this:\\n>>> tagger1._taggers == [tagger1]\\nTrue\\n>>> tagger2._taggers == [tagger2, tagger1]\\nTrue\\nChapter 4\\n89\\nThe _taggers is the internal list of backoff taggers that the SequentialBackoffTagger \\nuses when the tag() method is called. It goes through its list of taggers, calling choose_\\ntag() on each one. As soon as a tag is found, it stops and returns that tag. This means that \\nif the primary tagger can tag the word, then that\\'s the tag that will be returned. But if it returns \\nNone, then the next tagger is tried, and so on until a tag is found, or else None is returned. Of \\ncourse, None will never be returned if your inal backoff tagger is a DefaultTagger.\\nThere\\'s more...\\nWhile most of the taggers included in NLTK are subclasses of SequentialBackoffTagger, \\nnot all of them are. There\\'s a few taggers that we will cover in later recipes that cannot be \\nused as part of a backoff tagging chain, such as the BrillTagger. However, these taggers \\ngenerally take another tagger to use as a baseline, and a SequentialBackoffTagger is \\noften a good choice for that baseline.\\nPickling and unpickling a trained tagger\\nSince training a tagger can take a while, and you generally only need to do the training once, \\npickling a trained tagger is a useful way to save it for later usage. If your trained tagger is \\ncalled tagger, then here\\'s how to dump and load it with pickle:\\n>>> import pickle\\n>>> f = open(\\'tagger.pickle\\', \\'w\\')\\n>>> pickle.dump(tagger, f)\\n>>> f.close()\\n>>> f = open(\\'tagger.pickle\\', \\'r\\')\\n>>> tagger = pickle.load(f)\\nIf your tagger pickle ile is located in a NLTK data directory, you could also use nltk.data.\\nload(\\'tagger.pickle\\') to load the tagger.\\nSee also\\nIn the next recipe, we will combine more taggers with backoff tagging. Also see the previous \\ntwo recipes for details on the DefaultTagger and UnigramTagger.\\nTraining and combining Ngram taggers\\nIn addition to UnigramTagger, there are two more NgramTagger subclasses: \\nBigramTagger and TrigramTagger. BigramTagger uses the previous tag as part of \\nits context, while TrigramTagger uses the previous two tags. An ngram is a subsequence \\nof n items, so the BigramTagger looks at two items (the previous tag and word), and the \\nTrigramTagger looks at three items.\\nPart-of-Speech Tagging\\n90\\nThese two taggers are good at handling words whose part-of-speech tag is context dependent. \\nMany words have a different part-of-speech depending on how they are used. For example, \\nwe have been talking about taggers that \"tag\" words. In this case, \"tag\" is used as a verb. But \\nthe result of tagging is a part-of-speech tag, so \"tag\" can also be a noun. The idea with the \\nNgramTagger subclasses is that by looking at the previous words and part-of-speech tags, \\nwe can better guess the part-of-speech tag for the current word.\\nGetting ready\\nRefer to the irst two recipes of this chapter for details on constructing train_sents and \\ntest_sents.\\nHow to do it...\\nBy themselves, BigramTagger and TrigramTagger perform quite poorly. This is partly \\nbecause they cannot learn context from the irst word(s) in a sentence.\\n>>> from nltk.tag import BigramTagger, TrigramTagger\\n>>> bitagger = BigramTagger(train_sents)\\n>>> bitagger.evaluate(test_sents)\\n0.11336067342974315\\n>>> tritagger = TrigramTagger(train_sents)\\n>>> tritagger.evaluate(test_sents)\\n0.0688107058061731\\nWhere they can make a contribution is when we combine them with backoff tagging. This \\ntime, instead of creating each tagger individually, we will create a function that will take \\ntrain_sents, a list of SequentialBackoffTagger classes, and an optional inal backoff \\ntagger, and then train each tagger with the previous tagger as a backoff. Here\\'s code from \\ntag_util.py:\\ndef backoff_tagger(train_sents, tagger_classes, backoff=None):\\n  for cls in tagger_classes:\\n    backoff = cls(train_sents, backoff=backoff)\\n  return backoff\\nAnd to use it, we can do the following:\\n>>> from tag_util import backoff_tagger\\n>>> backoff = DefaultTagger(\\'NN\\')\\n>>> tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, \\nTrigramTagger], backoff=backoff)\\n>>> tagger.evaluate(test_sents)\\n0.88163177206993304\\nChapter 4\\n91\\nSo we have gained almost 1% accuracy by including the BigramTagger and \\nTrigramTagger in the backoff chain. For corpora other than treebank, the accuracy \\ngain may be more signiicant.\\nHow it works...\\nThe backoff_tagger function creates an instance of each tagger class in the list, giving it \\nthe train_sents and the previous tagger as a backoff. The order of the list of tagger classes \\nis quite important—the irst class in the list will be trained irst, and be given the initial backoff \\ntagger. This tagger will then become the backoff tagger for the next tagger class in the list. The \\ninal tagger returned will be an instance of the last tagger class in the list. Here\\'s some code \\nto clarify this chain:\\n>>> tagger._taggers[-1] == backoff\\nTrue\\n>>> isinstance(tagger._taggers[0], TrigramTagger)\\nTrue\\n>>> isinstance(tagger._taggers[1], BigramTagger)\\nTrue\\nSo we end up with a TrigramTagger, whose irst backoff is a BigramTagger. Then the next \\nbackoff will be a UnigramTagger, whose backoff is the DefaultTagger.\\nThere\\'s more...\\nThe backoff_tagger function doesn\\'t just work with NgramTagger classes. It can be used \\nfor constructing a chain containing any subclasses of SequentialBackoffTagger.\\nBigramTagger and TrigramTagger, because they are subclasses of NgramTagger and \\nContextTagger, can also take a model and cutoff argument, just like the UnigramTagger. \\nBut unlike for UnigramTagger, the context keys of the model must be 2-tuples, where the \\nirst element is a section of the history, and the second element is the current token. For \\nthe BigramTagger, an appropriate context key looks like ((prevtag,), word), and for \\nTrigramTagger it looks like ((prevtag1, prevtag2), word).\\nQuadgram Tagger\\nThe NgramTagger class can be used by itself to create a tagger that uses  Ngrams longer \\nthan three for its context key.\\n>>> from nltk.tag import NgramTagger\\n>>> quadtagger = NgramTagger(4, train_sents)\\n>>> quadtagger.evaluate(test_sents)\\n0.058191236779624435\\nPart-of-Speech Tagging\\n92\\nIt\\'s even worse than the TrigramTagger! Here\\'s an alternative implementation of a \\nQuadgramTagger that we can include in a list to backoff_tagger. This code can be \\nfound in taggers.py:\\nfrom nltk.tag import NgramTagger\\nclass QuadgramTagger(NgramTagger):\\n  def __init__(self, *args, **kwargs):\\n    NgramTagger.__init__(self, 4, *args, **kwargs)\\nThis is essentially how BigramTagger and TrigramTagger are implemented; simple \\nsubclasses of NgramTagger that pass in the number of ngrams to look at in the history \\nargument of the context() method.\\nNow let\\'s see how it does as part of a backoff chain:\\n>>> from taggers import QuadgramTagger\\n>>> quadtagger = backoff_tagger(train_sents, [UnigramTagger, \\nBigramTagger, TrigramTagger, QuadgramTagger], backoff=backoff)\\n>>> quadtagger.evaluate(test_sents)\\n0.88111374919058927\\nIt\\'s actually slightly worse than before when we stopped with the TrigramTagger. So the \\nlesson is that too much context can have a negative effect on accuracy.\\nSee also\\nThe previous two recipes cover the UnigramTagger and backoff tagging.\\nCreating a model of likely word tags\\nAs mentioned earlier in this chapter in the Training a unigram part-of-speech tagger recipe, \\nusing a custom model with a UnigramTagger should only be done if you know exactly what \\nyou are doing. In this recipe, we are going to create a model for the most common words, \\nmost of which always have the same tag no matter what.\\nChapter 4\\n93\\nHow to do it...\\nTo ind the most common words, we can use nltk.probability.FreqDist to count word \\nfrequencies in the treebank corpus. Then, we can create a ConditionalFreqDist for \\ntagged words, where we count the frequency of every tag for every word. Using these counts, \\nwe can construct a model of the 200 most frequent words as keys, with the most frequent tag \\nfor each word as a value. Here\\'s the model creation function deined in tag_util.py:\\nfrom nltk.probability import FreqDist, ConditionalFreqDist\\ndef word_tag_model(words, tagged_words, limit=200):\\n  fd = FreqDist(words)\\n  most_freq = fd.keys()[:limit]\\n  cfd = ConditionalFreqDist(tagged_words)\\n  return dict((word, cfd[word].max()) for word in most_freq)\\nAnd to use it with a UnigramTagger, we can do the following:\\n>>> from tag_util import word_tag_model\\n>>> from nltk.corpus import treebank\\n>>> model = word_tag_model(treebank.words(), treebank.tagged_words())\\n>>> tagger = UnigramTagger(model=model)\\n>>> tagger.evaluate(test_sents)\\n0.55972372113101665\\nAn accuracy of almost 56% is ok, but nowhere near as good as the trained UnigramTagger. \\nLet\\'s try adding it to our backoff chain:\\n>>> default_tagger = DefaultTagger(\\'NN\\')\\n>>> likely_tagger = UnigramTagger(model=model, backoff=default_tagger)\\n>>> tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, \\nTrigramTagger], backoff=likely_tagger)\\n>>> tagger.evaluate(test_sents)\\n0.88163177206993304\\nThe inal accuracy is exactly the same as without the likely_tagger. This is because the \\nfrequency calculations we did to create the model are almost exactly what happens when we \\ntrain a UnigramTagger.\\nPart-of-Speech Tagging\\n94\\nHow it works...\\nThe word_tag_model() function takes a list of all words, a list of all tagged words, and \\nthe maximum number of words we want to use for our model. We give the list of words to a \\nFreqDist, which counts the frequency of each word. Then we get the top 200 words from \\nthe FreqDist by calling fd.keys(), which returns all words ordered by highest frequency \\nto lowest. We give the list of tagged words to a ConditionalFreqDist, which creates a \\nFreqDist of tags for each word, with the word as the condition. Finally, we return a dict of \\nthe top 200 words mapped to their most likely tag.\\nThere\\'s more...\\nIt may seem useless to include this tagger as it does not change the accuracy. But the point of \\nthis recipe is to demonstrate how to construct a useful model for a UnigramTagger. Custom \\nmodel construction is a way to create a manual override of trained taggers that are otherwise \\nblack boxes. And by putting the likely tagger in the front of the chain, we can actually improve \\naccuracy a little bit:\\n>>> tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, \\nTrigramTagger], backoff=default_tagger)\\n>>> likely_tagger = UnigramTagger(model=model, backoff=tagger)\\n>>> likely_tagger.evaluate(test_sents)\\n0.88245197496222749\\nPutting custom model taggers at the front of the backoff chain gives you complete control  \\nover how speciic words are tagged, while letting the trained taggers handle everything else.\\nSee also\\nThe Training a unigram part-of-speech tagger recipe has details on the UnigramTagger \\nand a simple custom model example. See the earlier recipes Combining taggers with \\nbackoff tagging and Training and combining Ngram taggers for details on backoff tagging.\\nTagging with regular expressions\\nYou can use regular expression matching to tag words. For example, you can match \\nnumbers with \\\\d to assign the tag CD (which refers to a Cardinal number). Or you could \\nmatch on known word patterns, such as the sufix \"ing\". There\\'s lot of lexibility here, but  \\nbe careful of over-specifying since language is naturally inexact, and there are always \\nexceptions to the rule.\\nChapter 4\\n95\\nGetting ready\\nFor this recipe to make sense, you should be familiar with regular expression syntax and \\nPython\\'s re module.\\nHow to do it...\\nThe RegexpTagger expects a list of 2-tuples, where the irst element in the tuple is a \\nregular expression, and the second element is the tag. The following patterns can be  \\nfound in tag_util.py:\\npatterns = [\\n  (r\\'^\\\\d+$\\', \\'CD\\'),\\n  (r\\'.*ing$\\', \\'VBG\\'), # gerunds, i.e. wondering\\n  (r\\'.*ment$\\', \\'NN\\'), # i.e. wonderment\\n  (r\\'.*ful$\\', \\'JJ\\') # i.e. wonderful\\n]\\nOnce you have constructed this list of patterns, you can pass it into RegexpTagger.\\n>>> from tag_util import patterns\\n>>> from nltk.tag import RegexpTagger\\n>>> tagger = RegexpTagger(patterns)\\n>>> tagger.evaluate(test_sents)\\n0.037470321605870924\\nSo it\\'s not too great with just a few patterns, but since RegexpTagger is a subclass of \\nSequentialBackoffTagger, it can be useful as part of a backoff chain, especially if you \\nare able to come up with more word patterns.\\nHow it works...\\nThe RegexpTagger saves the patterns given at initialization, then on each call to choose_\\ntag(), it iterates over the patterns and returns the tag for the irst expression that matches \\nthe current word using re.match(). This means that if you have two expressions that could \\nmatch, the tag of the irst one will always be returned, and the second expression won\\'t even \\nbe tried.\\nThere\\'s more...\\nThe RegexpTagger can replace the DefaultTagger if you give it a pattern such as \\n(r\\'.*\\', \\'NN\\'). This pattern should, of course, be last in the list of patterns, otherwise \\nno other patterns will match.\\nPart-of-Speech Tagging\\n96\\nSee also\\nIn the next recipe, we will cover the AffixTagger, which learns how to tag based on preixes \\nand sufixes of words. And see the Default tagging recipe for details on the DefaultTagger.\\nAfix tagging\\nThe AffixTagger is another ContextTagger subclass, but this time the context is either \\nthe preix or the sufix of a word. This means the AffixTagger is able to learn tags based on \\nixed-length substrings of the beginning or ending of a word.\\nHow to do it...\\nThe default arguments for an AffixTagger specify three-character sufixes, and that words \\nmust be at least ive characters long. If a word is less than ive characters long, then None is \\nreturned as the tag.\\n>>> from nltk.tag import AffixTagger\\n>>> tagger = AffixTagger(train_sents)\\n>>> tagger.evaluate(test_sents)\\n0.27528599179797109\\nSo it does ok by itself with the default arguments. Let\\'s try it by specifying three-character \\npreixes:\\n>>> prefix_tagger = AffixTagger(train_sents, affix_length=3)\\n>>> prefix_tagger.evaluate(test_sents)\\n0.23682279300669112\\nTo learn on two-character sufixes, the code looks like this:\\n>>> suffix_tagger = AffixTagger(train_sents, affix_length=-2)\\n>>> suffix_tagger.evaluate(test_sents)\\n0.31953377940859057\\nHow it works...\\nA positive value for affix_length means that the AffixTagger will learn word preixes, \\nessentially word[:affix_length]. If the affix_length is negative, then sufixes are \\nlearned using word[affix_length:].\\nChapter 4\\n97\\nThere\\'s more...\\nYou can combine multiple afix taggers in a backoff chain if you want to learn about multiple \\ncharacter length afixes. Here\\'s an example of four AffixTagger classes learning about two \\nand three-character preixes and sufixes:\\n>>> pre3_tagger = AffixTagger(train_sents, affix_length=3)\\n>>> pre3_tagger.evaluate(test_sents)\\n0.23682279300669112\\n>>> pre2_tagger = AffixTagger(train_sents, affix_length=2, \\nbackoff=pre3_tagger)\\n>>> pre2_tagger.evaluate(test_sents)\\n0.29816533563565722\\n>>> suf2_tagger = AffixTagger(train_sents, affix_length=-2, \\nbackoff=pre2_tagger)\\n>>> suf2_tagger.evaluate(test_sents)\\n0.32523203108137277\\n>>> suf3_tagger = AffixTagger(train_sents, affix_length=-3, \\nbackoff=suf2_tagger)\\n>>> suf3_tagger.evaluate(test_sents)\\n0.35924886682495144\\nAs you can see, the accuracy goes up each time.\\nThe preceding ordering is not the best, nor is it the worst. I will leave it \\nto you to explore the possibilities and discover the best backoff chain of \\nAffixTagger and affix_length values.\\nMin stem length\\nAffixTagger also takes a min_stem_length keyword argument with a default value of 2. \\nIf the word length is less than min_stem_length plus the absolute value of affix_length, \\nthen None is returned by the context() method. Increasing min_stem_length forces \\nthe AffixTagger to only learn on longer words, while decreasing min_stem_length will \\nallow it to learn on shorter words. Of course, for shorter words, the affix_length could be \\nequal to or greater than the word length, and AffixTagger would essentially be acting like a \\nUnigramTagger.\\nSee also\\nYou can manually specify preixes and sufixes using regular expressions, as shown in the \\nprevious recipe. The Training a unigram part-of-speech tagger and Training and combining \\nNgram taggers recipes have details on NgramTagger subclasses, which are also subclasses \\nof ContextTagger.\\nPart-of-Speech Tagging\\n98\\nTraining a Brill tagger\\nThe BrillTagger is a transformation-based tagger. It is the irst tagger that is not a subclass \\nof SequentialBackoffTagger. Instead, the BrillTagger uses a series of rules to \\ncorrect the results of an initial tagger. These rules are scored based on how many errors they \\ncorrect minus the number of new errors they produce.\\nHow to do it...\\nHere\\'s a function from tag_util.py that trains a BrillTagger using \\nFastBrillTaggerTrainer. It requires an initial_tagger and train_sents.\\nfrom nltk.tag import brill\\ndef train_brill_tagger(initial_tagger, train_sents, **kwargs):\\n  sym_bounds = [(1,1), (2,2), (1,2), (1,3)]\\n  asym_bounds = [(-1,-1), (1,1)]\\n  templates = [\\n  brill.SymmetricProximateTokensTemplate(brill.ProximateTagsRule, \\n*sym_bounds),\\n    brill.SymmetricProximateTokensTemplate(brill.ProximateWordsRule, \\n*sym_bounds),\\n    brill.ProximateTokensTemplate(brill.ProximateTagsRule, *asym_\\nbounds),\\n    brill.ProximateTokensTemplate(brill.ProximateWordsRule, *asym_\\nbounds)\\n  ]\\n  trainer = brill.FastBrillTaggerTrainer(initial_tagger, templates, \\ndeterministic=True)\\n  return trainer.train(train_sents, **kwargs)\\nTo use it, we can create our initial_tagger from a backoff chain of NgramTagger \\nclasses, then pass that into the train_brill_tagger() function to get a \\nBrillTagger back.\\n>>> default_tagger = DefaultTagger(\\'NN\\')\\n>>> initial_tagger = backoff_tagger(train_sents, [UnigramTagger, \\nBigramTagger, TrigramTagger], backoff=default_tagger)\\n>>> initial_tagger.evaluate(test_sents)\\n0.88163177206993304\\n>>> from tag_util import train_brill_tagger\\n>>> brill_tagger = train_brill_tagger(initial_tagger, train_sents)\\n>>> brill_tagger.evaluate(test_sents)\\n0.88327217785452194\\nChapter 4\\n99\\nSo the BrillTagger has slightly increased accuracy over the initial_tagger.\\nHow it works...\\nThe FastBrillTaggerTrainer takes an initial_tagger and a list of templates. \\nThese templates must implement the BrillTemplateI interface. The two template \\nimplementations included with NLTK are ProximateTokensTemplate and \\nSymmetricProximateTokensTemplate. Each template is used to generate a list of \\nBrillRule subclasses. The actual class of the rules produced is passed in to the template \\nat initialization. The basic worklow looks like this:\\nThe two BrillRule subclasses used are ProximateTagsRule and \\nProximateWordsRule, which are both subclasses of ProximateTokensRule. \\nProximateTagsRule looks at surrounding tags to do error correction, and \\nProximateWordsRule looks at the surrounding words.\\nThe bounds that we pass in to each template are lists of (start, end) tuples that get \\npassed in to each rule as conditions. The conditions tell the rule which tokens it can look at. \\nFor example, if the condition is (1, 1), then the rule will only look at the next token. But if the \\ncondition is (1, 2), then the rule will look at both the next token and the token after it. For \\n(-1, -1) the rule will look only at the previous token.\\nProximateTokensTemplate produces ProximateTokensRule that look at each token for \\nits given conditions to do error correction. Positive and negative conditions must be explicitly \\nspeciied. SymmetricProximateTokensTemplate, on the other hand, produces pairs of \\nProximateTokensRule, where one rule uses the given conditions, and the other rule uses \\nthe negative of the conditions. So when we pass a list of positive (start, end) tuples to a \\nSymmetricProximateTokensTemplate, it will also produce a ProximateTokensRule \\nthat uses (-start, -end). This is why it\\'s symmetric—it produces rules that look on both \\nsides of the token.\\nUnlike with ProximateTokensTemplate, you should not give \\nnegative bounds to SymmetricProximateTokensTemplate, \\nsince it will produce those itself. Only use positive number bounds \\nwith SymmetricProximateTokensTemplate.\\nPart-of-Speech Tagging\\n100\\nThere\\'s more...\\nYou can control the number of rules generated using the max_rules keyword argument to \\nthe FastBrillTaggerTrainer.train() method. The default value is 200. You can also \\ncontrol the quality of rules used with the min_score keyword argument. The default value is \\n2, though 3 can be a good choice as well.\\nIncreasing max_rules or min_score will greatly increase training time, \\nwithout necessarily increasing accuracy. Change these values with care.\\nTracing\\nYou can watch the FastBrillTaggerTrainer do its work by passing trace=1 into the \\nconstructor. This can give you output such as:\\nTraining Brill tagger on 3000 sentences...\\n    Finding initial useful rules...\\n        Found 10709 useful rules.\\n    Selecting rules...\\nThis means it found 10709 rules with a score of at least min_score, and then it selects the \\nbest rules, keeping no more than max_rules.\\nThe default is trace=0, which means the trainer will work silently without printing its status.\\nSee also\\nThe Training and combining Ngram taggers recipe details the construction of the initial_\\ntagger used previously, and the Default tagging recipe explains the default_tagger.\\nTraining the TnT tagger\\nTnT stands for Trigrams\\'n\\'Tags. It is a statistical tagger based on second order Markov \\nmodels. You can read the original paper that lead to the implementation at http://acl.\\nldc.upenn.edu/A/A00/A00-1031.pdf.\\nHow to do it...\\nThe TnT tagger has a slightly different API than previous taggers we have encountered. You \\nmust explicitly call the train() method after you have created it. Here\\'s a basic example:\\n>>> from nltk.tag import tnt\\n>>> tnt_tagger = tnt.TnT()\\nChapter 4\\n101\\n>>> tnt_tagger.train(train_sents)\\n>>> tnt_tagger.evaluate(test_sents)\\n0.87580401467731495\\nIt\\'s quite a good tagger all by itself, only slightly less accurate than the BrillTagger from \\nthe previous recipe. But if you do not call train() before evaluate(), you will get an \\naccuracy of 0%.\\nHow it works...\\nTnT maintains a number of internal FreqDist and ConditionalFreqDist instances \\nbased on the training data. These frequency distributions count unigrams, bigrams, and \\ntrigrams. Then, during tagging, the frequencies are used to calculate the probabilities of \\npossible tags for each word. So instead of constructing a backoff chain of NgramTagger \\nsubclasses, the TnT tagger uses all the ngram models together to choose the best tag. It also \\ntries to guess the tags for the whole sentence at once, by choosing the most likely model for \\nthe entire sentence, based on the probabilities of each possible tag.\\nTraining is fairly quick, but tagging is signiicantly slower than \\nthe other taggers we have covered. This is due to all the loating \\npoint math that must be done to calculate the tag probabilities \\nof each word.\\nThere\\'s more...\\nTnT accepts a few optional keyword arguments. You can pass in a tagger for unknown \\nwords as unk. If this tagger is already trained, then you must also pass in Trained=True. \\nOtherwise it will call unk.train(data) with the same data you pass in to the train() \\nmethod. Since none of the previous taggers have a public train() method, we recommend \\nalways passing Trained=True if you also pass an unk tagger. Here\\'s an example using a \\nDefaultTagger, which does not require any training:\\n>>> from nltk.tag import DefaultTagger\\n>>> unk = DefaultTagger(\\'NN\\')\\n>>> tnt_tagger = tnt.TnT(unk=unk, Trained=True)\\n>>> tnt_tagger.train(train_sents)\\n>>> tnt_tagger.evaluate(test_sents)\\n0.89272609540254699\\nPart-of-Speech Tagging\\n102\\nSo we got an almost 2% increase in accuracy! You must use a tagger that can tag a single \\nword without having seen that word before. This is because the unknown tagger\\'s tag() \\nmethod is only called with a single word sentence. Other good candidates for an unknown \\ntagger are RegexpTagger or AffixTagger. Passing in a UnigramTagger that\\'s been \\ntrained on the same data is pretty much useless, as it will have seen the exact same words, \\nand therefore have the same unknown word blind spots.\\nControlling the beam search\\nAnother parameter you can modify for TnT is N, which controls the number of possible \\nsolutions the tagger maintains while trying to guess the tags for a sentence. N defaults to \\n1,000. Increasing it will greatly increase the amount of memory used during tagging, without \\nnecessarily increasing accuracy. Decreasing N will decrease memory usage, but could also \\ndecrease accuracy. Here\\'s what happens when you set N=100:\\n>>> tnt_tagger = tnt.TnT(N=100)\\n>>> tnt_tagger.train(train_sents)\\n>>> tnt_tagger.evaluate(test_sents)\\n0.87580401467731495\\nSo the accuracy is exactly the same, but we use signiicantly less memory to achieve it. \\nHowever, don\\'t assume that accuracy will not change if you decrease N; experiment with \\nyour own data to be sure.\\nCapitalization signiicance\\nYou can pass C=True if you want capitalization of words to be signiicant. The default is \\nC=False, which means all words are lowercased. The documentation on C says that treating \\ncapitalization as signiicant probably will not increase accuracy. In my own testing, there was a \\nvery slight (< 0.01%) increase in accuracy with C=True, probably because case-sensitivity can \\nhelp identify proper nouns.\\nSee also\\nWe covered the DefaultTagger in the Default tagging recipe, backoff tagging in \\nthe Combining taggers with backoff tagging recipe, NgramTagger subclasses in the \\nTraining a unigram part-of-speech tagger and Training combining Ngram taggers recipes, \\nRegexpTagger in the Tagging with regular expressions recipe, and the AffixTagger \\nin the Afix tagging recipe.\\nChapter 4\\n103\\nUsing WordNet for tagging\\nIf you remember from the Looking up synsets for a word in Wordnet recipe in \\nChapter 1, Tokenizing Text and WordNet Basics, WordNet synsets specify a part-of-speech \\ntag. It\\'s a very restricted set of possible tags, and many words have multiple synsets with \\ndifferent part-of-speech tags, but this information can be useful for tagging unknown words. \\nWordNet is essentially a giant dictionary, and it\\'s likely to contain many words that are not in \\nyour training data.\\nGetting ready\\nFirst, we need to decide how to map WordNet part-of-speech tags to the Penn Treebank part-\\nof-speech tags we have been using. The following is a table mapping one to the other. See the \\nLooking up synsets for a word in Wordnet recipe in Chapter 1, Tokenizing Text and WordNet \\nBasics for more details. The \"s\", which was not shown before, is just another kind of adjective, \\nat least for tagging purposes.\\nWordNet Tag\\nTreebank Tag\\nn\\nNN\\na\\nJJ\\ns\\nJJ\\nr\\nRB\\nv\\nVB\\nHow to do it...\\nNow we can create a class that will look up words in WordNet, then chose the most common tag \\nfrom the synsets it inds. The WordNetTagger deined next can be found in taggers.py:\\nfrom nltk.tag import SequentialBackoffTagger\\nfrom nltk.corpus import wordnet\\nfrom nltk.probability import FreqDist\\nclass WordNetTagger(SequentialBackoffTagger):\\n  \\'\\'\\'\\n  >>> wt = WordNetTagger()\\n  >>> wt.tag([\\'food\\', \\'is\\', \\'great\\'])\\n  [(\\'food\\', \\'NN\\'), (\\'is\\', \\'VB\\'), (\\'great\\', \\'JJ\\')]\\n  \\'\\'\\'\\n  def __init__(self, *args, **kwargs):\\n    SequentialBackoffTagger.__init__(self, *args, **kwargs)\\n    self.wordnet_tag_map = {\\nPart-of-Speech Tagging\\n104\\n      \\'n\\': \\'NN\\',\\n      \\'s\\': \\'JJ\\',\\n      \\'a\\': \\'JJ\\',\\n      \\'r\\': \\'RB\\',\\n      \\'v\\': \\'VB\\'\\n    }\\n  def choose_tag(self, tokens, index, history):\\n    word = tokens[index]\\n    fd = FreqDist()\\n    for synset in wordnet.synsets(word):\\n      fd.inc(synset.pos)\\n    return self.wordnet_tag_map.get(fd.max())\\nHow it works...\\nThe WordNetTagger simply counts the number of each part-of-speech tag found in the \\nsynsets for a word. The most common tag is then mapped to a treebank tag using an \\ninternal mapping. Here\\'s some sample usage code:\\n>>> from taggers import WordNetTagger\\n>>> wn_tagger = WordNetTagger()\\n>>> wn_tagger.evaluate(train_sents)\\n0.18451574615215904\\nSo it\\'s not too accurate, but that\\'s to be expected. We only have enough information to \\nproduce four different kinds of tags, while there are 36 possible tags in treebank. And \\nmany words can have different part-of-speech tags depending on their context. But if we put \\nthe WordNetTagger at the end of an NgramTagger backoff chain, then we can improve \\naccuracy over the DefaultTagger.\\n>>> from tag_util import backoff_tagger\\n>>> from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\\n>>> tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, \\nTrigramTagger], backoff=wn_tagger)\\n>>> tagger.evaluate(test_sents)\\n0.88564644938484782\\nSee also\\nThe Looking up synsets for a word in Wordnet recipe in Chapter 1, Tokenizing Text and \\nWordNet Basics details how to use the wordnet corpus and what kinds of part-of-speech \\ntags it knows about. And in the Combining taggers with backoff tagging and Training and \\ncombining Ngram taggers recipes, we went over backoff tagging with ngram taggers.\\nChapter 4\\n105\\nTagging proper names\\nUsing the included names corpus, we can create a simple tagger for tagging names as \\nproper nouns.\\nHow to do it...\\nThe NamesTagger is a subclass of SequentialBackoffTagger as it\\'s probably only useful \\nnear the end of a backoff chain. At initialization, we create a set of all names in the names \\ncorpus, lowercasing each name to make lookup easier. Then we implement the choose_\\ntag() method, which simply checks if the current word is in the names_set. If it is, we return \\nthe tag NNP (which is the tag for proper nouns). If it isn\\'t, we return None so the next tagger in \\nthe chain can tag the word. The following code can be found in taggers.py:\\nfrom nltk.tag import SequentialBackoffTagger\\nfrom nltk.corpus import names\\nclass NamesTagger(SequentialBackoffTagger):\\n  def __init__(self, *args, **kwargs):\\n    SequentialBackoffTagger.__init__(self, *args, **kwargs)\\n    self.name_set = set([n.lower() for n in names.words()])\\n  def choose_tag(self, tokens, index, history):\\n    word = tokens[index]\\n    if word.lower() in self.name_set:\\n      return \\'NNP\\'\\n    else:\\n      return None\\nHow it works...\\nNamesTagger should be pretty self-explanatory. Its usage is also simple:\\n>>> from taggers import NamesTagger\\n>>> nt = NamesTagger()\\n>>> nt.tag([\\'Jacob\\'])\\n[(\\'Jacob\\', \\'NNP\\')]\\nIt\\'s probably best to use the NamesTagger right before a DefaultTagger, so it\\'s at the end \\nof a backoff chain. But it could probably go anywhere in the chain since it\\'s unlikely to mistag \\na word.\\nPart-of-Speech Tagging\\n106\\nSee also\\nThe Combining taggers with backoff tagging recipe goes over the details of using \\nSequentialBackoffTagger subclasses.\\nClassiier based tagging\\nThe ClassifierBasedPOSTagger uses classiication to do part-of-speech tagging. \\nFeatures are extracted from words, then passed to an internal classiier. The classiier \\nclassiies the features and returns a label; in this case, a part-of-speech tag. Classiication  \\nwill be covered in detail in Chapter 7, Text Classiication.\\nClassifierBasedPOSTagger is a subclass of ClassifierBasedTagger that \\nimplements a feature detector that combines many of the techniques of previous taggers into \\na single feature set. The feature detector inds multiple length sufixes, does some regular \\nexpression matching, and looks at the unigram, bigram, and trigram history to produce a fairly \\ncomplete set of features for each word. The feature sets it produces are used to train the \\ninternal classiier, and are used for classifying words into part-of-speech tags.\\nHow to do it...\\nBasic usage of the ClassifierBasedPOSTagger is much like any other \\nSequentialBackoffTaggger. You pass in training sentences, it trains an internal classiier, \\nand you get a very accurate tagger.\\n>>> from nltk.tag.sequential import ClassifierBasedPOSTagger\\n>>> tagger = ClassifierBasedPOSTagger(train=train_sents)\\n>>> tagger.evaluate(test_sents)\\n0.93097345132743359\\nNotice a slight modiication to initialization—train_sents must be passed \\nin as the train keyword argument.\\nHow it works...\\nClassifierBasedPOSTagger inherits from ClassifierBasedTagger and only \\nimplements a feature_detector() method. All the training and tagging is done in \\nClassifierBasedTagger. It defaults to training a NaiveBayesClassifier with the \\ngiven training data. Once this classiier is trained, it is used to classify word features produced \\nby the feature_detector() method.\\nChapter 4\\n107\\nThe ClassifierBasedTagger is often the most accurate tagger, but it\\'s \\nalso one of the slowest taggers. If speed is an issue, you should stick with a \\nBrillTagger based on a backoff chain of NgramTagger subclasses and \\nother simple taggers.\\nThe ClassifierBasedTagger also inherits from FeatursetTaggerI (which is just an \\nempty class), creating an inheritance tree that looks like this:\\nThere\\'s more...\\nYou can use a different classiier instead of NaiveBayesClassifier by passing in your own \\nclassifier_builder function. For example, to use a MaxentClassifier, you would do \\nthe following:\\n>>> from nltk.classify import MaxentClassifier\\n>>> me_tagger = ClassifierBasedPOSTagger(train=train_sents, \\nclassifier_builder=MaxentClassifier.train)\\n>>> me_tagger.evaluate(test_sents)\\n0.93093028275415501\\nThe MaxentClassifier takes even longer to train than \\nNaiveBayesClassifier. If you have scipy and numpy installed, training \\nwill be faster than normal, but still slower than NaiveBayesClassifier.\\nPart-of-Speech Tagging\\n108\\nCustom feature detector\\nIf you want to do your own feature detection, there are two ways to do it.\\n1. Subclass ClassifierBasedTagger and implement a feature_detector() \\nmethod.\\n2. Pass a method as the feature_detector keyword argument into \\nClassifierBasedTagger at initialization.\\nEither way, you need a feature detection method that can take the same arguments as \\nchoose_tag(): tokens, index, and history. But instead of returning a tag, you return a \\ndict of key-value features, where the key is the feature name, and the value is the feature \\nvalue. A very simple example would be a unigram feature detector (found in tag_util.py).\\ndef unigram_feature_detector(tokens, index, history):\\n  return {\\'word\\': tokens[index]}\\nThen using the second method, you would pass the following into ClassifierBasedTagger \\nas feature_detector:\\n>>> from nltk.tag.sequential import ClassifierBasedTagger\\n>>> from tag_util import unigram_feature_detector\\n>>> tagger = ClassifierBasedTagger(train=train_sents, feature_\\ndetector=unigram_feature_detector)\\n>>> tagger.evaluate(test_sents)\\n0.87338657457371038\\nCutoff probability\\nBecause a classiier will always return the best result it can, passing in a backoff tagger \\nis useless unless you also pass in a cutoff_prob to specify the probability threshold for \\nclassiication. Then, if the probability of the chosen tag is less than cutoff_prob, the \\nbackoff tagger will be used. Here\\'s an example using the DefaultTagger as the backoff, \\nand setting cutoff_prob to 0.3:\\n>>> default = DefaultTagger(\\'NN\\')\\n>>> tagger = ClassifierBasedPOSTagger(train=train_sents, \\nbackoff=default, cutoff_prob=0.3)\\n>>> tagger.evaluate(test_sents)\\n0.93110295704726964\\nSo we get a slight increase in accuracy if the ClassifierBasedPOSTagger uses the \\nDefaultTagger whenever its tag probability is less than 30%.\\nChapter 4\\n109\\nPre-trained classiier\\nIf you want to use a classiier that\\'s already been trained, then you can pass that in to \\nClassifierBasedTagger or ClassifierBasedPOSTagger as classifier. In this \\ncase, the classifier_builder argument is ignored and no training takes place. However, \\nyou must ensure that the classiier has been trained on and can classify feature sets \\nproduced by whatever feature_detector() method you use.\\nSee also\\nChapter 7, Text Classiication will cover classiication in depth.\\n5\\nExtracting Chunks\\nIn this chapter, we will cover:\\n \\nf\\nChunking and chinking with regular expressions\\n \\nf\\nMerging and splitting chunks with regular expressions\\n \\nf\\nExpanding and removing chunks with regular expressions\\n \\nf\\nPartial parsing with regular expressions\\n \\nf\\nTraining a tagger-based chunker\\n \\nf\\nClassiication-based chunking\\n \\nf\\nExtracting named entities\\n \\nf\\nExtracting proper noun chunks\\n \\nf\\nExtracting location chunks\\n \\nf\\nTraining a named entity chunker\\nIntroduction\\nChunk extraction or partial parsing is the process of extracting short phrases from a \\npart-of-speech tagged sentence. This is different than full parsing, in that we are interested \\nin standalone chunks or phrases instead of full parse trees. The idea is that meaningful \\nphrases can be extracted from a sentence by simply looking for particular patterns of  \\npart-of-speech tags.\\nAs in Chapter 4, Part-of-Speech Tagging, we will be using the Penn Treebank corpus for basic \\ntraining and testing chunk extraction. We will also be using the CoNLL 2000 corpus as it has \\na simpler and more lexible format that supports multiple chunk types (refer to the Creating a \\nchunked phrase corpus recipe in Chapter 3, Creating Custom Corpora for more details on the \\nconll2000 corpus and IOB tags).\\nExtracting Chunks\\n112\\nChunking and chinking with regular  \\nexpressions\\nUsing modiied regular expressions, we can deine chunk patterns. These are patterns of \\npart-of-speech tags that deine what kinds of words make up a chunk. We can also deine \\npatterns for what kinds of words should not be in a chunk. These unchunked words are  \\n known as chinks.\\nA ChunkRule speciies what to include in a chunk, while a ChinkRule speciies what to \\nexclude from a chunk. In other words, chunking creates chunks, while chinking breaks up \\nthose chunks.\\nGetting ready\\nWe irst need to know how to deine chunk patterns. These are modiied regular expressions \\ndesigned to match sequences of part-of-speech tags. An individual tag is speciied by \\nsurrounding angle brackets, such as <NN> to match a noun tag. Multiple tags can then be \\ncombined, as in <DT><NN> to match a determiner followed by a noun. Regular expression \\nsyntax can be used within the angle brackets to match individual tag patterns, so you can \\ndo <NN.*> to match all nouns including NN and NNS. You can also use regular expression \\nsyntax outside of the angle brackets to match patterns of tags. <DT>?<NN.*>+ will match \\nan optional determiner followed by one or more nouns. The chunk patterns are converted \\ninternally to regular expressions using the tag_pattern2re_pattern() function:\\n>>> from nltk.chunk import tag_pattern2re_pattern\\n>>> tag_pattern2re_pattern(\\'<DT>?<NN.*>+\\')\\n\\'(<(DT)>)?(<(NN[^\\\\\\\\{\\\\\\\\}<>]*)>)+\\'\\nYou don\\'t have to use this function to do chunking, but it might be useful or interesting to see \\nhow your chunk patterns convert to regular expressions.\\nHow to do it...\\nThe pattern for specifying a chunk is to use surrounding curly braces, such as {<DT><NN>}. \\nTo specify a chink, you lip the braces, as in }<VB>{. These rules can be combined into a \\ngrammar for a particular phrase type. Here\\'s a grammar for noun-phrases that combines \\nboth a chunk and a chink pattern, along with the result of parsing the sentence \"The book  \\nhas many chapters\":\\n>>> from nltk.chunk import RegexpParser\\n>>> chunker = RegexpParser(r\\'\\'\\'\\n... NP:\\n...    {<DT><NN.*><.*>*<NN.*>}\\n...    }<VB.*>{\\nChapter 5\\n113\\n... \\'\\'\\')\\n>>> chunker.parse([(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\'),  \\n(\\'has\\', \\'VBZ\\'), (\\'many\\', \\'JJ\\'), (\\'chapters\\', \\'NNS\\')])\\nTree(\\'S\\', [Tree(\\'NP\\', [(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\')]),  \\n(\\'has\\', \\'VBZ\\'), Tree(\\'NP\\', [(\\'many\\', \\'JJ\\'), (\\'chapters\\',  \\n\\'NNS\\')])])\\nThe grammar tells the RegexpParser that there are two rules for parsing NP chunks. The \\nirst chunk pattern says that a chunk starts with a determiner followed by any kind of noun. \\nThen any number of other words is allowed, until a inal noun is found. The second pattern \\nsays that verbs should be chinked, thus separating any large chunks that contain a verb. \\nThe result is a tree with two noun-phrase chunks: \"the book\" and \"many chapters\".\\nTagged sentences are always parsed into a Tree (found in the \\nnltk.tree module). The top node of the Tree is \\'S\\', which \\nstands for sentence. Any chunks found will be subtrees whose \\nnodes will refer to the chunk type. In this case, the chunk type \\nis \\'NP\\' for noun-phrase. Trees can be drawn calling the draw() \\nmethod, as in t.draw().\\nHow it works...\\nHere\\'s what happens, step-by-step:\\n1. The sentence is converted into a lat Tree, as shown in the following igure:\\n2. The Tree is used to create a ChunkString.\\n3. RegexpParser parses the grammar to create a NP RegexpChunkParser with the \\ngiven rules.\\n4. A ChunkRule is created and applied to the ChunkString, which matches the entire \\nsentence into a chunk, as shown in the following igure:\\nExtracting Chunks\\n114\\n5. A ChinkRule is created and applied to the same ChunkString, which splits \\nthe big chunk into two smaller chunks with a verb between them, as shown in  \\nthe following igure:\\n6. The ChunkString is converted back to a Tree, now with two NP chunk subtrees, as \\nshown in the following igure:\\nYou can do this yourself using the classes in nltk.chunk.regexp. ChunkRule and \\nChinkRule are both subclasses of RegexpChunkRule and require two arguments: the \\npattern, and a description of the rule. ChunkString is an object that starts with a lat tree, \\nwhich is then modiied by each rule when it is passed in to the rule\\'s apply() method. A \\nChunkString is converted back to a Tree with the to_chunkstruct() method. Here\\'s \\nthe code to demonstrate it:\\n>>> from nltk.chunk.regexp import ChunkString, ChunkRule, ChinkRule\\n>>> from nltk.tree import Tree\\n>>> t = Tree(\\'S\\', [(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\'), (\\'has\\', \\'VBZ\\'), \\n(\\'many\\', \\'JJ\\'), (\\'chapters\\', \\'NNS\\')])\\n>>> cs = ChunkString(t)\\n>>> cs\\n<ChunkString: \\'<DT><NN><VBZ><JJ><NNS>\\'>\\n>>> ur = ChunkRule(\\'<DT><NN.*><.*>*<NN.*>\\', \\'chunk determiners and \\nnouns\\')\\n>>> ur.apply(cs)\\n>>> cs\\n<ChunkString: \\'{<DT><NN><VBZ><JJ><NNS>}\\'>\\n>>> ir = ChinkRule(\\'<VB.*>\\', \\'chink verbs\\')\\n>>> ir.apply(cs)\\n>>> cs\\n<ChunkString: \\'{<DT><NN>}<VBZ>{<JJ><NNS>}\\'>\\n>>> cs.to_chunkstruct()\\nTree(\\'S\\', [Tree(\\'CHUNK\\', [(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\')]), (\\'has\\',  \\n\\'VBZ\\'), Tree(\\'CHUNK\\', [(\\'many\\', \\'JJ\\'), (\\'chapters\\', \\'NNS\\')])])\\nThe preceding tree diagrams can be drawn at each step by calling cs.to_chunkstruct().\\ndraw().\\nChapter 5\\n115\\nThere\\'s more...\\nYou will notice that the subtrees from the ChunkString are tagged as \\'CHUNK\\' and not \\n\\'NP\\'. That\\'s because the previous rules are phrase agnostic; they create chunks without \\nneeding to know what kind of chunks they are.\\nInternally, the RegexpParser creates a RegexpChunkParser for each chunk phrase type. \\nSo if you are only chunking NP phrases, there will only be one RegexpChunkParser. The \\nRegexpChunkParser gets all the rules for the speciic chunk type, and handles applying the \\nrules in order and converting the \\'CHUNK\\' trees to the speciic chunk type, such as \\'NP\\'.\\nHere\\'s some code to illustrate the usage of RegexpChunkParser. We pass the previous \\ntwo rules into the RegexpChunkParser, and then parse the same sentence tree we \\ncreated before. The resulting tree is just like what we got from applying both rules in \\norder, except \\'CHUNK\\' has been replaced with \\'NP\\' in the two subtrees. This is because \\nRegexpChunkParser defaults to chunk_node=\\'NP\\'.\\n>>> from nltk.chunk import RegexpChunkParser\\n>>> chunker = RegexpChunkParser([ur, ir])\\n>>> chunker.parse(t)\\nTree(\\'S\\', [Tree(\\'NP\\', [(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\')]), (\\'has\\',  \\n\\'VBZ\\'), Tree(\\'NP\\', [(\\'many\\', \\'JJ\\'), (\\'chapters\\', \\'NNS\\')])])\\nDifferent chunk types\\nIf you wanted to parse a different chunk type, then you could pass that in as chunk_node \\nto RegexpChunkParser. Here\\'s the same code we have just seen, but instead of \\'NP\\' \\nsubtrees, we will call them \\'CP\\' for custom phrase.\\n>>> from nltk.chunk import RegexpChunkParser\\n>>> chunker = RegexpChunkParser([ur, ir], chunk_node=\\'CP\\')\\n>>> chunker.parse(t)\\nTree(\\'S\\', [Tree(\\'CP\\', [(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\')]),  \\n(\\'has\\', \\'VBZ\\'), Tree(\\'CP\\', [(\\'many\\', \\'JJ\\'), (\\'chapters\\',  \\n\\'NNS\\')])])\\nRegexpParser does this internally when you specify multiple phrase types. This will be \\ncovered in Partial parsing with regular expressions.\\nAlternative patterns\\nThe same parsing results can be obtained by using two chunk patterns in the grammar, and \\ndiscarding the chink pattern:\\n>>> chunker = RegexpParser(r\\'\\'\\'\\n... NP:\\n...    {<DT><NN.*>}\\n...    {<JJ><NN.*>}\\nExtracting Chunks\\n116\\n... \\'\\'\\')\\n>>> chunker.parse(t)\\nTree(\\'S\\', [Tree(\\'NP\\', [(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\')]), (\\'has\\',  \\n\\'VBZ\\'), Tree(\\'NP\\', [(\\'many\\', \\'JJ\\'), (\\'chapters\\', \\'NNS\\')])])\\nIn fact, you could reduce the two chunk patterns into a single pattern.\\n>>> chunker = RegexpParser(r\\'\\'\\'\\n... NP:\\n...    {(<DT>|<JJ>)<NN.*>}\\n... \\'\\'\\')\\n>>> chunker.parse(t)\\nTree(\\'S\\', [Tree(\\'NP\\', [(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\')]), (\\'has\\',  \\n\\'VBZ\\'), Tree(\\'NP\\', [(\\'many\\', \\'JJ\\'), (\\'chapters\\', \\'NNS\\')])])\\nHow you create and combine patterns is really up to you. Pattern creation is a process of trial \\nand error, and entirely depends on what your data looks like and which patterns are easiest  \\nto express.\\nChunk rule with context\\nYou can also create chunk rules with a surrounding tag context. For example, if your pattern \\nis <DT>{<NN>}, which will be parsed into a ChunkRuleWithContext. Any time there\\'s a \\ntag on either side of the curly braces, you will get a ChunkRuleWithContext instead of a \\nChunkRule. This can allow you to be more speciic about when to parse particular kinds \\nof chunks.\\nHere\\'s an example of using ChunkWithContext directly. It takes four arguments: the left \\ncontext, the pattern to chunk, the right context, and a description:\\n>>> from nltk.chunk.regexp import ChunkRuleWithContext\\n>>> ctx = ChunkRuleWithContext(\\'<DT>\\', \\'<NN.*>\\', \\'<.*>\\', \\'chunk nouns \\nonly after determiners\\')\\n>>> cs = ChunkString(t)\\n>>> cs\\n<ChunkString: \\'<DT><NN><VBZ><JJ><NNS>\\'>\\n>>> ctx.apply(cs)\\n>>> cs\\n<ChunkString: \\'<DT>{<NN>}<VBZ><JJ><NNS>\\'>\\n>>> cs.to_chunkstruct()\\nTree(\\'S\\', [(\\'the\\', \\'DT\\'), Tree(\\'CHUNK\\', [(\\'book\\', \\'NN\\')]), (\\'has\\',  \\n\\'VBZ\\'), (\\'many\\', \\'JJ\\'), (\\'chapters\\', \\'NNS\\')])\\nChapter 5\\n117\\nThis example only chunks nouns that follow a determiner, therefore ignoring the noun that \\nfollows an adjective. Here\\'s how it would look using the RegexpParser:\\n>>> chunker = RegexpParser(r\\'\\'\\'\\n... NP:\\n...    <DT>{<NN.*>}\\n... \\'\\'\\')\\n>>> chunker.parse(t)\\nTree(\\'S\\', [(\\'the\\', \\'DT\\'), Tree(\\'NP\\', [(\\'book\\', \\'NN\\')]), (\\'has\\',  \\n\\'VBZ\\'), (\\'many\\', \\'JJ\\'), (\\'chapters\\', \\'NNS\\')])\\nSee also\\nIn the next recipe, we will cover merging and splitting chunks.\\nMerging and splitting chunks with regular \\nexpressions\\nIn this recipe, we will cover two more rules for chunking. A MergeRule can merge two chunks \\ntogether based on the end of the irst chunk and the beginning of the second chunk. A \\nSplitRule will split a chunk into two based on the speciied split pattern.\\nHow to do it...\\nA SplitRule is speciied with two opposing curly braces surrounded by a pattern on either \\nside. To split a chunk after a noun, you would do <NN.*>}{<.*>. A MergeRule is speciied \\nby lipping the curly braces, and will join chunks where the end of the irst chunk matches \\nthe left pattern, and the beginning of the next chunk matches the right pattern. To merge two \\nchunks where the irst ends with a noun and the second begins with a noun, you would use \\n<NN.*>{}<NN.*>.\\nThe order of rules is very important and re-ordering can affect the \\nresults. The RegexpParser applies the rules one at a time from \\ntop to bottom, so each rule will be applied to the ChunkString \\nresulting from the previous rule.\\nExtracting Chunks\\n118\\nHere\\'s an example of splitting and merging, starting with the sentence tree as shown next:\\n1. The whole sentence is chunked, as shown in the following diagram:\\n2. The chunk is split into multiple chunks after every noun, as shown in the  \\nfollowing tree:\\n3. Each chunk with a determiner is split into separate chunks, creating four chunks \\nwhere there were three:\\n4. Chunks ending with a noun are merged with the next chunk if it begins with a noun, \\nreducing the four chunks back down to three, as shown in the following diagram:\\nUsing the RegexpParser, the code looks like this:\\n>>> chunker = RegexpParser(r\\'\\'\\'\\n... NP:\\n...     {<DT><.*>*<NN.*>}\\n...     <NN.*>}{<.*>\\nChapter 5\\n119\\n...     <.*>}{<DT>\\n...     <NN.*>{}<NN.*>\\n... \\'\\'\\')\\n>>> sent = [(\\'the\\', \\'DT\\'), (\\'sushi\\', \\'NN\\'), (\\'roll\\', \\'NN\\'),  \\n(\\'was\\', \\'VBD\\'), (\\'filled\\', \\'VBN\\'), (\\'with\\', \\'IN\\'), (\\'the\\',  \\n\\'DT\\'), (\\'fish\\', \\'NN\\')]\\n>>> chunker.parse(sent)\\nTree(\\'S\\', [Tree(\\'NP\\', [(\\'the\\', \\'DT\\'), (\\'sushi\\', \\'NN\\'),  \\n(\\'roll\\', \\'NN\\')]), Tree(\\'NP\\', [(\\'was\\', \\'VBD\\'), (\\'filled\\',  \\n\\'VBN\\'), (\\'with\\', \\'IN\\')]), Tree(\\'NP\\', [(\\'the\\', \\'DT\\'),  \\n(\\'fish\\', \\'NN\\')])])\\nAnd the inal tree of NP chunks is shown in the following diagram:\\nHow it works...\\nThe MergeRule and SplitRule classes take three arguments: the left pattern, right \\npattern, and a description. The RegexpParser takes care of splitting the original patterns on \\nthe curly braces to get the left and right sides, but you can also create these manually. Here\\'s \\na step-by-step walkthrough of how the original sentence is modiied by applying each rule:\\n>>> from nltk.chunk.regexp import MergeRule, SplitRule\\n>>> cs = ChunkString(Tree(\\'S\\', sent))\\n>>> cs\\n<ChunkString: \\'<DT><NN><NN><VBD><VBN><IN><DT><NN>\\'>\\n>>> ur = ChunkRule(\\'<DT><.*>*<NN.*>\\', \\'chunk determiner to noun\\')\\n>>> ur.apply(cs)\\n>>> cs\\n<ChunkString: \\'{<DT><NN><NN><VBD><VBN><IN><DT><NN>}\\'>\\n>>> sr1 = SplitRule(\\'<NN.*>\\', \\'<.*>\\', \\'split after noun\\')\\n>>> sr1.apply(cs)\\n>>> cs\\n<ChunkString: \\'{<DT><NN>}{<NN>}{<VBD><VBN><IN><DT><NN>}\\'>\\n>>> sr2 = SplitRule(\\'<.*>\\', \\'<DT>\\', \\'split before determiner\\')\\n>>> sr2.apply(cs)\\n>>> cs\\n<ChunkString: \\'{<DT><NN>}{<NN>}{<VBD><VBN><IN>}{<DT><NN>}\\'>\\n>>> mr = MergeRule(\\'<NN.*>\\', \\'<NN.*>\\', \\'merge nouns\\')\\n>>> mr.apply(cs)\\nExtracting Chunks\\n120\\n>>> cs\\n<ChunkString: \\'{<DT><NN><NN>}{<VBD><VBN><IN>}{<DT><NN>}\\'>\\n>>> cs.to_chunkstruct()\\nTree(\\'S\\', [Tree(\\'CHUNK\\', [(\\'the\\', \\'DT\\'), (\\'sushi\\', \\'NN\\'),  \\n(\\'roll\\', \\'NN\\')]), Tree(\\'CHUNK\\', [(\\'was\\', \\'VBD\\'),  \\n(\\'filled\\', \\'VBN\\'), (\\'with\\', \\'IN\\')]), Tree(\\'CHUNK\\',  \\n[(\\'the\\', \\'DT\\'), (\\'fish\\', \\'NN\\')])])\\nThere\\'s more...\\nThe parsing of the rules and splitting of left and right patterns is done in the static parse() \\nmethod of the RegexpChunkRule superclass. This is called by the RegexpParser to get the \\nlist of rules to pass in to the RegexpChunkParser. Here are some examples of parsing the \\npatterns used before:\\n>>> from nltk.chunk.regexp import RegexpChunkRule\\n>>> RegexpChunkRule.parse(\\'{<DT><.*>*<NN.*>}\\')\\n<ChunkRule: \\'<DT><.*>*<NN.*>\\'>\\n>>> RegexpChunkRule.parse(\\'<.*>}{<DT>\\')\\n<SplitRule: \\'<.*>\\', \\'<DT>\\'>\\n>>> RegexpChunkRule.parse(\\'<NN.*>{}<NN.*>\\')\\n<MergeRule: \\'<NN.*>\\', \\'<NN.*>\\'>\\nRule descriptions\\nDescriptions for each rule can be speciied with a comment string after the rule (a comment \\nstring must start with #). If no comment string is found, the rule\\'s description will be empty. \\nHere\\'s an example:\\n>>> RegexpChunkRule.parse(\\'{<DT><.*>*<NN.*>} # chunk everything\\').\\ndescr()\\n\\'chunk everything\\'\\n>>> RegexpChunkRule.parse(\\'{<DT><.*>*<NN.*>}\\').descr()\\n\\'\\'\\nComment string descriptions can also be used within grammar strings that are passed  \\nto RegexpParser.\\nSee also\\nThe previous recipe goes over how to use ChunkRule and how rules are passed in \\nto RegexpChunkParser.\\nChapter 5\\n121\\nExpanding and removing chunks with \\nregular expressions\\nThere are three RegexpChunkRule subclasses that are not supported by \\nRegexpChunkRule.parse() and therefore must be created manually if you want to use \\nthem. These rules are:\\n1. ExpandLeftRule: Adds unchunked (chink) words to the left of a chunk \\nto the chunk.\\n2. ExpandRightRule: Adds unchunked (chink) words to the right of a chunk \\nto the chunk.\\n3. UnChunkRule: Unchunk any matching chunk.\\nHow to do it...\\nExpandLeftRule and ExpandRightRule both take two patterns along with a description \\nas arguments. For ExpandLeftRule, the irst pattern is the chink we want to add to the \\nbeginning of the chunk, while the right pattern will match the beginning of the chunk we want \\nto expand. With ExpandRightRule, the left pattern should match the end of the chunk we \\nwant to expand, and the right pattern matches the chink we want to add to the end of the \\nchunk. The idea is similar to the MergeRule, but in this case we are merging chink words \\ninstead of other chunks.\\nUnChunkRule is the opposite of ChunkRule. Any chunk that exactly matches the \\nUnChunkRule pattern will be unchunked, and become a chink. Here\\'s some code \\ndemonstrating usage with the RegexpChunkParser:\\n>>> from nltk.chunk.regexp import ChunkRule, ExpandLeftRule, \\nExpandRightRule, UnChunkRule\\n>>> from nltk.chunk import RegexpChunkParser\\n>>> ur = ChunkRule(\\'<NN>\\', \\'single noun\\')\\n>>> el = ExpandLeftRule(\\'<DT>\\', \\'<NN>\\', \\'get left determiner\\')\\n>>> er = ExpandRightRule(\\'<NN>\\', \\'<NNS>\\', \\'get right plural noun\\')\\n>>> un = UnChunkRule(\\'<DT><NN.*>*\\', \\'unchunk everything\\')\\n>>> chunker = RegexpChunkParser([ur, el, er, un])\\n>>> sent = [(\\'the\\', \\'DT\\'), (\\'sushi\\', \\'NN\\'), (\\'rolls\\', \\'NNS\\')]\\n>>> chunker.parse(sent)\\nTree(\\'S\\', [(\\'the\\', \\'DT\\'), (\\'sushi\\', \\'NN\\'), (\\'rolls\\', \\'NNS\\')])\\nYou will notice the end result is a lat sentence, which is exactly what we started with. That\\'s \\nbecause the inal UnChunkRule undid the chunk created by the previous rules. Read on to \\nsee the step-by-step procedure of what happened.\\nExtracting Chunks\\n122\\nHow it works...\\nThe preceding rules were applied in the following order, starting with the sentence tree  \\nshown below:\\n1. Make single nouns into a chunk, as shown in the following diagram:\\n2. Expand left determiners into chunks that begin with a noun, as shown in the  \\nfollowing diagram:\\n3. Expand right plural nouns into chunks that end with a noun, chunking the whole \\nsentence as shown in the following diagram:\\n4. Unchunk every chunk that is a determiner + noun + plural noun, resulting in the \\noriginal sentence tree, as shown in the following diagram:\\nChapter 5\\n123\\nHere\\'s the code showing each step:\\n>>> from nltk.chunk.regexp import ChunkString\\n>>> from nltk.tree import Tree\\n>>> cs = ChunkString(Tree(\\'S\\', sent))\\n>>> cs\\n<ChunkString: \\'<DT><NN><NNS>\\'>\\n>>> ur.apply(cs)\\n>>> cs\\n<ChunkString: \\'<DT>{<NN>}<NNS>\\'>\\n>>> el.apply(cs)\\n>>> cs\\n<ChunkString: \\'{<DT><NN>}<NNS>\\'>\\n>>> er.apply(cs)\\n>>> cs\\n<ChunkString: \\'{<DT><NN><NNS>}\\'>\\n>>> un.apply(cs)\\n>>> cs\\n<ChunkString: \\'<DT><NN><NNS>\\'>\\nThere\\'s more...\\nIn practice, you can probably get away with only using the previous four rules: ChunkRule, \\nChinkRule, MergeRule, and SplitRule. But if you do need very ine-grained control \\nover chunk parsing and removing, now you know how to do it with the expansion and  \\nunchunk rules.\\nSee also\\nThe previous two recipes covered the more common chunk rules that are supported by \\nRegexpChunkRule.parse() and RegexpParser.\\nPartial parsing with regular expressions\\nSo far, we have only been parsing noun-phrases. But RegexpParser supports grammar with \\nmultiple phrase types, such as verb-phrases and prepositional-phrases. We can put the rules \\nwe have learned to use and deine a grammar that can be evaluated against the conll2000 \\ncorpus, which has NP, VP, and PP phrases.\\nExtracting Chunks\\n124\\nHow to do it...\\nWe will deine a grammar to parse three phrase types. For noun-phrases, we have a \\nChunkRule that looks for an optional determiner followed by one or more nouns. We \\nthen have a MergeRule for adding an adjective to the front of a noun chunk. For \\nprepositional-phrases, we simply chunk any IN word, such as \"in\" or \"on\". For verb-phrases, \\nwe chunk an optional modal word (such as \"should\") followed by a verb.\\nEach grammar rule is followed by a # comment. This comment is passed \\nin to each rule as the description. Comments are optional, but they \\ncan be helpful notes for understanding what the rule does, and will be \\nincluded in trace output.\\n>>> chunker = RegexpParser(r\\'\\'\\'\\n... NP:\\n... {<DT>?<NN.*>+}  # chunk optional determiner with nouns\\n... <JJ>{}<NN.*>  # merge adjective with noun chunk\\n... PP:\\n... {<IN>}  # chunk preposition\\n... VP:\\n... {<MD>?<VB.*>}  # chunk optional modal with verb\\n... \\'\\'\\')\\n>>> from nltk.corpus import conll2000\\n>>> score = chunker.evaluate(conll2000.chunked_sents())\\n>>> score.accuracy()\\n0.61485735457576884\\nWhen we call evaluate() on the chunker, we give it a list of chunked sentences and \\nget back a ChunkScore object, which can give us the accuracy of the chunker, along \\nwith a number of other metrics.\\nHow it works...\\nThe RegexpParser parses the grammar string into sets of rules, one set of rules for each \\nphrase type. These rules are used to create a RegexpChunkParser. The rules are parsed \\nusing RegexpChunkRule.parse(), which returns one of the ive subclasses: ChunkRule, \\nChinkRule, MergeRule, SplitRule, or ChunkRuleWithContext.\\nNow that the grammar has been translated into sets of rules, these rules are used to parse  \\na tagged sentence into a Tree structure. RegexpParser inherits from ChunkParserI, \\nwhich provides a parse() method to parse the tagged words. Whenever a part of the tagged \\ntokens match a chunk rule, a subtree is constructed so that the tagged tokens become the \\nleaves of a Tree whose node string is the chunk tag. ChunkParserI also provides the \\nevaluate() method, which compares the given chunked sentences to the output of the \\nparse() method to construct and return a ChunkScore object.\\nChapter 5\\n125\\nThere\\'s more...\\nYou can also evaluate this chunker on the treebank_chunk corpus.\\n>>> from nltk.corpus import treebank_chunk\\n>>> treebank_score = chunker.evaluate(treebank_chunk.chunked_sents())\\n>>> treebank_score.accuracy()\\n0.49033970276008493\\nThe treebank_chunk corpus is a special version of the treebank corpus that provides \\na chunked_sents() method. The regular treebank corpus cannot provide that method \\ndue to its ile format.\\nChunkScore metrics\\nChunkScore provides a few other metrics besides accuracy. Of the chunks the chunker \\nwas able to guess, precision tells you how many were correct. Recall tells you how well the \\nchunker did at inding correct chunks, compared to how many total chunks there were.\\n>>> score.precision()\\n0.60201948127375005\\n>>> score.recall()\\n0.60607250250584699\\nYou can also get lists of chunks that were missed by the chunker, chunks that were \\nincorrectly found, correct chunks, and guessed chunks. These can be useful to igure  \\nout how to improve your chunk grammar.\\n>>> len(score.missed())\\n47161\\n>>> len(score.incorrect())\\n47967\\n>>> len(score.correct())\\n119720\\n>>> len(score.guessed())\\n120526\\nAs you can see by the number of incorrect chunks, and by comparing guessed() and \\ncorrect(), our chunker guessed that there were more chunks that actually existed. \\nAnd it also missed a good number of correct chunks.\\nLooping and tracing\\nIf you want to apply the chunk rules in your grammar more than once, you pass loop=2 into \\nRegexpParser at initialization. The default is loop=1.\\nExtracting Chunks\\n126\\nTo watch an internal trace of the chunking process, pass trace=1 into RegexpParser. To \\nget even more output, pass in trace=2. This will give you a printout of what the chunker is \\ndoing as it is doing it. Rule comments/descriptions will be included in the trace output, giving \\nyou a good idea of which rule is applied when.\\nSee also\\nIf coming up with regular expression chunk patterns seems like too much work, then  \\nread the next recipes where we will cover how to train a chunker based on a corpus  \\nof chunked sentences.\\nTraining a tagger-based chunker\\nTraining a chunker can be a great alternative to manually specifying regular expression chunk \\npatterns. Instead of a painstaking process of trial and error to get the exact right patterns, we \\ncan use existing corpus data to train chunkers much like we did in Chapter 4, Part-of-Speech \\nTagging.\\nHow to do it...\\nAs with the part-of-speech tagging, we will use the treebank corpus data for training. But  \\nthis time we will use the treebank_chunk corpus, which is speciically formatted to \\nproduce chunked sentences in the form of trees. These chunked_sents() will be used \\nby a TagChunker class to train a tagger-based chunker. The TagChunker uses a helper \\nfunction conll_tag_chunks() to extract a list of (pos, iob) tuples from a list of Tree. \\nThese (pos, iob) tuples are then used to train a tagger in the same way (word, pos) \\ntuples were used in Chapter 4, Part-of-Speech Tagging to train part-of-speech taggers. But \\ninstead of learning part-of-speech tags for words, we are learning IOB tags for part-of-speech \\ntags. Here\\'s the code from chunkers.py:\\nimport nltk.chunk, itertools\\nfrom nltk.tag import UnigramTagger, BigramTagger\\nfrom tag_util import backoff_tagger\\ndef conll_tag_chunks(chunk_sents):\\n  tagged_sents = [nltk.chunk.tree2conlltags(tree) for tree in  \\nchunk_sents]\\n  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\\nclass TagChunker(nltk.chunk.ChunkParserI):\\n  def __init__(self, train_chunks, tagger_classes=[UnigramTagger, \\nBigramTagger]):\\n    train_sents = conll_tag_chunks(train_chunks)\\n    self.tagger = backoff_tagger(train_sents, tagger_classes)\\nChapter 5\\n127\\n  def parse(self, tagged_sent):\\n    if not tagged_sent: return None\\n    (words, tags) = zip(*tagged_sent)\\n    chunks = self.tagger.tag(tags)\\n    wtc = itertools.izip(words, chunks)\\n    return nltk.chunk.conlltags2tree([(w,t,c) for (w,(t,c)) in wtc])\\nOnce we have our trained TagChunker, we can then evaluate the ChunkScore the same \\nway we did for the RegexpParser in the previous recipes.\\n>>> from chunkers import TagChunker\\n>>> from nltk.corpus import treebank_chunk\\n>>> train_chunks = treebank_chunk.chunked_sents()[:3000]\\n>>> test_chunks = treebank_chunk.chunked_sents()[3000:]\\n>>> chunker = TagChunker(train_chunks)\\n>>> score = chunker.evaluate(test_chunks)\\n>>> score.accuracy()\\n0.97320393352514278\\n>>> score.precision()\\n0.91665343705350055\\n>>> score.recall()\\n0.9465573770491803\\nPretty darn accurate! Training a chunker is clearly a great alternative to manually speciied \\ngrammars and regular expressions.\\nHow it works...\\nRecall from the Creating a chunked phrase corpus recipe in Chapter 3, Creating Custom \\nCorpora that the conll2000 corpus deines chunks using IOB tags, which specify the type of \\nchunk and where it begins and ends. We can train a part-of-speech tagger on these IOB tag \\npatterns, and then use that to power a ChunkerI subclass. But irst we need to transform \\na Tree that you would get from the chunked_sents() method of a corpus into a format \\nusable by a part-of-speech tagger. This is what conll_tag_chunks() does. It uses nltk.\\nchunk.tree2conlltags() to convert a sentence Tree into a list of 3-tuples of the form \\n(word, pos, iob) where pos is the part-of-speech tag and iob is an IOB tag, such as B-NP \\nto mark the beginning of a noun-phrase, or I-NP to mark that the word is inside the noun-\\nphrase. The reverse of this method is nltk.chunk.conlltags2tree(). Here\\'s some code to \\ndemonstrate these nltk.chunk functions:\\n>>> import nltk.chunk\\n>>> from nltk.tree import Tree\\n>>> t = Tree(\\'S\\', [Tree(\\'NP\\', [(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\')])])\\n>>> nltk.chunk.tree2conlltags(t)\\n[(\\'the\\', \\'DT\\', \\'B-NP\\'), (\\'book\\', \\'NN\\', \\'I-NP\\')]\\n>>> nltk.chunk.conlltags2tree([(\\'the\\', \\'DT\\', \\'B-NP\\'), (\\'book\\', \\'NN\\', \\n\\'I-NP\\')])\\nTree(\\'S\\', [Tree(\\'NP\\', [(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\')])])\\nExtracting Chunks\\n128\\nThe next step is to convert these 3-tuples into 2-tuples that the tagger can recognize. Because \\nthe RegexpParser uses part-of-speech tags for chunk patterns, we will do that here too and \\nuse part-of-speech tags as if they were words to tag. By simply dropping the word from \\n3-tuple (word, pos, iob), the conll_tag_chunks() function returns a list of 2-tuples \\nof the form (pos, iob). When given the preceding example Tree in a list, the results are in \\na format we can feed to a tagger.\\n>>> conll_tag_chunks([t])\\n[[(\\'DT\\', \\'B-NP\\'), (\\'NN\\', \\'I-NP\\')]]\\nThe inal step is a subclass of ChunkParserI called TagChunker. It trains on a list of chunk \\ntrees using an internal tagger. This internal tagger is composed of a UnigramTagger and a \\nBigramTagger in a backoff chain, using the backoff_tagger() method created in the \\nTraining and combining Ngram taggers recipe in Chapter 4, Part-of-Speech Tagging.\\nFinally, ChunkerI subclasses must implement a parse() method that expects a part-of-speech \\ntagged sentence. We unzip that sentence into a list of words and part-of-speech tags. The tags \\nare then tagged by the tagger to get IOB tags, which are then re-combined with the words and \\npart-of-speech tags to create 3-tuples we can pass to nltk.chunk.conlltags2tree() to \\nreturn a inal Tree.\\nThere\\'s more...\\nSince we have been talking about the conll IOB tags, let us see how the TagChunker does \\non the conll2000 corpus:\\n>>> from nltk.corpus import conll2000\\n>>> conll_train = conll2000.chunked_sents(\\'train.txt\\')\\n>>> conll_test = conll2000.chunked_sents(\\'test.txt\\')\\n>>> chunker = TagChunker(conll_train)\\n>>> score = chunker.evaluate(conll_test)\\n>>> score.accuracy()\\n0.89505456234037617\\n>>> score.precision()\\n0.81148419743556754\\n>>> score.recall()\\n0.86441916769448635\\nNot quite as good as on treebank_chunk, but conll2000 is a much larger corpus, so it\\'s \\nnot too surprising.\\nChapter 5\\n129\\nUsing different taggers\\nIf you want to use different tagger classes with the TagChunker, you can pass them in as \\ntagger_classes. For example, here\\'s the TagChunker using just a UnigramTagger:\\n>>> from nltk.tag import UnigramTagger\\n>>> uni_chunker = TagChunker(train_chunks, tagger_\\nclasses=[UnigramTagger])\\n>>> score = uni_chunker.evaluate(test_chunks)\\n>>> score.accuracy()\\n0.96749259243354657\\nThe tagger_classes will be passed directly into the backoff_tagger() function, \\nwhich means they must be subclasses of SequentialBackoffTagger. In testing, \\nthe default of tagger_classes=[UnigramTagger, BigramTagger] produces the \\nbest results.\\nSee also\\nThe Training and combining Ngram taggers recipe in Chapter 4, Part-of-Speech Tagging \\ncovers backoff tagging with a UnigramTagger and BigramTagger. ChunkScore metrics \\nreturned by the evaluate() method of a chunker were explained in the previous recipe.\\nClassiication-based chunking\\nUnlike most part-of-speech taggers, the ClassifierBasedTagger learns from features. \\nThat means we can create a ClassifierChunker that can learn from both the words and \\npart-of-speech tags, instead of only the part-of-speech tags as the TagChunker does.\\nHow to do it...\\nFor the ClassifierChunker, we don\\'t want to discard the words from the training \\nsentences, as we did in the previous recipe. Instead, to remain compatible with the 2-tuple \\n(word, pos) format required for training a ClassiferBasedTagger, we convert the \\n(word, pos, iob) 3-tuples from nltk.chunk.tree2conlltags() into ((word, \\npos), iob) 2-tuples using the chunk_trees2train_chunks() function. This code \\ncan be found in chunkers.py:\\nimport nltk.chunk\\nfrom nltk.tag import ClassifierBasedTagger\\ndef chunk_trees2train_chunks(chunk_sents):\\n  tag_sents = [nltk.chunk.tree2conlltags(sent) for sent in chunk_\\nsents]\\n  return [[((w,t),c) for (w,t,c) in sent] for sent in tag_sents]\\nExtracting Chunks\\n130\\nNext, we need a feature detector function to pass into ClassifierBasedTagger. Our \\ndefault feature detector function, prev_next_pos_iob(), knows that the list of tokens \\nis really a list of (word, pos) tuples, and can use that to return a feature set suitable for a \\nclassiier. To give the classiier as much information as we can, this feature set contains the \\ncurrent, previous and next word, and part-of-speech tag, along with the previous IOB tag.\\ndef prev_next_pos_iob(tokens, index, history):\\n  word, pos = tokens[index]\\n  if index == 0:\\n    prevword, prevpos, previob = (\\'<START>\\',)*3\\n  else:\\n    prevword, prevpos = tokens[index-1]\\n    previob = history[index-1]\\n  if index == len(tokens) - 1:\\n    nextword, nextpos = (\\'<END>\\',)*2\\n  else:\\n    nextword, nextpos = tokens[index+1]\\n  feats = {\\n    \\'word\\': word,\\n    \\'pos\\': pos,\\n    \\'nextword\\': nextword,\\n    \\'nextpos\\': nextpos,\\n    \\'prevword\\': prevword,\\n    \\'prevpos\\': prevpos,\\n    \\'previob\\': previob\\n  }\\n  return feats\\nNow we can deine the ClassifierChunker, which uses an internal \\nClassifierBasedTagger with features extracted using prev_next_pos_iob(), \\nand training sentences from chunk_trees2train_chunks(). As a subclass of \\nChunkerParserI, it implements the parse() method, which converts the ((w, t), c) \\ntuples produced by the internal tagger into a Tree using nltk.chunk.conlltags2tree().\\nclass ClassifierChunker(nltk.chunk.ChunkParserI):\\n  def __init__(self, train_sents, feature_detector=prev_next_pos_iob, \\n**kwargs):\\n    if not feature_detector:\\n      feature_detector = self.feature_detector\\n    train_chunks = chunk_trees2train_chunks(train_sents)\\n    self.tagger = ClassifierBasedTagger(train=train_chunks,\\n      feature_detector=feature_detector, **kwargs)\\nChapter 5\\n131\\n  def parse(self, tagged_sent):\\n    if not tagged_sent: return None\\n    chunks = self.tagger.tag(tagged_sent)\\n    return nltk.chunk.conlltags2tree([(w,t,c) for ((w,t),c) in \\nchunks])\\nUsing the same train_chunks and test_chunks from the treebank_chunk corpus in \\nthe previous recipe, we can evaluate this code from chunkers.py:\\n>>> from chunkers import ClassifierChunker\\n>>> chunker = ClassifierChunker(train_chunks)\\n>>> score = chunker.evaluate(test_chunks)\\n>>> score.accuracy()\\n0.97217331558380216\\n>>> score.precision()\\n0.92588387933830685\\n>>> score.recall()\\n0.93590163934426229\\nCompared to the TagChunker, all the scores have gone up a bit. Let us see how it does on \\nconll2000:\\n>>> chunker = ClassifierChunker(conll_train)\\n>>> score = chunker.evaluate(conll_test)\\n>>> score.accuracy()\\n0.92646220740021534\\n>>> score.precision()\\n0.87379243109102189\\n>>> score.recall()\\n0.90073546206203459\\nThis is much improved over the TagChunker.\\nHow it works...\\nLike the TagChunker in the previous recipe, we are training a part-of-speech tagger for IOB \\ntagging. But in this case, we want to include the word as a feature to power a classiier. By \\ncreating nested 2-tuples of the form ((word, pos), iob), we can pass the word through \\nthe tagger into our feature detector function. chunk_trees2train_chunks() produces \\nthese nested 2-tuples, and prev_next_pos_iob() is aware of them and uses each \\nelement as a feature. The following features are extracted:\\n \\nf\\nThe current word and part-of-speech tag\\n \\nf\\nThe previous word, part-of-speech tag, and IOB tag\\n \\nf\\nThe next word and part-of-speech tag\\nExtracting Chunks\\n132\\nThe arguments to prev_next_pos_iob() look the same as the feature_detector() \\nmethod of the ClassifierBasedTagger: tokens, index, and history. But this time, \\ntokens will be a list of (word, pos) 2-tuples, and history will be a list of IOB tags. \\nThe special feature values \\'<START>\\' and \\'<END>\\' are used if there are no previous \\nor next tokens.\\nThe ClassifierChunker uses an internal ClassifierBasedTagger and \\nprev_next_pos_iob() as its default feature_detector. The results from the tagger, \\nwhich are in the same nested 2-tuple form, are then reformatted into 3-tuples to return a  \\ninal Tree using nltk.chunk.conlltags2tree().\\nThere\\'s more...\\nYou can use your own feature detector function by passing it in to the ClassifierChunker \\nas feature_detector. The tokens will contain a list of (word, tag) tuples, and \\nhistory will be a list of the previous IOB tags found.\\nUsing a different classiier builder\\nThe ClassifierBasedTagger defaults to using NaiveBayesClassifier.train \\nas its classifier_builder. But you can use any classiier you want by overriding \\nthe classifier_builder keyword argument. Here\\'s an example using \\nMaxentClassifier.train:\\n>>> from nltk.classify import MaxentClassifier\\n>>> builder = lambda toks: MaxentClassifier.train(toks, trace=0, max_\\niter=10, min_lldelta=0.01)\\n>>> me_chunker = ClassifierChunker(train_chunks, classifier_\\nbuilder=builder)\\n>>> score = me_chunker.evaluate(test_chunks)\\n>>> score.accuracy()\\n0.9748357452655988\\n>>> score.precision()\\n0.93794355504208615\\n>>> score.recall()\\n0.93163934426229511\\nInstead of using MaxentClassifier.train directly, it has been wrapped in a lambda so \\nthat its output is quiet (trace=0) and it inishes in a reasonable amount of time. As you can \\nsee, the scores are slightly different compared to using the NaiveBayesClassifier.\\nChapter 5\\n133\\nSee also\\nThe previous recipe, Training a tagger-based chunker, introduced the idea of using a \\npart-of-speech tagger for training a chunker. The Classiier-based tagging recipe in \\nChapter 4, Part-of-Speech Tagging describes ClassifierBasedPOSTagger, which \\nis a subclass of ClassifierBasedTagger. In Chapter 7, Text Classiication, we will \\ncover classiication in detail.\\nExtracting named entities\\nNamed entity recognition is a speciic kind of chunk extraction that uses entity tags instead \\nof, or in addition to, chunk tags. Common entity tags include PERSON, ORGANIZATION, and \\nLOCATION. Part-of-speech tagged sentences are parsed into chunk trees as with normal \\nchunking, but the nodes of the trees can be entity tags instead of chunk phrase tags.\\nHow to do it...\\nNLTK comes with a pre-trained named entity chunker. This chunker has been trained on \\ndata from the ACE program, a NIST (National Institute of Standards and Technology) \\nsponsored program for Automatic Content Extraction, which you can read more about here: \\nhttp://www.itl.nist.gov/iad/894.01/tests/ace/. Unfortunately, this data is not \\nincluded in the NLTK corpora, but the trained chunker is. This chunker can be used through \\nthe ne_chunk() method in the nltk.chunk module. ne_chunk() will chunk a single \\nsentence into a Tree. The following is an example using ne_chunk() on the irst tagged \\nsentence of the treebank_chunk corpus:\\n>>> from nltk.chunk import ne_chunk\\n>>> ne_chunk(treebank_chunk.tagged_sents()[0])\\nTree(\\'S\\', [Tree(\\'PERSON\\', [(\\'Pierre\\', \\'NNP\\')]), Tree(\\'ORGANIZATION\\',  \\n[(\\'Vinken\\', \\'NNP\\')]), (\\',\\', \\',\\'), (\\'61\\', \\'CD\\'), (\\'years\\', \\'NNS\\'),  \\n(\\'old\\', \\'JJ\\'), (\\',\\', \\',\\'), (\\'will\\', \\'MD\\'), (\\'join\\', \\'VB\\'), (\\'the\\',  \\n\\'DT\\'), (\\'board\\', \\'NN\\'), (\\'as\\', \\'IN\\'), (\\'a\\', \\'DT\\'), (\\'nonexecutive\\',  \\n\\'JJ\\'), (\\'director\\', \\'NN\\'), (\\'Nov.\\', \\'NNP\\'), (\\'29\\', \\'CD\\'), (\\'.\\',  \\n\\'.\\')])\\nYou can see two entity tags are found: PERSON and ORGANIZATION. Each of these subtrees \\ncontain a list of the words that are recognized as a PERSON or ORGANIZATION. To extract \\nthese named entities, we can write a simple helper method that will get the leaves of all the \\nsubtrees we are interested in.\\ndef sub_leaves(tree, node):\\n  return [t.leaves() for t in tree.subtrees \\n    (lambda s: s.node == node)]\\nExtracting Chunks\\n134\\nThen we can call this method to get all the PERSON or ORGANIZATION leaves from a tree.\\n>>> tree = ne_chunk(treebank_chunk.tagged_sents()[0])\\n>>> from chunkers import sub_leaves\\n>>> sub_leaves(tree, \\'PERSON\\')\\n[[(\\'Pierre\\', \\'NNP\\')]]\\n>>> sub_leaves(tree, \\'ORGANIZATION\\')\\n[[(\\'Vinken\\', \\'NNP\\')]]\\nYou may notice that the chunker has mistakenly separated \"Vinken\" into its own \\nORGANIZATION Tree instead of including it with the PERSON Tree containing \"Pierre\". Such \\nis the case with statistical natural language processing—you can\\'t always expect perfection.\\nHow it works...\\nThe pre-trained named entity chunker is much like any other chunker, and in fact uses a \\nMaxentClassifier powered ClassifierBasedTagger to determine IOB tags. But \\ninstead of B-NP and I-NP IOB tags, it uses B-PERSON, I-PERSON, B-ORGANIZATION, \\nI-ORGANIZATION, and more. It also uses the O tag to mark words that are not part of a \\nnamed entity (and thus outside the named entity subtrees).\\nThere\\'s more...\\nTo process multiple sentences at a time, you can use batch_ne_chunk(). Here\\'s an example \\nwhere we process the irst 10 sentences from treebank_chunk.tagged_sents() and \\nget the ORGANIZATION sub_leaves():\\n>>> from nltk.chunk import batch_ne_chunk\\n>>> trees = batch_ne_chunk(treebank_chunk.tagged_sents()[:10])\\n>>> [sub_leaves(t, \\'ORGANIZATION\\') for t in trees]\\n[[[(\\'Vinken\\', \\'NNP\\')]], [[(\\'Elsevier\\', \\'NNP\\')]],  \\n[[(\\'Consolidated\\', \\'NNP\\'), (\\'Gold\\', \\'NNP\\'), (\\'Fields\\',  \\n\\'NNP\\')]], [], [], [[(\\'Inc.\\', \\'NNP\\')], [(\\'Micronite\\',  \\n\\'NN\\')]], [[(\\'New\\', \\'NNP\\'), (\\'England\\', \\'NNP\\'), (\\'Journal\\',  \\n\\'NNP\\')]], [[(\\'Lorillard\\', \\'NNP\\')]], [], []]\\nYou can see there are a couple of multi-word ORGANIZATION chunks, such as \"New England \\nJournal\". There are also a few sentences that have no ORGANIZATION chunks, as indicated \\nby the empty lists [].\\nChapter 5\\n135\\nBinary named entity extraction\\nIf you don\\'t care about the particular kind of named entity to extract, you can pass \\nbinary=True into ne_chunk() or batch_ne_chunk(). Now, all named entities \\nwill be tagged with NE:\\n>>> ne_chunk(treebank_chunk.tagged_sents()[0], binary=True)\\nTree(\\'S\\', [Tree(\\'NE\\', [(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\')]),  \\n(\\',\\', \\',\\'), (\\'61\\', \\'CD\\'), (\\'years\\', \\'NNS\\'), (\\'old\\', \\'JJ\\'),  \\n(\\',\\', \\',\\'), (\\'will\\', \\'MD\\'), (\\'join\\', \\'VB\\'), (\\'the\\', \\'DT\\'),  \\n(\\'board\\', \\'NN\\'), (\\'as\\', \\'IN\\'), (\\'a\\', \\'DT\\'), (\\'nonexecutive\\',  \\n\\'JJ\\'), (\\'director\\', \\'NN\\'), (\\'Nov.\\', \\'NNP\\'), (\\'29\\', \\'CD\\'),  \\n(\\'.\\', \\'.\\')])\\nIf we get the sub_leaves(), we can see that \"Pierre Vinken\" is correctly combined into \\na single named entity.\\n>>> sub_leaves(ne_chunk(treebank_chunk.tagged_sents()[0],  \\nbinary=True), \\'NE\\')\\n[[(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\')]]\\nSee also\\nIn the next recipe, we will create our own simple named entity chunker.\\nExtracting proper noun chunks\\nA simple way to do named entity extraction is to chunk all proper nouns (tagged with NNP). We \\ncan tag these chunks as NAME, since the deinition of a proper noun is the name of a person, \\nplace, or thing.\\nHow to do it...\\nUsing the RegexpParser, we can create a very simple grammar that combines all \\nproper nouns into a NAME chunk. Then we can test this on the irst tagged sentence \\nof treebank_chunk to compare the results to the previous recipe.\\n>>> chunker = RegexpParser(r\\'\\'\\'\\n... NAME:\\n...   {<NNP>+}\\n... \\'\\'\\')\\n>>> sub_leaves(chunker.parse(treebank_chunk.tagged_sents()[0]),  \\n\\'NAME\\')\\n[[(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\')], [(\\'Nov.\\', \\'NNP\\')]]\\nAlthough we get \"Nov.\" as a NAME chunk, this isn\\'t a wrong result, as \"Nov.\" is the name \\nof a month.\\nExtracting Chunks\\n136\\nHow it works...\\nThe NAME chunker is a simple usage of the RegexpParser, covered in Chunking and \\nchinking with regular expressions, Merging and splitting chunks with regular expressions, \\nand Partial parsing with regular expressions recipes of this chapter. All sequences of NNP \\ntagged words are combined into NAME chunks.\\nThere\\'s more...\\nIf we wanted to be sure to only chunk the names of people, then we can build a \\nPersonChunker that uses the names corpus for chunking. This class can be \\nfound in chunkers.py:\\nimport nltk.chunk\\nfrom nltk.corpus import names\\nclass PersonChunker(nltk.chunk.ChunkParserI):\\n  def __init__(self):\\n    self.name_set = set(names.words())\\n  def parse(self, tagged_sent):\\n    iobs = []\\n    in_person = False\\n    for word, tag in tagged_sent:\\n      if word in self.name_set and in_person:\\n        iobs.append((word, tag, \\'I-PERSON\\'))\\n      elif word in self.name_set:\\n        iobs.append((word, tag, \\'B-PERSON\\'))\\n        in_person = True\\n      else:\\n        iobs.append((word, tag, \\'O\\'))\\n        in_person = False\\n    return nltk.chunk.conlltags2tree(iobs)\\nThe PersonChunker iterates over the tagged sentence, checking if each word is in its names_\\nset (constructed from the names corpus). If the current word is in the names_set, then it uses \\neither the B-PERSON or I-PERSON IOB tags, depending on whether the previous word was also \\nin the names_set. Any word that\\'s not in the names_set gets the O IOB tag. When complete, \\nthe list of IOB tags is converted to a Tree using nltk.chunk.conlltags2tree(). Using it \\non the same tagged sentence as before, we get the following result:\\n>>> from chunkers import PersonChunker\\n>>> chunker = PersonChunker()\\n>>> sub_leaves(chunker.parse(treebank_chunk.tagged_sents()[0]),  \\n\\'PERSON\\')\\n[[(\\'Pierre\\', \\'NNP\\')]]\\nChapter 5\\n137\\nWe no longer get \"Nov.\", but we have also lost \"Vinken\", as it is not found in the names corpus. \\nThis recipe highlights some of the dificulties of chunk extraction and natural language \\nprocessing in general:\\n \\nf\\nIf you use general patterns, you will get general results\\n \\nf\\nIf you are looking for speciic results, you must use speciic data\\n \\nf\\nIf your speciic data is incomplete, your results will be incomplete too\\nSee also\\nThe previous recipe deines the sub_leaves() method used to show the found \\nchunks. In the next recipe, we will cover how to ind LOCATION chunks based on the \\ngazetteers corpus.\\nExtracting location chunks\\nTo identify location chunks, we can make a different kind of ChunkParserI subclass \\nthat uses the gazetteers corpus to identify location words. gazetteers is a \\nWordListCorpusReader that contains the following location words:\\n \\nf\\nCountry names\\n \\nf\\nU.S. states and abbreviations\\n \\nf\\nMajor U.S. cities\\n \\nf\\nCanadian provinces\\n \\nf\\nMexican states\\nHow to do it...\\nThe LocationChunker, found in chunkers.py, iterates over a tagged sentence looking \\nfor words that are found in the gazetteers corpus. When it inds one or more location \\nwords, it creates a LOCATION chunk using IOB tags. The helper method iob_locations() \\nis where the IOB LOCATION tags are produced, and the parse() method converts these IOB \\ntags to a Tree.\\nimport nltk.chunk\\nfrom nltk.corpus import gazetteers\\nclass LocationChunker(nltk.chunk.ChunkParserI):\\n  def __init__(self):\\n    self.locations = set(gazetteers.words())\\n    self.lookahead = 0\\nExtracting Chunks\\n138\\n    for loc in self.locations:\\n      nwords = loc.count(\\' \\')\\n      if nwords > self.lookahead:\\n        self.lookahead = nwords\\n  def iob_locations(self, tagged_sent):\\n    i = 0\\n    l = len(tagged_sent)\\n    inside = False\\n    while i < l:\\n      word, tag = tagged_sent[i]\\n      j = i + 1\\n      k = j + self.lookahead\\n      nextwords, nexttags = [], []\\n      loc = False\\n      while j < k:\\n        if \\' \\'.join([word] + nextwords) in self.locations:\\n          if inside:\\n            yield word, tag, \\'I-LOCATION\\'\\n          else:\\n            yield word, tag, \\'B-LOCATION\\'\\n          for nword, ntag in zip(nextwords, nexttags):\\n            yield nword, ntag, \\'I-LOCATION\\'\\n          loc, inside = True, True\\n          i = j\\n          break\\n        if j < l:\\n          nextword, nexttag = tagged_sent[j]\\n          nextwords.append(nextword)\\n          nexttags.append(nexttag)\\n          j += 1\\n        else:\\n          break\\n      if not loc:\\n        inside = False\\n        i += 1\\n        yield word, tag, \\'O\\'\\nChapter 5\\n139\\n  def parse(self, tagged_sent):\\n    iobs = self.iob_locations(tagged_sent)\\n    return nltk.chunk.conlltags2tree(iobs)\\nWe can use the LocationChunker to parse the following sentence into two locations, \"San \\nFrancisco, CA is cold compared to San Jose, CA\":\\n>>> from chunkers import LocationChunker\\n>>> t = loc.parse([(\\'San\\', \\'NNP\\'), (\\'Francisco\\', \\'NNP\\'),  \\n(\\'CA\\', \\'NNP\\'), (\\'is\\', \\'BE\\'), (\\'cold\\', \\'JJ\\'), (\\'compared\\',  \\n\\'VBD\\'), (\\'to\\', \\'TO\\'), (\\'San\\', \\'NNP\\'), (\\'Jose\\', \\'NNP\\'),  \\n(\\'CA\\', \\'NNP\\')])\\n>>> sub_leaves(t, \\'LOCATION\\')\\n[[(\\'San\\', \\'NNP\\'), (\\'Francisco\\', \\'NNP\\'), (\\'CA\\', \\'NNP\\')],  \\n[(\\'San\\', \\'NNP\\'), (\\'Jose\\', \\'NNP\\'), (\\'CA\\', \\'NNP\\')]]\\nAnd the result is that we get two LOCATION chunks, just as expected.\\nHow it works...\\nThe LocationChunker starts by constructing a set of all locations in the gazetteers \\ncorpus. Then it inds the maximum number of words in a single location string, so it knows \\nhow many words it must look ahead when parsing a tagged sentence.\\nThe parse() method calls a helper method iob_locations(), which generates 3-tuples \\nof the form (word, pos, iob) where iob is either O if the word is not a location, or \\nB-LOCATION or I-LOCATION for LOCATION chunks. iob_locations() inds location \\nchunks by looking at the current word and the next words to check if the combined word is in \\nthe locations set. Multiple location words that are next to each other are then put into the \\nsame LOCATION chunk, such as in the preceding example with \"San Francisco\" and \"CA\".\\nLike in the previous recipe, it\\'s simpler and more convenient to construct a list of (word, \\npos, iob) tuples to pass in to nltk.chunk.conlltags2tree() to return a Tree. The \\nalternative is to construct a Tree manually, but that requires keeping track of children, \\nsubtrees, and where you currently are in the Tree.\\nThere\\'s more...\\nOne of the nice aspects of this LocationChunker is that it doesn\\'t care about the \\npart-of-speech tags. As long as the location words are found in the locations set, any  \\npart-of-speech tag will do.\\nExtracting Chunks\\n140\\nSee also\\nIn the next recipe, we will cover how to train a named entity chunker using the ieer corpus.\\nTraining a named entity chunker\\nYou can train your own named entity chunker using the ieer corpus, which stands for \\nInformation Extraction—Entity Recognition (ieer). It takes a bit of extra work though, \\nbecause the ieer corpus has chunk trees, but no part-of-speech tags for words.\\nHow to do it...\\nUsing the ieertree2conlltags() and ieer_chunked_sents() functions in \\nchunkers.py, we can create named entity chunk trees from the ieer corpus to train the \\nClassifierChunker created in Classiication-based chunking recipe of this chapter.\\nimport nltk.tag, nltk.chunk, itertools\\nfrom nltk.corpus import ieer\\ndef ieertree2conlltags(tree, tag=nltk.tag.pos_tag):\\n  words, ents = zip(*tree.pos())\\n  iobs = []\\n  prev = None\\n  for ent in ents:\\n    if ent == tree.node:\\n      iobs.append(\\'O\\')\\n      prev = None\\n    elif prev == ent:\\n      iobs.append(\\'I-%s\\' % ent)\\n    else:\\n      iobs.append(\\'B-%s\\' % ent)\\n      prev = ent\\n  words, tags = zip(*tag(words))\\n  return itertools.izip(words, tags, iobs)\\ndef ieer_chunked_sents(tag=nltk.tag.pos_tag):\\n  for doc in ieer.parsed_docs():\\n    tagged = ieertree2conlltags(doc.text, tag)\\n    yield nltk.chunk.conlltags2tree(tagged)\\nChapter 5\\n141\\nWe will use 80 out of 94 sentences for training, and the rest for testing. Then we can see how \\nit does on the irst sentence of the treebank_chunk corpus.\\n>>> from chunkers import ieer_chunked_sents, ClassifierChunker\\n>>> from nltk.corpus import treebank_chunk\\n>>> ieer_chunks = list(ieer_chunked_sents())\\n>>> len(ieer_chunks)\\n94\\n>>> chunker = ClassifierChunker(ieer_chunks[:80])\\n>>> chunker.parse(treebank_chunk.tagged_sents()[0])\\nTree(\\'S\\', [Tree(\\'LOCATION\\', [(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\')]),  \\n(\\',\\', \\',\\'), Tree(\\'DURATION\\', [(\\'61\\', \\'CD\\'), (\\'years\\', \\'NNS\\')]),  \\nTree(\\'MEASURE\\', [(\\'old\\', \\'JJ\\')]), (\\',\\', \\',\\'), (\\'will\\', \\'MD\\'),  \\n(\\'join\\', \\'VB\\'), (\\'the\\', \\'DT\\'), (\\'board\\', \\'NN\\'), (\\'as\\', \\'IN\\'), (\\'a\\',  \\n\\'DT\\'), (\\'nonexecutive\\', \\'JJ\\'), (\\'director\\', \\'NN\\'), Tree(\\'DATE\\',  \\n[(\\'Nov.\\', \\'NNP\\'), (\\'29\\', \\'CD\\')]), (\\'.\\', \\'.\\')])\\nSo it found a correct DURATION and DATE, but tagged \"Pierre Vinken\" as a LOCATION. Let us \\nsee how it scores against the rest of ieer chunk trees:\\n>>> score = chunker.evaluate(ieer_chunks[80:])\\n>>> score.accuracy()\\n0.88290183880706252\\n>>> score.precision()\\n0.40887174541947929\\n>>> score.recall()\\n0.50536352800953521\\nAccuracy is pretty good, but precision and recall are very low. That means lots of false \\nnegatives and false positives.\\nHow it works...\\nThe truth is, we are not working with ideal training data. The ieer trees generated by \\nieer_chunked_sents() are not entirely accurate. First, there are no explicit sentence \\nbreaks, so each document is a single tree. Second, the words are not explicitly tagged,  \\nso we have to guess using nltk.tag.pos_tag().\\nThe ieer corpus provides a parsed_docs() method that returns a list of documents with a \\ntext attribute. This text attribute is a document Tree that is converted to a list of 3-tuples \\nof the form (word, pos, iob). To get these inal 3-tuples, we must irst latten the Tree \\nusing tree.pos(), which returns a list of 2-tuples of the form (word, entity), where \\nentity is either the entity tag or the top tag of the tree. Any words whose entity is the top tag \\nare outside the named entity chunks and get the IOB tag O. All words that have unique entity \\ntags are either the beginning of or inside a named entity chunk. Once we have all the IOB \\ntags, then we can get the part-of-speech tags of all the words and join the words, part-of-\\nspeech tags, and IOB tags into 3-tuples using itertools.izip().\\nExtracting Chunks\\n142\\nThere\\'s more...\\nDespite the non-ideal training data, the ieer corpus provides a good place to start for training \\na named entity chunker. The data comes from the New York Times and AP Newswire reports. \\nEach doc from ieer.parsed_docs() also contains a headline attribute that is a Tree.\\n>>> from nltk.corpus import ieer\\n>>> ieer.parsed_docs()[0].headline\\nTree(\\'DOCUMENT\\', [\\'Kenyans\\', \\'protest\\', \\'tax\\', \\'hikes\\'])\\nSee also\\nThe Extracting named entities recipe in this chapter, covers the pre-trained named entity \\nchunker that comes included with NLTK.\\n6\\nTransforming Chunks \\nand Trees\\nIn this chapter, we will cover:\\n \\nf\\nFiltering insigniicant words\\n \\nf\\nCorrecting verb forms\\n \\nf\\nSwapping verb phrases\\n \\nf\\nSwapping noun cardinals\\n \\nf\\nSwapping ininitive phrases\\n \\nf\\nSingularizing plural nouns\\n \\nf\\nChaining chunk transformations\\n \\nf\\nConverting a chunk tree to text\\n \\nf\\nFlattening a deep tree\\n \\nf\\nCreating a shallow tree\\n \\nf\\nConverting tree nodes\\nIntroduction\\nNow that you know how to get chunks/phrases from a sentence, what do you do with them? \\nThis chapter will show you how to do various transforms on both chunks and trees. The chunk \\ntransforms are for grammatical correction and rearranging phrases without loss of meaning. \\nThe tree transforms give you ways to modify and latten deep parse trees.\\nTransforming Chunks and Trees\\n144\\nThe functions detailed in these recipes modify data, as opposed to learning from it. That \\nmeans it\\'s not safe to apply them indiscriminately. A thorough knowledge of the data you want \\nto transform, along with a few experiments, should help you decide which functions to apply \\nand when.\\nWhenever the term chunk is used in this chapter, it could refer to an actual chunk extracted \\nby a chunker, or it could simply refer to a short phrase or sentence in the form of a list of \\ntagged words. What\\'s important in this chapter is what you can do with a chunk, not where it \\ncame from.\\nFiltering insigniicant words\\nMany of the most commonly used words are insigniicant when it comes to discerning the \\nmeaning of a phrase. For example, in the phrase \"the movie was terrible\", the most signiicant \\nwords are \"movie\" and \"terrible\", while \"the\" and \"was\" are almost useless. You could get the \\nsame meaning if you took them out, such as \"movie terrible\" or \"terrible movie\". Either way, \\nthe sentiment is the same. In this recipe, we\\'ll learn how to remove the insigniicant words, \\nand keep the signiicant ones, by looking at their part-of-speech tags.\\nGetting ready\\nFirst, we need to decide which part-of-speech tags are signiicant and which are not. Looking \\nthrough the treebank corpus for stopwords yields the following table of insigniicant words \\nand tags:\\nWord\\nTag\\na\\nDT\\nall\\nPDT\\nan\\nDT\\nand\\nCC\\nor\\nCC\\nthat\\nWDT\\nthe\\nDT\\nOther than CC, all the tags end with DT. This means we can ilter out insigniicant words by \\nlooking at the tag\\'s sufix.\\nChapter 6\\n145\\nHow to do it...\\nIn transforms.py there is a function called filter_insignificant(). It takes a \\nsingle chunk, which should be a list of tagged words, and returns a new chunk without any \\ninsigniicant tagged words. It defaults to iltering out any tags that end with DT or CC.\\ndef filter_insignificant(chunk, tag_suffixes=[\\'DT\\', \\'CC\\']):\\n  good = []\\n  \\n  for word, tag in chunk:\\n    ok = True\\n    \\n    for suffix in tag_suffixes:\\n      if tag.endswith(suffix):\\n        ok = False\\n        break\\n    \\n    if ok:\\n      good.append((word, tag))\\n  \\n  return good\\nNow we can use it on the part-of-speech tagged version of \"the terrible movie\".\\n>>> from transforms import filter_insignificant\\n>>> filter_insignificant([(\\'the\\', \\'DT\\'), (\\'terrible\\', \\'JJ\\'), (\\'movie\\', \\n\\'NN\\')])\\n[(\\'terrible\\', \\'JJ\\'), (\\'movie\\', \\'NN\\')]\\nAs you can see, the word \"the\" is eliminated from the chunk.\\nHow it works...\\nfilter_insignificant() iterates over the tagged words in the chunk. For each tag, it \\nchecks if that tag ends with any of the tag_suffixes. If it does, then the tagged word is \\nskipped. However if the tag is ok, then the tagged word is appended to a new good chunk that \\nis returned.\\nThere\\'s more...\\nThe way filter_insignificant() is deined, you can pass in your own tag sufixes if DT \\nand CC are not enough, or are incorrect for your case. For example, you might decide that \\npossessive words and pronouns such as \"you\", \"your\", \"their\", and \"theirs\" are no good but DT \\nand CC words are ok. The tag sufixes would then be PRP and PRP$. Following is an example \\nof this function:\\nTransforming Chunks and Trees\\n146\\n>>> filter_insignificant([(\\'your\\', \\'PRP$\\'), (\\'book\\', \\'NN\\'), (\\'is\\', \\n\\'VBZ\\'), (\\'great\\', \\'JJ\\')], tag_suffixes=[\\'PRP\\', \\'PRP$\\'])\\n[(\\'book\\', \\'NN\\'), (\\'is\\', \\'VBZ\\'), (\\'great\\', \\'JJ\\')]\\nFiltering insigniicant words can be a good complement to stopword iltering for purposes such \\nas search engine indexing, querying, and text classiication.\\nSee also\\nThis recipe is analogous to the Filtering stopwords in a tokenized sentence recipe in \\nChapter 1, Tokenizing Text and WordNet Basics.\\nCorrecting verb forms\\nIt\\'s fairly common to ind incorrect verb forms in real-world language. For example, the correct \\nform of \"is our children learning?\" is \"are our children learning?\". The verb \"is\" should only be \\nused with singular nouns, while \"are\" is for plural nouns, such as \"children\". We can correct \\nthese mistakes by creating verb correction mappings that are used depending on whether \\nthere\\'s a plural or singular noun in the chunk.\\nGetting ready\\nWe irst need to deine the verb correction mappings in transforms.py. We\\'ll create two \\nmappings, one for plural to singular, and another for singular to plural.\\nplural_verb_forms = {\\n  (\\'is\\', \\'VBZ\\'): (\\'are\\', \\'VBP\\'),\\n  (\\'was\\', \\'VBD\\'): (\\'were\\', \\'VBD\\')\\n}\\nsingular_verb_forms = {\\n  (\\'are\\', \\'VBP\\'): (\\'is\\', \\'VBZ\\'),\\n  (\\'were\\', \\'VBD\\'): (\\'was\\', \\'VBD\\')\\n}\\nEach mapping has a tagged verb that maps to another tagged verb. These initial mappings \\ncover the basics of mapping, is to are, was to were, and vice versa.\\nChapter 6\\n147\\nHow to do it...\\nIn transforms.py there is a function called correct_verbs(). Pass it a chunk with \\nincorrect verb forms, and you\\'ll get a corrected chunk back. It uses a helper function \\nfirst_chunk_index() to search the chunk for the position of the irst tagged word \\nwhere pred returns True.\\ndef first_chunk_index(chunk, pred, start=0, step=1):\\n  l = len(chunk)\\n  end = l if step > 0 else -1\\n  \\n  for i in range(start, end, step):\\n    if pred(chunk[i]):\\n      return i\\n  \\n  return None\\ndef correct_verbs(chunk):\\n  vbidx = first_chunk_index(chunk, lambda (word, tag): tag.\\nstartswith(\\'VB\\'))\\n  # if no verb found, do nothing\\n  if vbidx is None:\\n    return chunk\\n  \\n  verb, vbtag = chunk[vbidx]\\n  nnpred = lambda (word, tag): tag.startswith(\\'NN\\')\\n  # find nearest noun to the right of verb\\n  nnidx = first_chunk_index(chunk, nnpred, start=vbidx+1)\\n  # if no noun found to right, look to the left\\n  if nnidx is None:\\n    nnidx = first_chunk_index(chunk, nnpred, start=vbidx-1, step=-1)\\n  # if no noun found, do nothing\\n  if nnidx is None:\\n    return chunk\\n  \\n  noun, nntag = chunk[nnidx]\\n  # get correct verb form and insert into chunk\\n  if nntag.endswith(\\'S\\'):\\n    chunk[vbidx] = plural_verb_forms.get((verb, vbtag), (verb, vbtag))\\n  else:\\n    chunk[vbidx] = singular_verb_forms.get((verb, vbtag), (verb, \\nvbtag))\\n  \\n  return chunk\\nTransforming Chunks and Trees\\n148\\nWhen we call it on a part-of-speech tagged \"is our children learning\" chunk, we get back the \\ncorrect form, \"are our children learning\".\\n>>> from transforms import correct_verbs\\n>>> correct_verbs([(\\'is\\', \\'VBZ\\'), (\\'our\\', \\'PRP$\\'), (\\'children\\', \\n\\'NNS\\'), (\\'learning\\', \\'VBG\\')])\\n[(\\'are\\', \\'VBP\\'), (\\'our\\', \\'PRP$\\'), (\\'children\\', \\'NNS\\'), (\\'learning\\', \\n\\'VBG\\')]\\nWe can also try this with a singular noun and an incorrect plural verb.\\n>>> correct_verbs([(\\'our\\', \\'PRP$\\'), (\\'child\\', \\'NN\\'), (\\'were\\', \\'VBD\\'), \\n(\\'learning\\', \\'VBG\\')])\\n[(\\'our\\', \\'PRP$\\'), (\\'child\\', \\'NN\\'), (\\'was\\', \\'VBD\\'), (\\'learning\\', \\n\\'VBG\\')]\\nIn this case, \"were\" becomes \"was\" because \"child\" is a singular noun.\\nHow it works...\\nThe correct_verbs() function starts by looking for a verb in the chunk. If no verb is found, \\nthe chunk is returned with no changes. Once a verb is found, we keep the verb, its tag, and its \\nindex in the chunk. Then we look on either side of the verb to ind the nearest noun, starting \\non the right, and only looking to the left if no noun is found on the right. If no noun is found at \\nall, the chunk is returned as is. But if a noun is found, then we lookup the correct verb form \\ndepending on whether or not the noun is plural.\\nRecall from Chapter 4, Part-of-Speech Tagging, that plural nouns are tagged with NNS, while \\nsingular nouns are tagged with NN. This means we can check the plurality of a noun by seeing \\nif its tag ends with S. Once we get the corrected verb form, it is inserted into the chunk to \\nreplace the original verb form.\\nTo make searching through the chunk easier, we deine a function called first_chunk_\\nindex(). It takes a chunk, a lambda predicate, the starting index, and a step increment. \\nThe predicate function is called with each tagged word until it returns True. If it never returns \\nTrue, then None is returned. The starting index defaults to zero and the step increment \\nto one. As you\\'ll see in upcoming recipes, we can search backwards by overriding start \\nand setting step to -1. This small utility function will be a key part of subsequent transform \\nfunctions.\\nSee also\\nThe next four recipes all make use of first_chunk_index() to perform chunk \\ntransformations.\\nChapter 6\\n149\\nSwapping verb phrases\\nSwapping the words around a verb can eliminate the passive voice from particular phrases. \\nFor example, \"the book was great\" can be transformed into \"the great book\".\\nHow to do it...\\nIn transforms.py there is a function called swap_verb_phrase(). It swaps the \\nright-hand side of the chunk with the left-hand side, using the verb as the pivot point. \\nIt uses the first_chunk_index() function deined in the previous recipe to ind the \\nverb to pivot around.\\ndef swap_verb_phrase(chunk):\\n  # find location of verb\\n  vbpred = lambda (word, tag): tag != \\'VBG\\' and tag.startswith(\\'VB\\') \\nand len(tag) > 2\\n  vbidx = first_chunk_index(chunk, vbpred)\\n  \\n  if vbidx is None:\\n    return chunk\\n  \\n  return chunk[vbidx+1:] + chunk[:vbidx]\\nNow we can see how it works on the part-of-speech tagged phrase \"the book was great\".\\n>>> from transforms import swap_verb_phrase\\n>>> swap_verb_phrase([(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\'), (\\'was\\', \\'VBD\\'), \\n(\\'great\\', \\'JJ\\')])\\n[(\\'great\\', \\'JJ\\'), (\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\')]\\nThe result is \"great the book\". This phrase clearly isn\\'t grammatically correct, so read on to \\nlearn how to ix it.\\nHow it works...\\nUsing first_chunk_index() from the previous recipe, we start by inding the irst \\nmatching verb that is not a gerund (a word that ends in \"ing\") tagged with VBG. Once we\\'ve \\nfound the verb, we return the chunk with the right side before the left, and remove the verb.\\nThe reason we don\\'t want to pivot around a gerund is that gerunds are commonly used to \\ndescribe nouns, and pivoting around one would remove that description. Here\\'s an example \\nwhere you can see how not pivoting around a gerund is a good thing:\\n>>> swap_verb_phrase([(\\'this\\', \\'DT\\'), (\\'gripping\\', \\'VBG\\'), (\\'book\\', \\n\\'NN\\'), (\\'is\\', \\'VBZ\\'), (\\'fantastic\\', \\'JJ\\')])\\nTransforming Chunks and Trees\\n150\\n[(\\'fantastic\\', \\'JJ\\'), (\\'this\\', \\'DT\\'), (\\'gripping\\', \\'VBG\\'), (\\'book\\', \\n\\'NN\\')]\\nIf we had pivoted around the gerund, the result would be \"book is fantastic this\", and we\\'d lose \\nthe gerund \"gripping\".\\nThere\\'s more...\\nFiltering insigniicant words makes the inal result more readable. By iltering either before \\nor after swap_verb_phrase(), we get \"fantastic gripping book\" instead of \"fantastic this \\ngripping book\".\\n>>> from transforms import swap_verb_phrase, filter_insignificant\\n>>> swap_verb_phrase(filter_insignificant([(\\'this\\', \\'DT\\'), \\n(\\'gripping\\', \\'VBG\\'), (\\'book\\', \\'NN\\'), (\\'is\\', \\'VBZ\\'), (\\'fantastic\\', \\n\\'JJ\\')]))\\n[(\\'fantastic\\', \\'JJ\\'), (\\'gripping\\', \\'VBG\\'), (\\'book\\', \\'NN\\')]\\n>>> filter_insignificant(swap_verb_phrase([(\\'this\\', \\'DT\\'), \\n(\\'gripping\\', \\'VBG\\'), (\\'book\\', \\'NN\\'), (\\'is\\', \\'VBZ\\'), (\\'fantastic\\', \\n\\'JJ\\')]))\\n[(\\'fantastic\\', \\'JJ\\'), (\\'gripping\\', \\'VBG\\'), (\\'book\\', \\'NN\\')]\\nEither way, we get a shorter grammatical chunk with no loss of meaning.\\nSee also\\nThe previous recipe deines first_chunk_index(), which is used to ind the verb in \\nthe chunk.\\nSwapping noun cardinals\\nIn a chunk, a cardinal word—tagged as CD—refers to a number, such as \"10\". These cardinals \\noften occur before or after a noun. For normalization purposes, it can be useful to always put \\nthe cardinal before the noun.\\nHow to do it...\\nThe function swap_noun_cardinal() is deined in transforms.py. It swaps any cardinal \\nthat occurs immediately after a noun with the noun, so that the cardinal occurs immediately \\nbefore the noun.\\ndef swap_noun_cardinal(chunk):\\n  cdidx = first_chunk_index(chunk, lambda (word, tag): tag == \\'CD\\')\\n  # cdidx must be > 0 and there must be a noun immediately before it\\n  if not cdidx or not chunk[cdidx-1][1].startswith(\\'NN\\'):\\nChapter 6\\n151\\n    return chunk\\n  \\n  noun, nntag = chunk[cdidx-1]\\n  chunk[cdidx-1] = chunk[cdidx]\\n  chunk[cdidx] = noun, nntag\\n  return chunk\\nLet\\'s try it on a date, such as \"Dec 10\", and another common phrase \"the top 10\".\\n>>> from transforms import swap_noun_cardinal\\n>>> swap_noun_cardinal([(\\'Dec.\\', \\'NNP\\'), (\\'10\\', \\'CD\\')])\\n[(\\'10\\', \\'CD\\'), (\\'Dec.\\', \\'NNP\\')]\\n>>> swap_noun_cardinal([(\\'the\\', \\'DT\\'), (\\'top\\', \\'NN\\'), (\\'10\\', \\'CD\\')])\\n[(\\'the\\', \\'DT\\'), (\\'10\\', \\'CD\\'), (\\'top\\', \\'NN\\')]\\nThe result is that the numbers are now in front of the noun, creating \"10 Dec\" and \"the \\n10 top\".\\nHow it works...\\nWe start by looking for a CD tag in the chunk. If no CD is found, or if the CD is at the \\nbeginning of the chunk, then the chunk is returned as is. There must also be a noun \\nimmediately before the CD. If we do ind a CD with a noun preceding it, then we swap  \\nthe noun and cardinal in place.\\nSee also\\nThe Correcting verb forms recipe deines the first_chunk_index() function, used to ind \\ntagged words in a chunk.\\nSwapping ininitive phrases\\nAn ininitive phrase has the form \"A of B\", such as \"book of recipes\". These can often be \\ntransformed into a new form while retaining the same meaning, such as \"recipes book\".\\nHow to do it...\\nAn ininitive phrase can be found by looking for a word tagged with IN. The function \\nswap_infinitive_phrase(), deined in transforms.py, will return a chunk that \\nswaps the portion of the phrase after the IN word with the portion before the IN word.\\ndef swap_infinitive_phrase(chunk):\\n  inpred = lambda (word, tag): tag == \\'IN\\' and word != \\'like\\'\\n  inidx = first_chunk_index(chunk, inpred)\\nTransforming Chunks and Trees\\n152\\n  \\n  if inidx is None:\\n    return chunk\\n  \\n  nnpred = lambda (word, tag): tag.startswith(\\'NN\\')\\n  nnidx = first_chunk_index(chunk, nnpred, start=inidx, step=-1) or 0\\n  \\n  return chunk[:nnidx] + chunk[inidx+1:] + chunk[nnidx:inidx]\\nThe function can now be used to transform \"book of recipes\" into \"recipes book\".\\n>>> from transforms import swap_infinitive_phrase\\n>>> swap_infinitive_phrase([(\\'book\\', \\'NN\\'), (\\'of\\', \\'IN\\'), (\\'recipes\\', \\n\\'NNS\\')])\\n[(\\'recipes\\', \\'NNS\\'), (\\'book\\', \\'NN\\')]\\nHow it works...\\nThis function is similar to the swap_verb_phrase() function described in the Swapping \\nverb phrases recipe. The inpred lambda is passed to first_chunk_index() to look for \\na word whose tag is IN. Next, nnpred is used to ind the irst noun that occurs before the IN \\nword, so we can insert the portion of the chunk after the IN word between the noun and the \\nbeginning of the chunk. A more complicated example should demonstrate this:\\n>>> swap_infinitive_phrase([(\\'delicious\\', \\'JJ\\'), (\\'book\\', \\'NN\\'), \\n(\\'of\\', \\'IN\\'), (\\'recipes\\', \\'NNS\\')])\\n[(\\'delicious\\', \\'JJ\\'), (\\'recipes\\', \\'NNS\\'), (\\'book\\', \\'NN\\')]\\nWe don\\'t want the result to be \"recipes delicious book\". Instead, we want to insert \"recipes\" \\nbefore the noun \"book\", but after the adjective \"delicious\". Hence, the need to ind the nnidx \\noccurring before the inidx.\\nThere\\'s more...\\nYou\\'ll notice that the inpred lambda checks to make sure the word is not \"like\". That\\'s \\nbecause \"like\" phrases must be treated differently, as transforming them the same way \\nwill result in an ungrammatical phrase. For example, \"tastes like chicken\" should not be \\ntransformed into \"chicken tastes\":\\n>>> swap_infinitive_phrase([(\\'tastes\\', \\'VBZ\\'), (\\'like\\', \\'IN\\'), \\n(\\'chicken\\', \\'NN\\')])\\n[(\\'tastes\\', \\'VBZ\\'), (\\'like\\', \\'IN\\'), (\\'chicken\\', \\'NN\\')]\\nChapter 6\\n153\\nSee also\\nIn the next recipe, we\\'ll learn how to transform \"recipes book\" into the more normal form \\n\"recipe book\".\\nSingularizing plural nouns\\nAs we saw in the previous recipe, the transformation process can result in phrases such as \\n\"recipes book\". This is a NNS followed by an NN, when a more proper version of the phrase \\nwould be \"recipe book\", which is an NN followed by another NN. We can do another transform \\nto correct these improper plural nouns.\\nHow to do it...\\ntransforms.py deines a function called singularize_plural_noun(), which will \\nde-pluralize a plural noun (tagged with NNS) that is followed by another noun.\\ndef singularize_plural_noun(chunk):\\n  nnspred = lambda (word, tag): tag == \\'NNS\\'\\n  nnsidx = first_chunk_index(chunk, nnspred)\\n  \\n  if nnsidx is not None and nnsidx+1 < len(chunk) and chunk[nnsidx+1]\\n[1][:2] == \\'NN\\':\\n    noun, nnstag = chunk[nnsidx]\\n    chunk[nnsidx] = (noun.rstrip(\\'s\\'), nnstag.rstrip(\\'S\\'))\\n  \\n  return chunk\\nUsing it on \"recipes book\", we get the more correct form, \"recipe book\".\\n>>> from transforms import singularize_plural_noun\\n>>> singularize_plural_noun([(\\'recipes\\', \\'NNS\\'), (\\'book\\', \\'NN\\')])\\n[(\\'recipe\\', \\'NN\\'), (\\'book\\', \\'NN\\')]\\nHow it works...\\nWe start by looking for a plural noun with the tag NNS. If found, and if the next word is a noun \\n(determined by making sure the tag starts with NN), then we de-pluralize the plural noun by \\nremoving an \"s\" from the right side of both the tag and the word.\\nThe tag is assumed to be capitalized, so an uppercase \"S\" is removed from the right side of \\nthe tag, while a lowercase \"s\" is removed from the right side of the word.\\nTransforming Chunks and Trees\\n154\\nSee also\\nThe previous recipe shows how a transformation can result in a plural noun followed by a \\nsingular noun, though this could also occur naturally in real-world text.\\nChaining chunk transformations\\nThe transform functions deined in the previous recipes can be chained together to normalize \\nchunks. The resulting chunks are often shorter with no loss of meaning.\\nHow to do it...\\nIn transforms.py is the function transform_chunk(). It takes a single chunk and an \\noptional list of transform functions. It calls each transform function on the chunk, one at a \\ntime, and returns the inal chunk.\\ndef transform_chunk(chunk, chain=[filter_insignificant, swap_verb_\\nphrase, swap_infinitive_phrase, singularize_plural_noun], trace=0):\\n  for f in chain:\\n    chunk = f(chunk)\\n    \\n    if trace:\\n      print f.__name__, \\':\\', chunk\\n  \\n  return chunk\\nUsing it on the phrase \"the book of recipes is delicious\", we get \"delicious recipe book\":\\n>>> from transforms import transform_chunk\\n>>> transform_chunk([(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\'), (\\'of\\', \\'IN\\'), \\n(\\'recipes\\', \\'NNS\\'), (\\'is\\', \\'VBZ\\'), (\\'delicious\\', \\'JJ\\')])\\n[(\\'delicious\\', \\'JJ\\'), (\\'recipe\\', \\'NN\\'), (\\'book\\', \\'NN\\')]\\nHow it works...\\nThe transform_chunk() function defaults to chaining the following functions in order:\\n \\nf\\nfilter_insignificant()\\n \\nf\\nswap_verb_phrase()\\n \\nf\\nswap_infinitive_phrase()\\n \\nf\\nsingularize_plural_noun()\\nEach function transforms the chunk that results from the previous function, starting with the \\noriginal chunk.\\nChapter 6\\n155\\nThe order in which you apply transform functions can be \\nsigniicant. Experiment with your own data to determine which \\ntransforms are best, and in which order they should be applied.\\nThere\\'s more...\\nYou can pass trace=1 into transform_chunk() to get an output at each step.\\n>>> from transforms import transform_chunk\\n>>> transform_chunk([(\\'the\\', \\'DT\\'), (\\'book\\', \\'NN\\'), (\\'of\\', \\'IN\\'), \\n(\\'recipes\\', \\'NNS\\'), (\\'is\\', \\'VBZ\\'), (\\'delicious\\', \\'JJ\\')], trace=1)\\nfilter_insignificant : [(\\'book\\', \\'NN\\'), (\\'of\\', \\'IN\\'), (\\'recipes\\', \\n\\'NNS\\'), (\\'is\\', \\'VBZ\\'), (\\'delicious\\', \\'JJ\\')]\\nswap_verb_phrase : [(\\'delicious\\', \\'JJ\\'), (\\'book\\', \\'NN\\'), (\\'of\\', \\'IN\\'), \\n(\\'recipes\\', \\'NNS\\')]\\nswap_infinitive_phrase : [(\\'delicious\\', \\'JJ\\'), (\\'recipes\\', \\'NNS\\'), \\n(\\'book\\', \\'NN\\')]\\nsingularize_plural_noun : [(\\'delicious\\', \\'JJ\\'), (\\'recipe\\', \\'NN\\'), \\n(\\'book\\', \\'NN\\')]\\n[(\\'delicious\\', \\'JJ\\'), (\\'recipe\\', \\'NN\\'), (\\'book\\', \\'NN\\')]\\nThis shows you the result of each transform function, which is then passed in to the next \\ntransform function until a inal chunk is returned.\\nSee also\\nThe transform functions used were deined in the previous recipes of this chapter.\\nConverting a chunk tree to text\\nAt some point, you may want to convert a Tree or sub-tree back to a sentence or chunk string. \\nThis is mostly straightforward, except when it comes to properly outputting punctuation.\\nHow to do it...\\nWe\\'ll use the irst Tree of the treebank_chunk as our example. The obvious irst step is to \\njoin all the words in the tree with a space.\\n>>> from nltk.corpus import treebank_chunk\\n>>> tree = treebank_chunk.chunked_sents()[0]\\n>>> \\' \\'.join([w for w, t in tree.leaves()])\\n\\'Pierre Vinken , 61 years old , will join the board as a nonexecutive \\ndirector Nov. 29 .\\'\\nTransforming Chunks and Trees\\n156\\nAs you can see, the punctuation isn\\'t quite right. The commas and period are treated as \\nindividual words, and so get the surrounding spaces as well. We can ix this using regular \\nexpression substitution. This is implemented in the chunk_tree_to_sent() function \\nfound in transforms.py.\\nimport re\\npunct_re = re.compile(r\\'\\\\s([,\\\\.;\\\\?])\\')\\ndef chunk_tree_to_sent(tree, concat=\\' \\'):\\n  s = concat.join([w for w, t in tree.leaves()])\\n  return re.sub(punct_re, r\\'\\\\g<1>\\', s)\\nUsing this function results in a much cleaner sentence, with no space before each  \\npunctuation mark:\\n>>> from transforms import chunk_tree_to_sent\\n>>> chunk_tree_to_sent(tree)\\n\\'Pierre Vinken, 61 years old, will join the board as a nonexecutive \\ndirector Nov. 29.\\'\\nHow it works...\\nTo correct the extra spaces in front of the punctuation, we create a regular expression \\npunct_re that will match a space followed by any of the known punctuation characters. We \\nhave to escape both \\'.\\' and \\'?\\' with a \\'\\\\\\' since they are special characters. The punctuation is \\nsurrounded by parenthesis so we can use the matched group for substitution.\\nOnce we have our regular expression, we deine chunk_tree_to_sent(), whose irst \\nstep is to join the words by a concatenation character that defaults to a space. Then we can \\ncall re.sub() to replace all the punctuation matches with just the punctuation group. This \\neliminates the space in front of the punctuation characters, resulting in a more correct string.\\nThere\\'s more...\\nWe can simplify this function a little by using nltk.tag.untag() to get words from the \\ntree\\'s leaves, instead of using our own list comprehension.\\nimport nltk.tag, re\\npunct_re = re.compile(r\\'\\\\s([,\\\\.;\\\\?])\\')\\ndef chunk_tree_to_sent(tree, concat=\\' \\'):\\n  s = concat.join(nltk.tag.untag(tree.leaves()))\\n  return re.sub(punct_re, r\\'\\\\g<1>\\', s)\\nChapter 6\\n157\\nSee also\\nThe nltk.tag.untag() function was covered at the end of the Default tagging recipe in \\nChapter 4, Part-of-Speech Tagging.\\nFlattening a deep tree\\nSome of the included corpora contain parsed sentences, which are often deep trees of nested \\nphrases. Unfortunately, these trees are too deep to use for training a chunker, since IOB tag \\nparsing is not designed for nested chunks. To make these trees usable for chunker training, \\nwe must latten them.\\nGetting ready\\nWe\\'re going to use the irst parsed sentence of the treebank corpus as our example. Here\\'s \\na diagram showing how deeply nested this tree is:\\nYou may notice that the part-of-speech tags are part of the tree structure, instead of  \\nbeing included with the word. This will be handled next using the Tree.pos() method, \\nwhich was designed speciically for combining words with pre-terminal Tree nodes such \\nas part-of-speech tags.\\nHow to do it...\\nIn transforms.py there is a function named flatten_deeptree(). It takes a single \\nTree and will return a new Tree that keeps only the lowest level trees. It uses a helper \\nfunction flatten_childtrees() to do most of the work.\\nfrom nltk.tree import Tree\\ndef flatten_childtrees(trees):\\n  children = []\\nTransforming Chunks and Trees\\n158\\n  \\n  for t in trees:\\n    if t.height() < 3:\\n      children.extend(t.pos())\\n    elif t.height() == 3:\\n      children.append(Tree(t.node, t.pos()))\\n    else:\\n      children.extend(flatten_childtrees([c for c in t]))\\n  \\n  return children\\ndef flatten_deeptree(tree):\\n  return Tree(tree.node, flatten_childtrees([c for c in tree]))\\nWe can use it on the irst parsed sentence of the treebank corpus to get a latter tree:\\n>>> from nltk.corpus import treebank\\n>>> from transforms import flatten_deeptree\\n>>> flatten_deeptree(treebank.parsed_sents()[0])\\nTree(\\'S\\', [Tree(\\'NP\\', [(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\')]), (\\',\\', \\n\\',\\'), Tree(\\'NP\\', [(\\'61\\', \\'CD\\'), (\\'years\\', \\'NNS\\')]), (\\'old\\', \\'JJ\\'), \\n(\\',\\', \\',\\'), (\\'will\\', \\'MD\\'), (\\'join\\', \\'VB\\'), Tree(\\'NP\\', [(\\'the\\', \\n\\'DT\\'), (\\'board\\', \\'NN\\')]), (\\'as\\', \\'IN\\'), Tree(\\'NP\\', [(\\'a\\', \\'DT\\'), \\n(\\'nonexecutive\\', \\'JJ\\'), (\\'director\\', \\'NN\\')]), Tree(\\'NP-TMP\\', [(\\'Nov.\\', \\n\\'NNP\\'), (\\'29\\', \\'CD\\')]), (\\'.\\', \\'.\\')])\\nThe result is a much latter Tree that only includes NP phrases. Words that are not part of a \\nNP phrase are separated. This latter tree is shown as follows:\\nThis Tree is quite similar to the irst chunk Tree from the treebank_chunk corpus. The \\nmain difference is that the rightmost NP Tree is separated into two sub-trees in the previous \\ndiagram, one of them named NP-TMP.\\nThe irst tree from treebank_chunk is shown as follows for comparison:\\nChapter 6\\n159\\nHow it works...\\nThe solution is composed of two functions: flatten_deeptree() returns a new Tree from \\nthe given tree by calling flatten_childtrees() on each of the given tree\\'s children.\\nflatten_childtrees() is a recursive function that drills down into the Tree until it inds \\nchild trees whose height() is equal to or less than three. A Tree whose height() is less \\nthan three looks like this:\\n>>> from nltk.tree import Tree\\n>>> Tree(\\'NNP\\', [\\'Pierre\\']).height()\\n2\\nThese short trees are converted into lists of tuples using the pos() function.\\n>>> Tree(\\'NNP\\', [\\'Pierre\\']).pos()\\n[(\\'Pierre\\', \\'NNP\\')]\\nTrees whose height() is equal to three are the lowest level trees that we\\'re interested in \\nkeeping. These trees look like this:\\n>>> Tree(\\'NP\\', [Tree(\\'NNP\\', [\\'Pierre\\']), Tree(\\'NNP\\', [\\'Vinken\\'])]).\\nheight()\\n3\\nWhen we call pos() on that tree, we get:\\n>>> Tree(\\'NP\\', [Tree(\\'NNP\\', [\\'Pierre\\']), Tree(\\'NNP\\', [\\'Vinken\\'])]).\\npos()\\n[(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\')]\\nThe recursive nature of flatten_childtrees() eliminates all trees whose height is \\ngreater than three.\\nTransforming Chunks and Trees\\n160\\nThere\\'s more...\\nFlattening a deep Tree allows us to call nltk.chunk.util.tree2conlltags() on the \\nlattened Tree, a necessary step to train a chunker. If you try to call this function before \\nlattening the Tree, you get a ValueError exception.\\n>>> from nltk.chunk.util import tree2conlltags\\n>>> tree2conlltags(treebank.parsed_sents()[0])\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\n  File \"/usr/local/lib/python2.6/dist-packages/nltk/chunk/util.py\", \\nline 417, in tree2conlltags\\n    raise ValueError, \"Tree is too deeply nested to be printed in \\nCoNLL format\"\\nValueError: Tree is too deeply nested to be printed in CoNLL format\\nHowever, after lattening there\\'s no problem:\\n>>> tree2conlltags(flatten_deeptree(treebank.parsed_sents()[0]))\\n[(\\'Pierre\\', \\'NNP\\', \\'B-NP\\'), (\\'Vinken\\', \\'NNP\\', \\'I-NP\\'), (\\',\\', \\',\\', \\n\\'O\\'), (\\'61\\', \\'CD\\', \\'B-NP\\'), (\\'years\\', \\'NNS\\', \\'I-NP\\'), (\\'old\\', \\'JJ\\', \\n\\'O\\'), (\\',\\', \\',\\', \\'O\\'), (\\'will\\', \\'MD\\', \\'O\\'), (\\'join\\', \\'VB\\', \\'O\\'), \\n(\\'the\\', \\'DT\\', \\'B-NP\\'), (\\'board\\', \\'NN\\', \\'I-NP\\'), (\\'as\\', \\'IN\\', \\'O\\'), \\n(\\'a\\', \\'DT\\', \\'B-NP\\'), (\\'nonexecutive\\', \\'JJ\\', \\'I-NP\\'), (\\'director\\', \\n\\'NN\\', \\'I-NP\\'), (\\'Nov.\\', \\'NNP\\', \\'B-NP-TMP\\'), (\\'29\\', \\'CD\\', \\'I-NP-TMP\\'), \\n(\\'.\\', \\'.\\', \\'O\\')]\\nBeing able to latten trees, opens up the possibility of training a chunker on corpora consisting \\nof deep parse trees.\\nCESS-ESP and CESS-CAT treebank\\nThe cess_esp and cess_cat corpora have parsed sentences, but no chunked sentences. \\nIn other words, they have deep trees that must be lattened in order to train a chunker. In fact, \\nthe trees are so deep that a diagram can\\'t be shown, but the lattening can be demonstrated \\nby showing the height() of the tree before and after lattening.\\n>>> from nltk.corpus import cess_esp\\n>>> cess_esp.parsed_sents()[0].height()\\n22\\n>>> flatten_deeptree(cess_esp.parsed_sents()[0]).height()\\n3\\nChapter 6\\n161\\nSee also\\nThe Training a tagger-based chunker recipe in Chapter 5, Extracting Chunks covers training a \\nchunker using IOB tags.\\nCreating a shallow tree\\nIn the previous recipe, we lattened a deep Tree by only keeping the lowest level sub-trees. In \\nthis recipe, we\\'ll keep only the highest level sub-trees instead.\\nHow to do it...\\nWe\\'ll be using the irst parsed sentence from the treebank corpus as our example. Recall \\nfrom the previous recipe that the sentence Tree looks like this:\\nThe shallow_tree() function deined in transforms.py eliminates all the nested \\nsub-trees, keeping only the top tree nodes.\\nfrom nltk.tree import Tree\\ndef shallow_tree(tree):\\n  children = []\\n  \\n  for t in tree:\\n    if t.height() < 3:\\n      children.extend(t.pos())\\n    else:\\n      children.append(Tree(t.node, t.pos()))\\n  \\n  return Tree(tree.node, children)\\nTransforming Chunks and Trees\\n162\\nUsing it on the irst parsed sentence in treebank results in a Tree with only two sub-trees.\\n>>> from transforms import shallow_tree\\n>>> shallow_tree(treebank.parsed_sents()[0])\\nTree(\\'S\\', [Tree(\\'NP-SBJ\\', [(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\'), (\\',\\', \\n\\',\\'), (\\'61\\', \\'CD\\'), (\\'years\\', \\'NNS\\'), (\\'old\\', \\'JJ\\'), (\\',\\', \\',\\')]), \\nTree(\\'VP\\', [(\\'will\\', \\'MD\\'), (\\'join\\', \\'VB\\'), (\\'the\\', \\'DT\\'), (\\'board\\', \\n\\'NN\\'), (\\'as\\', \\'IN\\'), (\\'a\\', \\'DT\\'), (\\'nonexecutive\\', \\'JJ\\'), (\\'director\\', \\n\\'NN\\'), (\\'Nov.\\', \\'NNP\\'), (\\'29\\', \\'CD\\')]), (\\'.\\', \\'.\\')])\\nWe can visually and programmatically see the difference, as shown in the following diagram \\nand code:\\n>>> treebank.parsed_sents()[0].height()\\n7\\n>>> shallow_tree(treebank.parsed_sents()[0]).height()\\n3\\nAs in the previous recipe, the height of the new tree is three so it can be used for training a \\nchunker.\\nHow it works...\\nThe shallow_tree() function iterates over each of the top-level sub-trees in order to \\ncreate new child trees. If the height() of a sub-tree is less than three, then that sub-tree \\nis replaced by a list of its part-of-speech tagged children. All other sub-trees are replaced by \\na new Tree whose children are the part-of-speech tagged leaves. This eliminates all nested \\nsub-trees while retaining the top-level sub-trees.\\nThis function is an alternative to flatten_deeptree() from the previous recipe, for when \\nyou want to keep the higher level tree nodes and ignore the lower level nodes.\\nSee also\\nThe previous recipe covers how to latten a Tree and keep the lowest level sub-trees, as \\nopposed to keeping the highest level sub-trees.\\nChapter 6\\n163\\nConverting tree nodes\\nAs you\\'ve seen in previous recipes, parse trees often have a variety of Tree node types that \\nare not present in chunk trees. If you want to use the parse trees to train a chunker, then \\nyou\\'ll probably want to reduce this variety by converting some of these tree nodes to more \\ncommon node types.\\nGetting ready\\nFirst, we have to decide what Tree nodes need to be converted. Let\\'s take a look at that irst \\nTree again:\\nImmediately you can see that there are two alternative NP sub-trees: NP-SBJ and NP-TMP. \\nLet\\'s convert both of those to NP. The mapping will be as follows:\\nOriginal Node\\nNew Node\\nNP-SBJ\\nNP\\nNP-TMP\\nNP\\nHow to do it...\\nIn transforms.py there is a function convert_tree_nodes(). It takes two arguments: \\nthe Tree to convert, and a node conversion mapping. It returns a new Tree with all \\nmatching nodes replaced based on the values in the mapping.\\nfrom nltk.tree import Tree\\ndef convert_tree_nodes(tree, mapping):\\n  children = []\\n  \\n  for t in tree:\\n    if isinstance(t, Tree):\\nTransforming Chunks and Trees\\n164\\n      children.append(convert_tree_nodes(t, mapping))\\n    else:\\n      children.append(t)\\n  \\n  node = mapping.get(tree.node, tree.node)\\n  return Tree(node, children)\\nUsing the mapping table shown earlier, we can pass it in as a dict to convert_tree_\\nnodes() and convert the irst parsed sentence from treebank.\\n>>> from transforms import convert_tree_nodes\\n>>> mapping = {\\'NP-SBJ\\': \\'NP\\', \\'NP-TMP\\': \\'NP\\'}\\n>>> convert_tree_nodes(treebank.parsed_sents()[0], mapping)\\nTree(\\'S\\', [Tree(\\'NP\\', [Tree(\\'NP\\', [Tree(\\'NNP\\', [\\'Pierre\\']), \\nTree(\\'NNP\\', [\\'Vinken\\'])]), Tree(\\',\\', [\\',\\']), Tree(\\'ADJP\\', [Tree(\\'NP\\', \\n[Tree(\\'CD\\', [\\'61\\']), Tree(\\'NNS\\', [\\'years\\'])]), Tree(\\'JJ\\', [\\'old\\'])]), \\nTree(\\',\\', [\\',\\'])]), Tree(\\'VP\\', [Tree(\\'MD\\', [\\'will\\']), Tree(\\'VP\\', \\n[Tree(\\'VB\\', [\\'join\\']), Tree(\\'NP\\', [Tree(\\'DT\\', [\\'the\\']), Tree(\\'NN\\', \\n[\\'board\\'])]), Tree(\\'PP-CLR\\', [Tree(\\'IN\\', [\\'as\\']), Tree(\\'NP\\', \\n[Tree(\\'DT\\', [\\'a\\']), Tree(\\'JJ\\', [\\'nonexecutive\\']), Tree(\\'NN\\', \\n[\\'director\\'])])]), Tree(\\'NP\\', [Tree(\\'NNP\\', [\\'Nov.\\']), Tree(\\'CD\\', \\n[\\'29\\'])])])]), Tree(\\'.\\', [\\'.\\'])])\\nIn the following diagram, you can see that the NP-* sub-trees have been replaced with \\nNP sub-trees:\\nHow it works...\\nconvert_tree_nodes() recursively converts every child sub-tree using the mapping. \\nThe Tree is then rebuilt with the converted nodes and children until the entire Tree has \\nbeen converted.\\nThe result is a brand new Tree instance with new sub-trees whose nodes have been \\nconverted.\\nChapter 6\\n165\\nSee also\\nThe previous two recipes cover different methods of lattening a parse Tree, both of which \\ncan produce sub-trees that may require mapping before using them to train a chunker. \\nChunker training is covered in the Training a tagger-based chunker recipe in Chapter 5, \\nExtracting Chunks.\\n7\\nText Classiication\\nIn this chapter, we will cover:\\n \\nf\\nBag of Words feature extraction\\n \\nf\\nTraining a naive Bayes classiier\\n \\nf\\nTraining a decision tree classiier\\n \\nf\\nTraining a maximum entropy classiier\\n \\nf\\nMeasuring precision and recall of a classiier\\n \\nf\\nCalculating high information words\\n \\nf\\nCombining classiiers with voting\\n \\nf\\nClassifying with multiple binary classiiers\\nIntroduction\\nText classiication is a way to categorize documents or pieces of text. By examining the \\nword usage in a piece of text, classiiers can decide what class label to assign to it. A binary \\nclassiier decides between two labels, such as positive or negative. The text can either be \\none label or the other, but not both, whereas a multi-label classiier can assign one or more \\nlabels to a piece of text.\\nClassiication works by learning from labeled feature sets, or training data, to later classify \\nan unlabeled feature set. A feature set is basically a key-value mapping of feature names to \\nfeature values. In the case of text classiication, the feature names are usually words, and the \\nvalues are all True. As the documents may have unknown words, and the number of possible \\nwords may be very large, words that don\\'t occur in the text are omitted, instead of including \\nthem in a feature set with the value False.\\nText Classiication\\n168\\nAn instance is a single feature set. It represents a single occurrence of a combination of \\nfeatures. We will use instance and feature set interchangeably. A labeled feature set is an \\ninstance with a known class label that we can use for training or evaluation.\\nBag of Words feature extraction\\nText feature extraction is the process of transforming what is essentially a list of words into a \\nfeature set that is usable by a classiier. The NLTK classiiers expect dict style feature sets, \\nso we must therefore transform our text into a dict. The Bag of Words model is the simplest \\nmethod; it constructs a word presence feature set from all the words of an instance.\\nHow to do it...\\nThe idea is to convert a list of words into a dict, where each word becomes a key with the \\nvalue True. The bag_of_words() function in featx.py looks like this:\\ndef bag_of_words(words):\\n  return dict([(word, True) for word in words])\\nWe can use it with a list of words, in this case the tokenized sentence \"the quick brown fox\":\\n>>> from featx import bag_of_words\\n>>> bag_of_words([\\'the\\', \\'quick\\', \\'brown\\', \\'fox\\'])\\n{\\'quick\\': True, \\'brown\\': True, \\'the\\': True, \\'fox\\': True}\\nThe resulting dict is known as a bag of words because the words are not in order, and it \\ndoesn\\'t matter where in the list of words they occurred, or how many times they occurred. All \\nthat matters is that the word is found at least once.\\nHow it works...\\nThe bag_of_words() function is a very simple list comprehension that constructs a dict \\nfrom the given words, where every word gets the value True.\\nSince we have to assign a value to each word in order to create a dict, True is a logical \\nchoice for the value to indicate word presence. If we knew the universe of all possible words, \\nwe could assign the value False to all the words that are not in the given list of words. But \\nmost of the time, we don\\'t know all possible words beforehand. Plus, the dict that would \\nresult from assigning False to every possible word would be very large (assuming all words in \\nthe English language are possible). So instead, to keep feature extraction simple and use less \\nmemory, we stick with assigning the value True to all words that occur at least once. We don\\'t \\nassign the value False to any words since we don\\'t know what the set of possible words are; \\nwe only know about the words we are given.\\nChapter 7\\n169\\nThere\\'s more...\\nIn the default Bag of Words model, all words are treated equally. But that\\'s not always a good \\nidea. As we already know, some words are so common that they are practically meaningless. If \\nyou have a set of words that you want to exclude, you can use the bag_of_words_not_in_\\nset() function in featx.py.\\ndef bag_of_words_not_in_set(words, badwords):\\n  return bag_of_words(set(words) - set(badwords))\\nThis function can be used, among other things, to ilter stopwords. Here\\'s an example where \\nwe ilter the word \"the\" from \"the quick brown fox\":\\n>>> from featx import bag_of_words_not_in_set\\n>>> bag_of_words_not_in_set([\\'the\\', \\'quick\\', \\'brown\\', \\'fox\\'],  \\n[\\'the\\'])\\n{\\'quick\\': True, \\'brown\\': True, \\'fox\\': True}\\nAs expected, the resulting dict has \"quick\", \"brown\", and \"fox\", but not \"the\".\\nFiltering stopwords\\nHere\\'s an example of using the bag_of_words_not_in_set() function to ilter all \\nEnglish stopwords:\\nfrom nltk.corpus import stopwords\\ndef bag_of_non_stopwords(words, stopfile=\\'english\\'):\\n  badwords = stopwords.words(stopfile)\\n  return bag_of_words_not_in_set(words, badwords)\\nYou can pass a different language ilename as the stopfile keyword argument if you are \\nusing a language other than English. Using this function produces the same result as the \\nprevious example:\\n>>> from featx import bag_of_non_stopwords\\n>>> bag_of_non_stopwords([\\'the\\', \\'quick\\', \\'brown\\', \\'fox\\'])\\n{\\'quick\\': True, \\'brown\\': True, \\'fox\\': True}\\nHere, \"the\" is a stopword, so it is not present in the returned dict.\\nText Classiication\\n170\\nIncluding signiicant bigrams\\nIn addition to single words, it often helps to include signiicant bigrams. As signiicant bigrams \\nare less common than most individual words, including them in the Bag of Words can help the \\nclassiier make better decisions. We can use the BigramCollocationFinder covered in \\nthe Discovering word collocations recipe of Chapter 1, Tokenizing Text and WordNet Basics, to \\nind signiicant bigrams. bag_of_bigrams_words() found in featx.py will return a dict \\nof all words along with the 200 most signiicant bigrams.\\nfrom nltk.collocations import BigramCollocationFinder\\nfrom nltk.metrics import BigramAssocMeasures\\ndef bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, \\nn=200):\\n  bigram_finder = BigramCollocationFinder.from_words(words)\\n  bigrams = bigram_finder.nbest(score_fn, n)\\n  return bag_of_words(words + bigrams)\\nThe bigrams will be present in the returned dict as (word1, word2) and will have the \\nvalue as True. Using the same example words as before, we get all words plus every bigram:\\n>>> from featx import bag_of_bigrams_words\\n>>> bag_of_bigrams_words([\\'the\\', \\'quick\\', \\'brown\\', \\'fox\\'])\\n{\\'brown\\': True, (\\'brown\\', \\'fox\\'): True, (\\'the\\', \\'quick\\'):  \\nTrue, \\'fox\\': True, (\\'quick\\', \\'brown\\'): True, \\'quick\\': True,  \\n\\'the\\': True}\\nYou can change the maximum number of bigrams found by altering the keyword argument n.\\nSee also\\nThe Discovering word collocations recipe of Chapter 1, Tokenizing Text and WordNet Basics \\ncovers the BigramCollocationFinder in more detail. In the next recipe, we will train a \\nNaiveBayesClassifier using feature sets created with the Bag of Words model.\\nTraining a naive Bayes classiier\\nNow that we can extract features from text, we can train a classiier. The easiest classiier \\nto get started with is the NaiveBayesClassifier. It uses Bayes Theorem to predict the \\nprobability that a given feature set belongs to a particular label. The formula is:\\nP(label | features) = P(label) * P(features | label) / P(features)\\nChapter 7\\n171\\n \\nf\\nP(label) is the prior probability of the label occurring, which is the same as \\nthe likelihood that a random feature set will have the label. This is based on the \\nnumber of training instances with the label compared to the total number of training \\ninstances. For example, if 60/100 training instances have the label, the prior \\nprobability of the label is 60 percent.\\n \\nf\\nP(features | label) is the prior probability of a given feature set being \\nclassiied as that label. This is based on which features have occurred with each  \\nlabel in the training data.\\n \\nf\\nP(features) is the prior probability of a given feature set occurring. This is the \\nlikelihood of a random feature set being the same as the given feature set, and is \\nbased on the observed feature sets in the training data. For example, if the given \\nfeature set occurs twice in 100 training instances, the prior probability is 2 percent.\\n \\nf\\nP(label | features) tells us the probability that the given features should have \\nthat label. If this value is high, then we can be reasonably conident that the label is \\ncorrect for the given features.\\nGetting ready\\nWe are going to be using the movie_reviews corpus for our initial classiication examples. \\nThis corpus contains two categories of text: pos and neg. These categories are exclusive, \\nwhich makes a classiier trained on them a binary classiier. Binary classiiers have only two \\nclassiication labels, and will always choose one or the other.\\nEach ile in the movie_reviews corpus is composed of either positive or negative movie \\nreviews. We will be using each ile as a single instance for both training and testing the \\nclassiier. Because of the nature of the text and its categories, the classiication we will be \\ndoing is a form of sentiment analysis. If the classiier returns pos, then the text expresses \\npositive sentiment, whereas if we get neg, then the text expresses negative sentiment.\\nHow to do it...\\nFor training, we need to irst create a list of labeled feature sets. This list should be of the form \\n[(featureset, label)] where the featureset is a dict, and label is the known class \\nlabel for the featureset. The label_feats_from_corpus() function in featx.py takes a \\ncorpus, such as movie_reviews, and a feature_detector function, which defaults to bag_\\nof_words. It then constructs and returns a mapping of the form {label: [featureset]}. \\nWe can use this mapping to create a list of labeled training instances and testing instances. The \\nreason to do it this way is because we can get a fair sample from each label.\\nimport collections\\ndef label_feats_from_corpus(corp, feature_detector=bag_of_words):\\n  label_feats = collections.defaultdict(list)\\n  for label in corp.categories():\\nText Classiication\\n172\\n    for fileid in corp.fileids(categories=[label]):\\n      feats = feature_detector(corp.words(fileids=[fileid]))\\n      label_feats[label].append(feats)\\n  return label_feats\\nOnce we can get a mapping of label : feature sets, we want to construct a list of labeled \\ntraining instances and testing instances. The function split_label_feats() in featx.\\npy takes a mapping returned from label_feats_from_corpus() and splits each list of \\nfeature sets into labeled training and testing instances.\\ndef split_label_feats(lfeats, split=0.75):\\n  train_feats = []\\n  test_feats = []\\n  for label, feats in lfeats.iteritems():\\n    cutoff = int(len(feats) * split)\\n    train_feats.extend([(feat, label) for feat in  \\nfeats[:cutoff]])\\n    test_feats.extend([(feat, label) for feat in  \\nfeats[cutoff:]])\\n  return train_feats, test_feats\\nUsing these functions with the movie_reviews corpus gives us the lists of labeled feature \\nsets we need to train and test a classiier.\\n>>> from nltk.corpus import movie_reviews\\n>>> from featx import label_feats_from_corpus, split_label_feats\\n>>> movie_reviews.categories()\\n[\\'neg\\', \\'pos\\']\\n>>> lfeats = label_feats_from_corpus(movie_reviews)\\n>>> lfeats.keys()\\n[\\'neg\\', \\'pos\\']\\n>>> train_feats, test_feats = split_label_feats(lfeats)\\n>>> len(train_feats)\\n1500\\n>>> len(test_feats)\\n500\\nSo there are 1,000 pos iles, 1,000 neg iles, and we end up with 1,500 labeled training \\ninstances and 500 labeled testing instances, each composed of equal parts pos and neg. \\nNow we can train a NaiveBayesClassifier using its train() class method,\\n>>> from nltk.classify import NaiveBayesClassifier\\n>>> nb_classifier = NaiveBayesClassifier.train(train_feats)\\n>>> nb_classifier.labels()\\n[\\'neg\\', \\'pos\\']\\nChapter 7\\n173\\nLet\\'s test the classiier on a couple of made up reviews. The classify() method takes a \\nsingle argument, which should be a feature set. We can use the same bag_of_words() \\nfeature detector on a made up list of words to get our feature set.\\n>>> from featx import bag_of_words\\n>>> negfeat = bag_of_words([\\'the\\', \\'plot\\', \\'was\\', \\'ludicrous\\'])\\n>>> nb_classifier.classify(negfeat)\\n\\'neg\\'\\n>>> posfeat = bag_of_words([\\'kate\\', \\'winslet\\', \\'is\\', \\'accessible\\'])\\n>>> nb_classifier.classify(posfeat)\\n\\'pos\\'\\nHow it works...\\nThe label_feats_from_corpus() assumes that the corpus is categorized, and that a \\nsingle ile represents a single instance for feature extraction. It iterates over each category \\nlabel, and extracts features from each ile in that category using the feature_detector() \\nfunction, which defaults to bag_of_words(). It returns a dict whose keys are the category \\nlabels, and the values are lists of instances for that category.\\nIf we had the label_feats_from_corpus() function, return a list of \\nlabeled feature sets, instead of a dict, it would be much harder to get the \\nbalanced training data. The list would be ordered by label, and if you took a \\nslice of it, you would almost certainly be getting far more of one label than \\nanother. By returning a dict, you can take slices from the feature sets of \\neach label.\\nNow we need to split the labeled feature sets into training and testing instances using \\nsplit_label_feats(). This function allows us to take a fair sample of labeled feature \\nsets from each label, using the split keyword argument to determine the size of the sample. \\nsplit defaults to 0.75, which means the irst three-fourths of the labeled feature sets for \\neach label will be used for training, and the remaining one-fourth will be used for testing.\\nOnce we have split up our training and testing feats, we train a classiier using \\nthe NaiveBayesClassifier.train() method. This class method builds two \\nprobability distributions for calculating prior probabilities. These are passed in to the \\nNaiveBayesClassifier constructor. The label_probdist contains P(label), the prior \\nprobability for each label. The feature_probdist contains P(feature name = feature \\nvalue | label). In our case, it will store P(word=True | label). Both are calculated \\nbased on the frequency of occurrence of each label, and each feature name and value in the \\ntraining data.\\nText Classiication\\n174\\nThe NaiveBayesClassifier inherits from ClassifierI, which requires subclasses to \\nprovide a labels() method, and at least one of the classify() and prob_classify() \\nmethods. The following diagram shows these and other methods, which will be covered shortly:\\nThere\\'s more...\\nWe can test the accuracy of the classiier using nltk.classify.util.accuracy() \\nand the test_feats created previously.\\n>>> from nltk.classify.util import accuracy\\n>>> accuracy(nb_classifier, test_feats)\\n0.72799999999999998\\nThis tells us that the classiier correctly guessed the label of nearly 73 percent of the testing \\nfeature sets.\\nClassiication probability\\nWhile the classify() method returns only a single label, you can use the prob_classify() \\nmethod to get the classiication probability of each label. This can be useful if you want to use \\nprobability thresholds greater than 50 percent for classiication.\\n>>> probs = nb_classifier.prob_classify(test_feats[0][0])\\n>>> probs.samples()\\n[\\'neg\\', \\'pos\\']\\n>>> probs.max()\\n\\'pos\\'\\n>>> probs.prob(\\'pos\\')\\n0.99999996464309127\\n>>> probs.prob(\\'neg\\')\\n3.5356889692409258e-08\\nChapter 7\\n175\\nIn this case, the classiier says that the irst testing instance is nearly 100 percent likely \\nto be pos.\\nMost informative features\\nThe NaiveBayesClassifier has two methods that are quite useful for learning about your \\ndata. Both methods take a keyword argument n to control how many results to show. The \\nmost_informative_features() method returns a list of the form [(feature name, \\nfeature value)] ordered by most informative to least informative. In our case, the feature \\nvalue will always be True.\\n>>> nb_classifier.most_informative_features(n=5)\\n[(\\'magnificent\\', True), (\\'outstanding\\', True), (\\'insulting\\', True), \\n(\\'vulnerable\\', True), (\\'ludicrous\\', True)]\\nThe show_most_informative_features() method will print out the results from \\nmost_informative_features() and will also include the probability of a feature pair \\nbelonging to each label.\\n>>> nb_classifier.show_most_informative_features(n=5)\\nMost Informative Features\\n    magnificent = True    pos : neg = 15.0 : 1.0\\n    outstanding = True    pos : neg = 13.6 : 1.0\\n    insulting = True      neg : pos = 13.0 : 1.0\\n    vulnerable = True     pos : neg = 12.3 : 1.0\\n    ludicrous = True      neg : pos = 11.8 : 1.0\\nThe informativeness, or information gain, of each feature pair is based on the prior \\nprobability of the feature pair occurring for each label. More informative features are those \\nthat occur primarily in one label and not the other. Less informative features are those that \\noccur frequently in both labels.\\nTraining estimator\\nDuring training, the NaiveBayesClassifier constructs its probability distributions using an \\nestimator parameter, which defaults to nltk.probability.ELEProbDist. But you can \\nuse any estimator you want, and there are quite a few to choose from. The only constraints \\nare that it must inherit from nltk.probability.ProbDistI and its constructor must take \\na bins keyword argument. Here\\'s an example using the LaplaceProdDist:\\n>>> from nltk.probability import LaplaceProbDist\\n>>> nb_classifier = NaiveBayesClassifier.train(train_feats, \\nestimator=LaplaceProbDist)\\n>>> accuracy(nb_classifier, test_feats)\\n0.71599999999999997\\nText Classiication\\n176\\nAs you can see, accuracy is slightly lower, so choose your estimator carefully.\\nYou cannot use nltk.probability.MLEProbDist as the estimator, or \\nany ProbDistI subclass that does not take the bins keyword argument. \\nTraining will fail with TypeError: __init__() got an unexpected \\nkeyword argument \\'bins\\'.\\nManual training\\nYou don\\'t have to use the train() class method to construct a NaiveBayesClassifier. \\nYou can instead create the label_probdist and feature_probdist manually. label_\\nprobdist should be an instance of ProbDistI, and should contain the prior probabilities \\nfor each label. feature_probdist should be a dict whose keys are tuples of the form \\n(label, feature name) and whose values are instances of ProbDistI that have the \\nprobabilities for each feature value. In our case, each ProbDistI should have only one value, \\nTrue=1. Here\\'s a very simple example using manually constructed DictionaryProbDist:\\n>>> from nltk.probability import DictionaryProbDist\\n>>> label_probdist = DictionaryProbDist({\\'pos\\': 0.5, \\'neg\\': 0.5})\\n>>> true_probdist = DictionaryProbDist({True: 1})\\n>>> feature_probdist = {(\\'pos\\', \\'yes\\'): true_probdist, (\\'neg\\', \\'no\\'): \\ntrue_probdist}\\n>>> classifier = NaiveBayesClassifier(label_probdist, feature_\\nprobdist)\\n>>> classifier.classify({\\'yes\\': True})\\n\\'pos\\'\\n>>> classifier.classify({\\'no\\': True})\\n\\'neg\\'\\nSee also\\nIn the next recipes, we will train two more classiiers, the DecisionTreeClassifier, and \\nthe MaxentClassifier. In the Measuring precision and recall of a classiier recipe in this \\nchapter, we will use precision and recall instead of accuracy to evaluate the classiiers. And \\nthen in the Calculating high information words recipe, we will see how using only the most \\ninformative features can improve classiier performance.\\nThe movie_reviews corpus is an instance of CategorizedPlaintextCorpusReader, \\nwhich is covered in the Creating a categorized text corpus recipe in Chapter 3, Creating \\nCustom Corpora.\\nChapter 7\\n177\\nTraining a decision tree classiier\\nThe DecisionTreeClassifier works by creating a tree structure, where each node \\ncorresponds to a feature name, and the branches correspond to the feature values. Tracing \\ndown the branches, you get to the leaves of the tree, which are the classiication labels.\\nGetting ready\\nFor the DecisionTreeClassifier to work for text classiication, you must use NLTK 2.0b9 \\nor later. This is because earlier versions are unable to deal with unknown features. If the \\nDecisionTreeClassifier encountered a word/feature that it hadn\\'t seen before, then \\nit raised an exception. This bug has now been ixed by yours truly, and is included in all NLTK \\nversions since 2.0b9.\\nHow to do it...\\nUsing the same train_feats and test_feats we created from the movie_reviews \\ncorpus in the previous recipe, we can call the DecisionTreeClassifier.train() class \\nmethod to get a trained classiier. We pass binary=True because all of our features are \\nbinary: either the word is present or it\\'s not. For other classiication use cases where you have \\nmulti-valued features, you will want to stick to the default binary=False.\\nIn this context, binary refers to feature values, and is not to be confused \\nwith a binary classiier. Our word features are binary because the value is \\neither True, or the word is not present. If our features could take more than \\ntwo values, we would have to use binary=False. A binary classiier, on the \\nother hand, is a classiier that only chooses between two labels. In our case, \\nwe are training a binary DecisionTreeClassifier on binary features. \\nBut it\\'s also possible to have a binary classiier with non-binary features, or a \\nnon-binary classiier with binary features.\\nFollowing is the code for training and evaluating the accuracy of a \\nDecisionTreeClassifier:\\n>>> from nltk.classify import DecisionTreeClassifier\\n>>> dt_classifier = DecisionTreeClassifier.train(train_feats, \\nbinary=True, entropy_cutoff=0.8, depth_cutoff=5, support_cutoff=30)\\n>>> accuracy(dt_classifier, test_feats)\\n0.68799999999999994\\nText Classiication\\n178\\nThe DecisionTreeClassifier can take much longer to train than the \\nNaiveBayesClassifier. For that reason, the default parameters have \\nbeen overridden so it trains faster. These parameters will be explained later.\\nHow it works...\\nThe DecisionTreeClassifier, like the NaiveBayesClassifier, is also an instance of \\nClassifierI. During training, the DecisionTreeClassifier creates a tree where the \\nchild nodes are also instances of DecisionTreeClassifier. The leaf nodes contain only \\na single label, while the intermediate child nodes contain decision mappings for each feature. \\nThese decisions map each feature value to another DecisionTreeClassifier, which itself \\nmay contain decisions for another feature, or it may be a inal leaf node with a classiication \\nlabel. The train() class method builds this tree from the ground up, starting with the leaf \\nnodes. It then reines itself to minimize the number of decisions needed to get to a label by \\nputting the most informative features at the top.\\nTo classify, the DecisionTreeClassifier looks at the given feature set and traces down \\nthe tree, using known feature names and values to make decisions. Because we are creating \\na binary tree, each DecisionTreeClassifier instance also has a default decision tree, \\nwhich it uses when a known feature is not present in the feature set being classiied. This is \\na common occurrence in text-based feature sets, and indicates that a known word was not in \\nthe text being classiied. This also contributes information towards a classiication decision.\\nThere\\'s more...\\nThe parameters passed in to DecisionTreeClassifier.train() can be tweaked to \\nimprove accuracy or decrease training time. Generally, if you want to improve accuracy, you \\nmust accept a longer training time and if you want to decrease the training time, the accuracy \\nwill most likely decrease as well.\\nEntropy cutoff\\nThe entropy_cutoff is used during the tree reinement process. If the entropy of the \\nprobability distribution of label choices in the tree is greater than the entropy_cutoff, \\nthen the tree is reined further. But if the entropy is lower than the entropy_cutoff, \\nthen tree reinement is halted.\\nEntropy is the uncertainty of the outcome. As entropy approaches 1.0, uncertainty increases \\nand, conversely, as entropy approaches 0.0, uncertainty decreases. In other words, when  \\nyou have similar probabilities, the entropy will be high as each probability has a similar \\nlikelihood (or uncertainty of occurrence). But the more the probabilities differ, the lower  \\nthe entropy will be.\\nChapter 7\\n179\\nEntropy is calculated by giving nltk.probability.entropy() a MLEProbDist created \\nfrom a FreqDist of label counts. Here\\'s an example showing the entropy of various \\nFreqDist values:\\n>>> from nltk.probability import FreqDist, MLEProbDist, entropy\\n>>> fd = FreqDist({\\'pos\\': 30, \\'neg\\': 10})\\n>>> entropy(MLEProbDist(fd))\\n0.81127812445913283\\n>>> fd[\\'neg\\'] = 25\\n>>> entropy(MLEProbDist(fd))\\n0.99403021147695647\\n>>> fd[\\'neg\\'] = 30\\n>>> entropy(MLEProbDist(fd))\\n1.0\\n>>> fd[\\'neg\\'] = 1\\n>>> entropy(MLEProbDist(fd))\\n0.20559250818508304\\nWhat this all means is that if the label occurrence is very skewed one way or the other, the \\ntree doesn\\'t need to be reined because entropy/uncertainty is low. But when the entropy  \\nis greater than entropy_cutoff then the tree must be reined with further decisions to \\nreduce the uncertainty. Higher values of entropy_cutoff will decrease both accuracy \\nand training time.\\nDepth cutoff\\nThe depth_cutoff is also used during reinement to control the depth of the tree. The \\ninal decision tree will never be deeper than the depth_cutoff. The default value is 100, \\nwhich means that classiication may require up to 100 decisions before reaching a leaf node. \\nDecreasing the depth_cutoff will decrease the training time and most likely decrease the \\naccuracy as well.\\nSupport cutoff\\nThe support_cutoff controls how many labeled feature sets are required to reine the \\ntree. As the DecisionTreeClassifier reines itself, labeled feature sets are eliminated \\nonce they no longer provide value to the training process. When the number of labeled  \\nfeature sets is less than or equal to support_cutoff, reinement stops, at least for that \\nsection of the tree.\\nAnother way to look at it is that support_cutoff speciies the minimum number of \\ninstances that are required to make a decision about a feature. If support_cutoff is 20, \\nand you have less than 20 labeled feature sets with a given feature, then you don\\'t have \\nenough instances to make a good decision, and reinement around that feature must come  \\nto a stop.\\nText Classiication\\n180\\nSee also\\nThe previous recipe covered the creation of training and test feature sets from the movie_\\nreviews corpus. In the next recipe, we will cover training a MaxentClassifier, and in the \\nMeasuring precision and recall of a classiier recipe in this chapter, we will use precision and \\nrecall to evaluate all the classiiers.\\nTraining a maximum entropy classiier\\nThe third classiier which we will cover is the MaxentClassifier, also known as a \\nconditional exponential classiier. The maximum entropy classiier converts labeled feature \\nsets to vectors using encoding. This encoded vector is then used to calculate weights for each \\nfeature that can then be combined to determine the most likely label for a feature set.\\nGetting ready\\nThe MaxentClassifier requires the numpy package, and optionally the scipy package. \\nThis is because the feature encodings use numpy arrays. Having scipy installed also means \\nyou will be able to use faster algorithms that consume less memory. You can ind installation \\nfor both at http://www.scipy.org/Installing_SciPy.\\nMany of the algorithms can be quite memory hungry, so you may want \\nto quit all your other programs while training a MaxentClassifier, \\njust to be safe.\\nHow to do it...\\nWe will use the same train_feats and test_feats from the movie_reviews corpus \\nthat we constructed before, and call the MaxentClassifier.train() class method. \\nLike the DecisionTreeClassifier, MaxentClassifier.train() has its own speciic \\nparameters that have been tweaked to speed up training. These parameters will be explained \\nin more detail later.\\n>>> from nltk.classify import MaxentClassifier\\n>>> me_classifier = MaxentClassifier.train(train_feats, \\nalgorithm=\\'iis\\', trace=0, max_iter=1, min_lldelta=0.5)\\n>>> accuracy(me_classifier, test_feats)\\n0.5\\nThe reason this classiier has such a low accuracy is because the parameters have been set \\nsuch that it is unable to learn a more accurate model. This is due to the time required to train \\na suitable model using the iis algorithm. Higher accuracy models can be learned much \\nfaster using the scipy algorithms.\\nChapter 7\\n181\\nIf training is taking a long time, you can usually cut it off manually by hitting \\nCtrl + C. This should stop the current iteration and still return a classiier \\nbased on whatever state the model is in.\\nHow it works...\\nLike the previous classiiers, MaxentClassifier inherits from ClassifierI. Depending \\non the algorithm, MaxentClassifier.train() calls one of the training functions in the \\nnltk.classify.maxent module. If scipy is not installed, the default algorithm is iis, \\nand the function used is train_maxent_classifier_with_iis(). The other algorithm \\nthat doesn\\'t require scipy is gis, which uses the train_maxent_classifier_with_\\ngis() function. gis stands for General Iterative Scaling, while iis stands for Improved \\nIterative Scaling. If scipy is installed, the train_maxent_classifier_with_scipy() \\nfunction is used, and the default algorithm is cg. If megam is installed and you specify the \\nmegam algorithm, then train_maxent_classifier_with_megam() is used.\\nThe basic idea behind the maximum entropy model is to build some probability distributions \\nthat it the observed data, then choose whichever probability distribution has the highest \\nentropy. The gis and iis algorithms do so by iteratively improving the weights used to \\nclassify features. This is where the max_iter and min_lldelta parameters come into play.\\nThe max_iter speciies the maximum number of iterations to go through and update the \\nweights. More iterations will generally improve accuracy, but only up to a point. Eventually, the \\nchanges from one iteration to the next will hit a plateau and further iterations are useless.\\nThe min_lldelta speciies the minimum change in the log likelihood required to continue \\niteratively improving the weights. Before beginning training iterations, an instance of the \\nnltk.classify.util.CutoffChecker is created. When its check() method is called, \\nit uses functions such as nltk.classify.util.log_likelihood() to decide whether \\nthe cutoff limits have been reached. The log likelihood is the log (using math.log()) of \\nthe average label probability of the training data (which is the log of the average likelihood of \\na label). As the log likelihood increases, the model improves. But it too will reach a plateau \\nwhere further increases are so small that there is no point in continuing. Specifying the \\nmin_lldelta allows you to control how much each iteration must increase the log likelihood \\nbefore stopping iterations.\\nText Classiication\\n182\\nThere\\'s more...\\nLike the NaiveBayesClassifier, you can see the most informative features by calling the \\nshow_most_informative_features() method.\\n>>> me_classifier.show_most_informative_features(n=4)\\n-0.740 worst==True and label is \\'pos\\'\\n0.740 worst==True and label is \\'neg\\'\\n0.715 bad==True and label is \\'neg\\'\\n-0.715 bad==True and label is \\'pos\\'\\nThe numbers shown are the weights for each feature. This tells us that the word worst is \\nnegatively weighted towards the pos label, and positively weighted towards the neg label. In \\nother words, if the word worst is found in the feature set, then there\\'s a strong possibility that \\nthe text should be classiied neg.\\nScipy algorithms\\nThe algorithms available when scipy is installed are:\\n \\nf\\nCG (Conjugate gradient algorithm)—the default scipy algorithm\\n \\nf\\nBFGS (Broyden-Fletcher-Goldfarb-Shanno algorithm)—very memory hungry\\n \\nf\\nPowell\\n \\nf\\nLBFGSB (limited memory version of BFGS)\\n \\nf\\nNelder-Mead\\nHere\\'s what happens when you use the CG algorithm:\\n>>> me_classifier = MaxentClassifier.train(train_feats, \\nalgorithm=\\'cg\\', trace=0, max_iter=10)\\n>>> accuracy(me_classifier, test_feats)\\n0.85599999999999998\\nThis is the most accurate classiier so far.\\nChapter 7\\n183\\nMegam algorithm\\nIf you have installed the megam package, then you can use the megam algorithm. It\\'s a \\nbit faster than the scipy algorithms and about as accurate. Installation instructions and \\ninformation can be found at http://www.cs.utah.edu/~hal/megam/. The function \\nnltk.classify.megam.config_megam() can be used to specify where the megam \\nexecutable is found. Or, if megam can be found in the standard executable paths, NLTK will \\nconigure it automatically.\\n>>> me_classifier = MaxentClassifier.train(train_feats, \\nalgorithm=\\'megam\\', trace=0, max_iter=10)\\n[Found megam: /usr/local/bin/megam]\\n>>> accuracy(me_classifier, test_feats)\\n0.86799999999999999\\nThe megam algorithm is highly recommended for its accuracy and speed of training.\\nSee also\\nThe Bag of Words feature extraction and the Training a naive Bayes classiier recipes in this \\nchapter show how to construct the training and testing features from the movie_reviews \\ncorpus. In the next recipe, we will cover how and why to evaluate a classiier using precision \\nand recall instead of accuracy.\\nMeasuring precision and recall of a \\nclassiier\\nIn addition to accuracy, there are a number of other metrics used to evaluate classiiers. \\nTwo of the most common are precision and recall. To understand these two metrics, we \\nmust irst understand false positives and false negatives. False positives happen when a \\nclassiier classiies a feature set with a label it shouldn\\'t have. False negatives happen when \\na classiier doesn\\'t assign a label to a feature set that should have it. In a binary classiier, \\nthese errors happen at the same time.\\nHere\\'s an example: the classiier classiies a movie review as pos, when it should have been \\nneg. This counts as a false positive for the pos label, and a false negative for the neg label. \\nIf the classiier had correctly guessed neg, then it would count as a true positive for the neg \\nlabel, and a true negative for the pos label.\\nHow does this apply to precision and recall? Precision is the lack of false positives, and recall \\nis the lack of false negatives. As you will see, these two metrics are often in competition: the \\nmore precise a classiier is, the lower the recall, and vice versa.\\nText Classiication\\n184\\nHow to do it...\\nLet\\'s calculate the precision and recall of the NaiveBayesClassifier we trained in \\nthe Training a naive Bayes classiier recipe. The precision_recall() function in \\nclassification.py looks like this:\\nimport collections\\nfrom nltk import metrics\\ndef precision_recall(classifier, testfeats):\\n  refsets = collections.defaultdict(set)\\n  testsets = collections.defaultdict(set)\\n  for i, (feats, label) in enumerate(testfeats):\\n    refsets[label].add(i)\\n    observed = classifier.classify(feats)\\n    testsets[observed].add(i)\\n  precisions = {}\\n  recalls = {}\\n  for label in classifier.labels():\\n    precisions[label] = metrics.precision(refsets[label], \\ntestsets[label])\\n    recalls[label] = metrics.recall(refsets[label], testsets[label])\\n  return precisions, recalls\\nThis function takes two arguments:\\n1. The trained classiier.\\n2. Labeled test features, also known as a gold standard.\\nThese are the same arguments you pass to the accuracy() function. The precision_\\nrecall() returns two dictionaries; the irst holds the precision for each label, and the \\nsecond holds the recall for each label. Here\\'s an example usage with the nb_classifier \\nand the test_feats we created in the Training a naive Bayes classiier recipe earlier:\\n>>> from classification import precision_recall\\n>>> nb_precisions, nb_recalls = precision_recall(nb_classifier, test_\\nfeats)\\n>>> nb_precisions[\\'pos\\']\\n0.6413612565445026\\n>>> nb_precisions[\\'neg\\']\\n0.9576271186440678\\n>>> nb_recalls[\\'pos\\']\\n0.97999999999999998\\n>>> nb_recalls[\\'neg\\']\\n0.45200000000000001\\nChapter 7\\n185\\nThis tells us that while the NaiveBayesClassifier can correctly identify most of the pos \\nfeature sets (high recall), it also classiies many of the neg feature sets as pos (low precision). \\nThis behavior contributes to the high precision but low recall for the neg label—as the neg \\nlabel isn\\'t given often (low recall), and when it is, it\\'s very likely to be correct (high precision). \\nThe conclusion could be that there are certain common words that are biased towards the \\npos label, but occur frequently enough in the neg feature sets to cause mis-classiications. \\nTo correct this behavior, we will use only the most informative words in the next recipe, \\nCalculating high information words.\\nHow it works...\\nTo calculate precision and recall, we must build two sets for each label. The irst set is known \\nas the reference set, and contains all the correct values. The second set is called the test \\nset, and contains the values guessed by the classiier. These two sets are compared to \\ncalculate the precision or recall for each label.\\nPrecision is deined as the size of the intersection of both sets divided by the size of the test \\nset. In other words, the percentage of the test set that was guessed correctly. In Python, the \\ncode is float(len(reference.intersection(test))) / len(test).\\nRecall is the size of the intersection of both sets divided by the size of the reference set, \\nor the percentage of the reference set that was guessed correctly. The Python code is \\nfloat(len(reference.intersection(test))) / len(reference).\\nThe precision_recall() function in classification.py iterates over the labeled test \\nfeatures and classiies each one. We store the numeric index of the feature set (starting with \\n0) in the reference set for the known training label, and also store the index in the test set for \\nthe guessed label. If the classiier guesses pos but the training label is neg, then the index is \\nstored in the reference set for neg and the test set for pos.\\nWe use the numeric index because the feature sets aren\\'t \\nhashable, and we need a unique value for each feature set.\\nThe nltk.metrics package contains functions for calculating both precision and recall, \\nso all we really have to do is build the sets, then call the appropriate function.\\nText Classiication\\n186\\nThere\\'s more...\\nLet\\'s try it with the MaxentClassifier we trained in the previous recipe:\\n>>> me_precisions, me_recalls = precision_recall(me_classifier, test_\\nfeats)\\n>>> me_precisions[\\'pos\\']\\n0.8801652892561983\\n>>> me_precisions[\\'neg\\']\\n0.85658914728682167\\n>>> me_recalls[\\'pos\\']\\n0.85199999999999998\\n>>> me_recalls[\\'neg\\']\\n0.88400000000000001\\nThis classiier is much more well-rounded than the NaiveBayesClassifier. In this case, \\nthe label bias is much less signiicant, and the reason is that the MaxentClassifier weighs \\nits features according to its own internal model. Words that are more signiicant are those \\nthat occur primarily in a single label, and will get higher weights in the model. Words that are \\ncommon to both labels will get lower weights, as they are less signiicant.\\nF-measure\\nThe F-measure is deined as the weighted harmonic mean of precision and recall. If p is the \\nprecision, and r is the recall, the formula is:\\n1/(alpha/p + (1-alpha)/r)\\nwhere alpha is a weighing constant that defaults to 0.5. You can use nltk.metrics.f_\\nmeasure() to get the F-measure. It takes the same arguments as for the precision() \\nand recall() functions: a reference set and a test set. It\\'s often used instead of \\naccuracy to measure a classiier. However, precision and recall are found to be much \\nmore useful metrics, as the F-measure can hide the kinds of imbalances we saw with the \\nNaiveBayesClassifier.\\nSee also\\nIn the Training a naive Bayes classiier recipe, we collected training and testing feature sets, \\nand trained the NaiveBayesClassifier. The MaxentClassifier was trained in the \\nTraining a maximum entropy classiier recipe. In the next recipe, we will explore eliminating \\nthe less signiicant words, and use only the high information words to create our feature sets.\\nChapter 7\\n187\\nCalculating high information words\\nA high information word is a word that is strongly biased towards a single classiication \\nlabel. These are the kinds of words we saw when we called the show_most_informative_\\nfeatures() method on both the NaiveBayesClassifier and the MaxentClassifier. \\nSomewhat surprisingly, the top words are different for both classiiers. This discrepancy is due \\nto how each classiier calculates the signiicance of each feature, and it\\'s actually beneicial to \\nhave these different methods as they can be combined to improve accuracy, as we will see in \\nthe next recipe, Combining classiiers with voting.\\nThe low information words are words that are common to all labels. It may be counter-intuitive, \\nbut eliminating these words from the training data can actually improve accuracy, precision, and \\nrecall. The reason this works is that using only high information words reduces the noise and \\nconfusion of a classiier\\'s internal model. If all the words/features are highly biased one way or \\nthe other, it\\'s much easier for the classiier to make a correct guess.\\nHow to do it...\\nFirst, we need to calculate the high information words in the movie_review corpus. \\nWe can do this using the high_information_words() function in featx.py:\\nfrom nltk.metrics import BigramAssocMeasures\\nfrom nltk.probability import FreqDist, ConditionalFreqDist\\ndef high_information_words(labelled_words, score_\\nfn=BigramAssocMeasures.chi_sq, min_score=5):\\n  word_fd = FreqDist()\\n  label_word_fd = ConditionalFreqDist()\\n  for label, words in labelled_words:\\n    for word in words:\\n      word_fd.inc(word)\\n      label_word_fd[label].inc(word)\\n  n_xx = label_word_fd.N()\\n  high_info_words = set()\\n  for label in label_word_fd.conditions():\\n    n_xi = label_word_fd[label].N()\\n    word_scores = collections.defaultdict(int)\\n    for word, n_ii in label_word_fd[label].iteritems():\\n      n_ix = word_fd[word]\\n      score = score_fn(n_ii, (n_ix, n_xi), n_xx)\\n      word_scores[word] = score\\n    bestwords = [word for word, score in word_scores.iteritems() if \\nscore >= min_score]\\nText Classiication\\n188\\n    high_info_words |= set(bestwords)\\n  return high_info_words\\nIt takes one argument , which is a list of 2-tuples of the form [(label, words)] where \\nlabel is the classiication label, and words is a list of words that occur under that label. It \\nreturns a list of the high information words, sorted from most informative to least informative.\\nOnce we have the high information words, we use the feature detector function bag_of_\\nwords_in_set(), also found in featx.py, which will let us ilter out all low information \\nwords.\\ndef bag_of_words_in_set(words, goodwords):\\n  return bag_of_words(set(words) & set(goodwords))\\nWith this new feature detector, we can call label_feats_from_corpus() and get a new \\ntrain_feats and test_feats using split_label_feats(). These two functions were \\ncovered in the Training a naive Bayes classiier recipe in this chapter.\\n>>> from featx import high_information_words, bag_of_words_in_set\\n>>> labels = movie_reviews.categories()\\n>>> labeled_words = [(l, movie_reviews.words(categories=[l])) for l in \\nlabels]\\n>>> high_info_words = set(high_information_words(labeled_words))\\n>>> feat_det = lambda words: bag_of_words_in_set(words, high_info_\\nwords)\\n>>> lfeats = label_feats_from_corpus(movie_reviews, feature_\\ndetector=feat_det)\\n>>> train_feats, test_feats = split_label_feats(lfeats)\\nNow that we have new training and testing feature sets, let\\'s train and evaluate a \\nNaiveBayesClassifier:\\n>>> nb_classifier = NaiveBayesClassifier.train(train_feats)\\n>>> accuracy(nb_classifier, test_feats)\\n0.91000000000000003\\n>>> nb_precisions, nb_recalls = precision_recall(nb_classifier, test_\\nfeats)\\n>>> nb_precisions[\\'pos\\']\\n0.89883268482490275\\n>>> nb_precisions[\\'neg\\']\\n0.92181069958847739\\n>>> nb_recalls[\\'pos\\']\\n0.92400000000000004\\n>>> nb_recalls[\\'neg\\']\\n0.89600000000000002\\nChapter 7\\n189\\nWhile the neg precision and pos recall have both decreased somewhat, neg recall \\nand pos precision have increased drastically. Accuracy is now a little higher than the \\nMaxentClassifier.\\nHow it works...\\nThe high_information_words() function starts by counting the frequency of every word, \\nas well as the conditional frequency for each word within each label. This is why we need the \\nwords to be labelled, so we know how often each word occurs in each label.\\nOnce we have this FreqDist and ConditionalFreqDist, we can score each word on a \\nper-label basis. The default score_fn is nltk.metrics.BigramAssocMeasures.chi_\\nsq(), which calculates the chi-square score for each word using the following parameters:\\n1. n_ii: The frequency of the word in the label.\\n2. n_ix: The total frequency of the word across all labels.\\n3. n_xi: The total frequency of all words that occurred in the label.\\n4. n_xx: The total frequency for all words in all labels.\\nThe simplest way to think about these numbers is that the closer n_ii is to n_ix, the higher \\nthe score. Or, the more often a word occurs in a label, relative to its overall occurrence, the \\nhigher the score.\\nOnce we have the scores for each word in each label, we can ilter out all words whose score \\nis below the min_score threshold. We keep the words that meet or exceed the threshold, \\nand return all high scoring words in each label.\\nIt is recommended to experiment with different values of min_score to \\nsee what happens. In some cases, less words may improve the metrics even \\nmore, while in other cases more words is better.\\nThere\\'s more...\\nThere are a number of other scoring functions available in the BigramAssocMeasures \\nclass, such as phi_sq() for phi-square, pmi() for pointwise mutual information, and \\njaccard() for using the Jaccard index. They all take the same arguments, and so can be \\nused interchangeably with chi_sq().\\nText Classiication\\n190\\nMaxentClassiier with high information words\\nLet\\'s evaluate the MaxentClassifier using the high information words feature sets:\\n>>> me_classifier = MaxentClassifier.train(train_feats, \\nalgorithm=\\'megam\\', trace=0, max_iter=10)\\n>>> accuracy(me_classifier, test_feats)\\n0.88200000000000001\\n>>> me_precisions, me_recalls = precision_recall(me_classifier, test_\\nfeats)\\n>>> me_precisions[\\'pos\\']\\n0.88663967611336036\\n>>> me_precisions[\\'neg\\']\\n0.87747035573122534\\n>>> me_recalls[\\'pos\\']\\n0.876\\n>>> me_recalls[\\'neg\\']\\n0.88800000000000001\\nAs you can see, the improvements are much more modest than with the \\nNaiveBayesClassifier due to the fact that the MaxentClassifier already weights \\nall features by signiicance. But using only the high information words still makes a positive \\ndifference compared to when we used all the words. And the precisions and recalls for each \\nlabel are closer to each other, giving the MaxentClassifier even more well-rounded \\nperformance.\\nDecisionTreeClassiier with high information words\\nNow, let\\'s evaluate the DecisionTreeClassifier:\\n>>> dt_classifier = DecisionTreeClassifier.train(train_feats, \\nbinary=True, depth_cutoff=20, support_cutoff=20, entropy_cutoff=0.01)\\n>>> accuracy(dt_classifier, test_feats)\\n0.68600000000000005\\n>>> dt_precisions, dt_recalls = precision_recall(dt_classifier, test_\\nfeats)\\n>>> dt_precisions[\\'pos\\']\\n0.6741573033707865\\n>>> dt_precisions[\\'neg\\']\\n0.69957081545064381\\n>>> dt_recalls[\\'pos\\']\\n0.71999999999999997\\n>>> dt_recalls[\\'neg\\']\\n0.65200000000000002\\nChapter 7\\n191\\nThe accuracy is about the same, even with a larger depth_cutoff, and smaller support_\\ncutoff and entropy_cutoff. The results show that the DecisionTreeClassifier was \\nalready putting the high information features at the top of the tree, and it will only improve if \\nwe increase the depth signiicantly. But that could make training time prohibitively long.\\nSee also\\nWe started this chapter with the Bag of Words feature extraction recipe. The \\nNaiveBayesClassifier was originally trained in the Training a naive Bayes classiier \\nrecipe, and the MaxentClassifier was trained in the Training a maximum entropy \\nclassiier recipe. Details on precision and recall can be found in the Measuring precision \\nand recall of a classiier recipe. We will be using only high information words in the next two \\nrecipes, where we combine classiiers.\\nCombining classiiers with voting\\nOne way to improve classiication performance is to combine classiiers. The simplest way to \\ncombine multiple classiiers is to use voting, and choose whichever label gets the most votes. \\nFor this style of voting, it\\'s best to have an odd number of classiiers so that there are no ties. \\nThis means combining at least three classiiers together. The individual classiiers should also \\nuse different algorithms; the idea is that multiple algorithms are better than one, and the \\ncombination of many can compensate for individual bias.\\nGetting ready\\nAs we need to have at least three trained classiiers to combine, we are going to use a \\nNaiveBayesClassifier, a DecisionTreeClassifier, and a MaxentClassifier, \\nall trained on the highest information words of the movie_reviews corpus. These were all \\ntrained in the previous recipe, so we will combine these three classiiers with voting.\\nHow to do it...\\nIn the classification.py module, there is a MaxVoteClassifier class.\\nimport itertools\\nfrom nltk.classify import ClassifierI\\nfrom nltk.probability import FreqDist\\nclass MaxVoteClassifier(ClassifierI):\\n  def __init__(self, *classifiers):\\n    self._classifiers = classifiers\\nText Classiication\\n192\\n    self._labels = sorted(set(itertools.chain(*[c.labels() for c in \\nclassifiers])))\\n  def labels(self):\\n    return self._labels\\n  def classify(self, feats):\\n    counts = FreqDist()\\n    for classifier in self._classifiers:\\n      counts.inc(classifier.classify(feats))\\n    return counts.max()\\nTo create it, you pass in a list of classiiers that you want to combine. Once created, it works \\njust like any other classiier. Though it may take about three times longer to classify, it should \\ngenerally be at least as accurate as any individual classiier.\\n>>> from classification import MaxVoteClassifier\\n>>> mv_classifier = MaxVoteClassifier(nb_classifier, dt_classifier, \\nme_classifier)\\n>>> mv_classifier.labels()\\n[\\'neg\\', \\'pos\\']\\n>>> accuracy(mv_classifier, test_feats)\\n0.89600000000000002\\n>>> mv_precisions, mv_recalls = precision_recall(mv_classifier, test_\\nfeats)\\n>>> mv_precisions[\\'pos\\']\\n0.8928571428571429\\n>>> mv_precisions[\\'neg\\']\\n0.89919354838709675\\n>>> mv_recalls[\\'pos\\']\\n0.90000000000000002\\n>>> mv_recalls[\\'neg\\']\\n0.89200000000000002\\nThese metrics are about on par with the MaxentClassifier and \\nNaiveBayesClassifier. Some numbers are slightly better, some worse. It\\'s likely \\nthat a signiicant improvement to the DecisionTreeClassifier could produce \\nsome better numbers.\\nChapter 7\\n193\\nHow it works...\\nThe MaxVoteClassifier extends the nltk.classify.ClassifierI interface, which \\nrequires implementing at least two methods:\\n \\nf\\nThe labels() function must return a list of possible labels. This will be the union \\nof the labels() of each classiier passed in at initialization.\\n \\nf\\nThe classify() function takes a single feature set and returns a label. The \\nMaxVoteClassifier iterates over its classiiers and calls classify() on each \\nof them, recording their label as a vote in a FreqDist. The label with the most votes \\nis returned using FreqDist.max().\\nWhile it doesn\\'t check for this, the MaxVoteClassifier assumes that all the classiiers \\npassed in at initialization use the same labels. Breaking this assumption may lead to odd \\nbehavior.\\nSee also\\nIn the previous recipe, we trained a NaiveBayesClassifier, a MaxentClassifier, and \\na DecisionTreeClassifier using only the highest information words. In the next recipe, \\nwe will use the reuters corpus and combine many binary classiiers in order to create a \\nmulti-label classiier.\\nClassifying with multiple binary classiiers\\nSo far we have focused on binary classiiers, which classify with one of two possible labels. \\nThe same techniques for training a binary classiier can also be used to create a multi-class \\nclassiier, which is a classiier that can classify with one of many possible labels. But there \\nare also cases where you need to be able to classify with multiple labels. A classiier that can \\nreturn more than one label is a multi-label classiier.\\nA common technique for creating a multi-label classiier is to combine many binary classiiers, \\none for each label. You train each binary classiier so that it either returns a known label, or \\nreturns something else to signal that the label does not apply. Then you can run all the binary \\nclassiiers on your feature set to collect all the applicable labels.\\nText Classiication\\n194\\nGetting ready\\nThe reuters corpus contains multi-labeled text that we can use for training and evaluation.\\n>>> from nltk.corpus import reuters\\n>>> len(reuters.categories())\\n90\\nWe will train one binary classiier per label, which means we will end up with  \\n90 binary classiiers.\\nHow to do it...\\nFirst, we should calculate the high information words in the reuters corpus. This is done \\nwith the reuters_high_info_words() function in featx.py.\\nfrom nltk.corpus import reuters\\ndef reuters_high_info_words(score_fn=BigramAssocMeasures.chi_sq):\\n  labeled_words = []\\n  for label in reuters.categories():\\n    labeled_words.append((label, reuters.words(categories=[label])))\\n  return high_information_words(labeled_words, score_fn=score_fn)\\nThen we need to get training and test feature sets based on those high information words. \\nThis is done with the reuters_train_test_feats(), also found in featx.py. It defaults \\nto using bag_of_words() as its feature_detector, but we will be overriding this using \\nbag_of_words_in_set() to use only the high information words.\\ndef reuters_train_test_feats(feature_detector=bag_of_words):\\n  train_feats = []\\n  test_feats = []\\n  for fileid in reuters.fileids():\\n    if fileid.startswith(\\'training\\'):\\n      featlist = train_feats\\n    else: # fileid.startswith(\\'test\\')\\n      featlist = test_feats\\n    feats = feature_detector(reuters.words(fileid))\\n    labels = reuters.categories(fileid)\\n    featlist.append((feats, labels))\\n  return train_feats, test_feats\\nChapter 7\\n195\\nWe can use these two functions to get a list of multi-labeled training and testing feature sets.\\n>>> from featx import reuters_high_info_words, reuters_train_test_\\nfeats\\n>>> rwords = reuters_high_info_words()\\n>>> featdet = lambda words: bag_of_words_in_set(words, rwords)\\n>>> multi_train_feats, multi_test_feats = reuters_train_test_\\nfeats(featdet)\\nThe multi_train_feats and multi_test_feats are multi-labeled feature sets. \\nThat means they have a list of labels, instead of a single label, and they look like the \\n[(featureset, [label])], as each feature set can have one or more labels. With this \\ntraining data, we can train multiple binary classiiers. The train_binary_classifiers() \\nfunction in the classification.py takes a training function, a list of multi-label feature \\nsets, and a set of possible labels to return a dict of the label : binary classiier.\\ndef train_binary_classifiers(trainf, labelled_feats, labelset):\\n  pos_feats = collections.defaultdict(list)\\n  neg_feats = collections.defaultdict(list)\\n  classifiers = {}\\n  for feat, labels in labelled_feats:\\n    for label in labels:\\n      pos_feats[label].append(feat)\\n    for label in labelset - set(labels):\\n      neg_feats[label].append(feat)\\n  for label in labelset:\\n    postrain = [(feat, label) for feat in pos_feats[label]]\\n    negtrain = [(feat, \\'!%s\\' % label) for feat in neg_feats[label]]\\n    classifiers[label] = trainf(postrain + negtrain)\\n  return classifiers\\nTo use this function, we need to provide a training function that takes a single \\nargument, which is the training data. This will be a simple lambda wrapper around the \\nMaxentClassifier.train(), so we can specify extra keyword arguments.\\n>>> from classification import train_binary_classifiers\\n>>> trainf = lambda train_feats: MaxentClassifier.train(train_feats, \\nalgorithm=\\'megam\\', trace=0, max_iter=10)\\n>>> labelset = set(reuters.categories())\\n>>> classifiers = train_binary_classifiers(trainf, multi_train_feats, \\nlabelset)\\n>>> len(classifiers)\\n90\\nText Classiication\\n196\\nNow we can deine a MultiBinaryClassifier, which takes a list of labeled classiiers \\nof the form [(label, classifier)] where the classifier is assumed to be a binary \\nclassiier that either returns the label, or something else if the label doesn\\'t apply.\\nfrom nltk.classify import MultiClassifierI\\nclass MultiBinaryClassifier(MultiClassifierI):\\n  def __init__(self, *label_classifiers):\\n    self._label_classifiers = dict(label_classifiers)\\n    self._labels = sorted(self._label_classifiers.keys())\\n  def labels(self):\\n    return self._labels\\n  def classify(self, feats):\\n    lbls = set()\\n    for label, classifier in self._label_classifiers.iteritems():\\n      if classifier.classify(feats) == label:\\n        lbls.add(label)\\n    return lbls\\nWe can construct this class using the binary classiiers we just created.\\n>>> from classification import MultiBinaryClassifier\\n>>> multi_classifier = MultiBinaryClassifier(*classifiers.items())\\nTo evaluate this classiier, we can use precision and recall, but not accuracy. That\\'s because \\nthe accuracy function assumes single values, and doesn\\'t take into account partial matches. \\nFor example, if the multi-classiier returns three labels for a feature set, and two of them are \\ncorrect but the third is not, then the accuracy() would mark that as incorrect. So instead \\nof using accuracy, we will use the masi distance, which measures partial overlap between \\ntwo sets. The lower the masi distance, the better the match. A lower average masi distance, \\ntherefore, means more accurate partial matches. The multi_metrics() function in the \\nclassification.py calculates the precision and recall of each label, along with the \\naverage masi distance.\\nimport collections\\nfrom nltk import metrics\\ndef multi_metrics(multi_classifier, test_feats):\\n  mds = []\\n  refsets = collections.defaultdict(set)\\n  testsets = collections.defaultdict(set)\\n  for i, (feat, labels) in enumerate(test_feats):\\n    for label in labels:\\n      refsets[label].add(i)\\n    guessed = multi_classifier.classify(feat)\\nChapter 7\\n197\\n    for label in guessed:\\n      testsets[label].add(i)\\n    mds.append(metrics.masi_distance(set(labels), guessed))\\n  avg_md = sum(mds) / float(len(mds))\\n  precisions = {}\\n  recalls = {}\\n  for label in multi_classifier.labels():\\n    precisions[label] = metrics.precision(refsets[label], \\ntestsets[label])\\n    recalls[label] = metrics.recall(refsets[label], testsets[label])\\n  return precisions, recalls, avg_md\\nUsing this with the multi_classifier we just created, gives us the following results:\\n>>> from classification import multi_metrics\\n>>> multi_precisions, multi_recalls, avg_md = multi_metrics(multi_\\nclassifier, multi_test_feats)\\n>>> avg_md\\n0.18191264129488705\\nSo our average masi distance is fairly low, which means our multi-label classiier is usually \\nmostly accurate. Let\\'s take a look at a few precisions and recalls:\\n>>> multi_precisions[\\'zinc\\']\\n1.0\\n>>> multi_recalls[\\'zinc\\']\\n0.84615384615384615\\n>>> len(reuters.fileids(categories=[\\'zinc\\']))\\n34\\n>>> multi_precisions[\\'sunseed\\']\\n0.5\\n>>> multi_recalls[\\'sunseed\\']\\n0.20000000000000001\\n>>> len(reuters.fileids(categories=[\\'sunseed\\']))\\n16\\n>>> multi_precisions[\\'rand\\']\\nNone\\n>>> multi_recalls[\\'rand\\']\\n0.0\\n>>> len(reuters.fileids(categories=[\\'rand\\']))\\n3\\nText Classiication\\n198\\nAs you can see, there\\'s quite a range of values. But, in general, the labels that have more \\nfeature sets will have higher precision and recall, and those with less feature sets will have \\nlower performance. When there\\'s not a lot of feature sets for a classiier to learn from, you \\ncan\\'t expect it to perform well.\\nHow it works...\\nThe reuters_high_info_words() function is fairly simple; it constructs a list of \\n[(label, words)] for each category of the reuters corpus, then passes it in to the \\nhigh_information_words() function to return a list of the most informative words in \\nthe reuters corpus.\\nWith the resulting set of words, we create a feature detector function using the bag_of_\\nwords_in_set(). This is then passed in to the reuters_train_test_feats(), which \\nreturns two lists, the irst containing [(feats, labels)] for all the training iles, and the \\nsecond list has the same for all the test iles.\\nNext, we train a binary classiier for each label using train_binary_classifiers(). \\nThis function constructs two lists for each label, one containing positive training feature \\nsets, the other containing negative training feature sets. The Positive feature sets are those \\nfeature sets that classify for the label. The Negative feature sets for a label comes from the \\npositive feature sets for all other labels. For example, a feature set that is positive for zinc \\nand sunseed is a negative example for all the other 88 labels. Once we have positive and \\nnegative feature sets for each label, we can train a binary classiier for each label using the \\ngiven training function.\\nWith the resulting dictionary of binary classiiers, we create an instance of the \\nMultiBinaryClassifier. This class extends the nltk.classify.MultiClassifierI \\ninterface, which requires at least two functions:\\n1. The labels() function must return a list of possible labels.\\n2. The classify() function takes a single feature set and returns a set of labels. \\nTo create this set, we iterate over the binary classiiers, and any time a call to the \\nclassify() returns its label, we add it to the set. If it returns something else, \\nwe continue.\\nChapter 7\\n199\\nFinally, we evaluate the multi-label classiier using the multi_metrics() function. \\nIt is similar to the precision_recall() function from the Measuring precision and \\nrecall of a classiier recipe, but in this case we know the classiier is an instance of the \\nMultiClassifierI and it can therefore return multiple labels. It also keeps track of \\nthe masi distance for each set of classiication labels using the nltk.metrics.masi_\\ndistance(). The multi_metrics() function returns three values:\\n1. A dictionary of precisions for each label.\\n2. A dictionary of recalls for each label.\\n3. The average masi distance for each feature set.\\nThere\\'s more...\\nThe nature of the reuters corpus introduces the class-imbalance problem. This problem \\noccurs when some labels have very few feature sets, and other labels have many. The \\nbinary classiiers that have few positive instances to train on end up with far more negative \\ninstances, and are therefore strongly biased towards the negative label. There\\'s nothing \\ninherently wrong about this, as the bias relects the data, but the negative instances can \\noverwhelm the classiier to the point where it\\'s nearly impossible to get a positive result. There \\nare a number of advanced techniques for overcoming this problem, but they are out of the \\nscope of this book.\\nSee also\\nThe MaxentClassifier is covered in the Training a maximum entropy classiier recipe in \\nthis chapter. The Measuring precision and recall of a classiier recipe shows how to evaluate \\na classiier, while the Calculating high information words recipe describes how to use only the \\nbest features.\\n8\\nDistributed \\nProcessing and \\nHandling Large \\nDatasets\\nIn this chapter, we will cover:\\n \\nf\\nDistributed tagging with execnet\\n \\nf\\nDistributed chunking with execnet\\n \\nf\\nParallel list processing with execnet\\n \\nf\\nStoring a frequency distribution in Redis\\n \\nf\\nStoring a conditional frequency distribution in Redis\\n \\nf\\nStoring an ordered dictionary in Redis\\n \\nf\\nDistributed word scoring with Redis and execnet\\nDistributed Processing and Handling Large Datasets\\n202\\nIntroduction\\nNLTK is great for in-memory single-processor natural language processing. However, there are \\ntimes when you have a lot of data to process and want to take advantage of multiple CPUs, \\nmulti-core CPUs, and even multiple computers. Or perhaps you want to store frequencies \\nand probabilities in a persistent, shared database so multiple processes can access it \\nsimultaneously. For the irst case, we\\'ll be using execnet to do parallel and distributed \\nprocessing with NLTK. For the second case, you\\'ll learn how to use the Redis data structure \\nserver/database to store frequency distributions and more.\\nDistributed tagging with execnet\\nExecnet is a distributed execution library for python. It allows you to create gateways \\nand channels for remote code execution. A gateway is a connection from the calling \\nprocess to a remote environment. The remote environment can be a local subprocess or \\nan SSH connection to a remote node. A channel is created from a gateway and handles \\ncommunication between the channel creator and the remote code.\\nSince many NLTK processes require 100 percent CPU utilization during computation, execnet \\nis an ideal way to distribute that computation for maximum resource usage. You can create \\none gateway per CPU core, and it doesn\\'t matter whether the cores are in your local computer \\nor spread across remote machines. In many situations, you only need to have the trained \\nobjects and data on a single machine, and can send the objects and data to the remote \\nnodes as needed.\\nGetting ready\\nYou\\'ll need to install execnet for this to work. It should be as simple as sudo pip install \\nexecnet or sudo easy_install execnet. The current version of execnet, as of this \\nwriting, is 1.0.8. The execnet homepage, which has API documentation and examples, is at \\nhttp://codespeak.net/execnet/.\\nHow to do it...\\nWe start by importing the required modules, as well as an additional module remote_tag.\\npy that will be explained in the next section. We also need to import pickle so we can \\nserialize the tagger. Execnet does not natively know how to deal with complex objects such \\nas  a part-of-speech tagger, so we must dump the tagger to a string using pickle.dumps(). \\nWe\\'ll use the default tagger that\\'s used by the nltk.tag.pos_tag() function, but you \\ncould load and dump any pre-trained part-of-speech tagger as long as it implements the \\nTaggerI interface.\\nChapter 8\\n203\\nOnce we have a serialized tagger, we start execnet by making a gateway with execnet.\\nmakegateway(). The default gateway creates a Python subprocess, and we can call the \\nremote_exec() method with the remote_tag module to create a channel. With an open \\nchannel, we send over the serialized tagger and then the irst tokenized sentence of the \\ntreebank corpus.\\nYou don\\'t have to do any special serialization of simple types \\nsuch as lists and tuples, since execnet already knows how \\nto handle serializing the built-in types.\\nNow if we call channel.receive(), we get back a tagged sentence that is equivalent to the \\nirst tagged sentence in the treebank corpus, so we know the tagging worked. We end by \\nexiting the gateway, which closes the channel and kills the subprocess.\\n>>> import execnet, remote_tag, nltk.tag, nltk.data\\n>>> from nltk.corpus import treebank\\n>>> import cPickle as pickle\\n>>> tagger = pickle.dumps(nltk.data.load(nltk.tag._POS_TAGGER))\\n>>> gw = execnet.makegateway()\\n>>> channel = gw.remote_exec(remote_tag)\\n>>> channel.send(tagger)\\n>>> channel.send(treebank.sents()[0])\\n>>> tagged_sentence = channel.receive()\\n>>> tagged_sentence == treebank.tagged_sents()[0]\\nTrue\\n>>> gw.exit()\\nVisually, the communication process looks like this:\\nDistributed Processing and Handling Large Datasets\\n204\\nHow it works...\\nThe gateway\\'s remote_exec() method takes a single argument that can be one of the \\nfollowing three types:\\n1. A string of code to execute remotely.\\n2. The name of a pure function that will be serialized and executed remotely.\\n3. The name of a pure module whose source will be executed remotely.\\nWe use the third option with the remote_tag.py module, which is deined as follows:\\n  import cPickle as pickle\\n  \\n  if __name__ == \\'__channelexec__\\':\\n    tagger = pickle.loads(channel.receive())\\n    \\n    for sentence in channel:\\n      channel.send(tagger.tag(sentence))\\nA pure module is a module that is self-contained. It can only access Python modules that \\nare available where it executes, and does not have access to any variables or states that \\nexist wherever the gateway is initially created. To detect that the module is being executed by \\nexecnet, you can look at the __name__ variable. If it\\'s equal to \\'__channelexec__\\', then \\nit is being used to create a remote channel. This is similar to doing if __name__ == \\'__\\nmain__\\' to check if a module is being executed on the command line.\\nThe irst thing we do is call channel.receive() to get the serialized tagger, which we \\nload using pickle.loads(). You may notice that channel is not imported anywhere—that\\'s \\nbecause it is included in the global namespace of the module. Any module that execnet \\nexecutes remotely has access to the channel variable in order to communicate with the \\nchannel creator.\\nOnce we have the tagger, we iteratively tag() each tokenized sentence that we receive \\nfrom the channel. This allows us to tag as many sentences as the sender wants to send, \\nas iteration will not stop until the channel is closed. What we\\'ve essentially created is a \\ncompute node for part-of-speech tagging that dedicates 100 percent of its resources to \\ntagging whatever sentences it receives. As long as the channel remains open, the node is \\navailable for processing.\\nChapter 8\\n205\\nThere\\'s more...\\nThis is a simple example that opens a single gateway and channel. But execnet can do a lot \\nmore, such as opening multiple channels to increase parallel processing, as well as opening \\ngateways to remote hosts over SSH to do distributed processing.\\nMultiple channels\\nWe can create multiple channels, one per gateway, to make the processing more parallel. \\nEach gateway creates a new subprocess (or remote interpreter if using an SSH gateway) \\nand we use one channel per gateway for communication. Once we\\'ve created two channels, \\nwe can combine them using the MultiChannel class, which allows us to iterate over the \\nchannels, and make a receive queue to receive messages from each channel.\\nAfter creating each channel and sending the tagger, we cycle through the channels to send an \\neven number of sentences to each channel for tagging. Then we collect all the responses from \\nthe queue. A call to queue.get() will return a 2-tuple of (channel, message) in case \\nyou need to know which channel the message came from.\\nIf you don\\'t want to wait forever, you can also pass a timeout \\nkeyword argument with the maximum number of seconds you want \\nto wait, as in queue.get(timeout=4). This can be a good way \\nto handle network errors.\\nOnce all the tagged sentences have been collected, we can exit the gateways. Here\\'s the code:\\n>>> import itertools\\n>>> gw1 = execnet.makegateway()\\n>>> gw2 = execnet.makegateway()\\n>>> ch1 = gw1.remote_exec(remote_tag)\\n>>> ch1.send(tagger)\\n>>> ch2 = gw2.remote_exec(remote_tag)\\n>>> ch2.send(tagger)\\n>>> mch = execnet.MultiChannel([ch1, ch2])\\n>>> queue = mch.make_receive_queue()\\n>>> channels = itertools.cycle(mch)\\n>>> for sentence in treebank.sents()[:4]:\\n...    channel = channels.next()\\n...    channel.send(sentence)\\n>>> tagged_sentences = []\\n>>> for i in range(4):\\n...    channel, tagged_sentence = queue.get()\\n...    tagged_sentences.append(tagged_sentence)\\n>>> len(tagged_sentences)\\nDistributed Processing and Handling Large Datasets\\n206\\n4\\n>>> gw1.exit()\\n>>> gw2.exit()\\nLocal versus remote gateways\\nThe default gateway spec is popen, which creates a Python subprocess on the \\nlocal machine. This means execnet.makegateway() is equivalent to execnet.\\nmakegateway(\\'popen\\'). If you have passwordless SSH access to a remote machine, then \\nyou can create a remote gateway using execnet.makegateway(\\'ssh=remotehost\\') \\nwhere remotehost should be the hostname of the machine. A SSH gateway spawns a new \\nPython interpreter for executing the code remotely. As long as the code you\\'re using for remote \\nexecution is pure, you only need a Python interpreter on the remote machine.\\nChannels work exactly the same no matter what kind of gateway is used; the only difference \\nwill be communication time. This means you can mix and match local subprocesses with \\nremote interpreters to distribute your computations across many machines in a network. \\nThere are many more details on gateways in the API documentation at http://codespeak.\\nnet/execnet/basics.html.\\nSee also\\nPart-of-speech tagging and taggers are covered in detail in Chapter 4, Part-of-Speech \\nTagging. In the next recipe, we\\'ll use execnet to do distributed chunk extraction.\\nDistributed chunking with execnet\\nIn this recipe, we\\'ll do chunking and tagging over an execnet gateway. This will be very \\nsimilar to the tagging in the previous recipe, but we\\'ll be sending two objects instead of one, \\nand we will be receiving a Tree instead of a list, which requires pickling and unpickling for \\nserialization.\\nGetting ready\\nAs in the previous recipe, you must have execnet installed.\\nChapter 8\\n207\\nHow to do it...\\nThe setup code is very similar to the last recipe, and we\\'ll use the same pickled tagger as \\nwell. First we\\'ll pickle the default chunker used by nltk.chunk.ne_chunk(), though any \\nchunker would do. Next, we make a gateway for the remote_chunk module, get a channel, \\nand send the pickled tagger and chunker over. Then we receive back a pickled Tree, \\nwhich we can unpickle and inspect to see the result. Finally, we exit the gateway.\\n>>> import execnet, remote_chunk\\n>>> import nltk.data, nltk.tag, nltk.chunk\\n>>> import cPickle as pickle\\n>>> from nltk.corpus import treebank_chunk\\n>>> tagger = pickle.dumps(nltk.data.load(nltk.tag._POS_TAGGER))\\n>>> chunker = pickle.dumps(nltk.data.load(nltk.chunk._MULTICLASS_NE_\\nCHUNKER))\\n>>> gw = execnet.makegateway()\\n>>> channel = gw.remote_exec(remote_chunk)\\n>>> channel.send(tagger)\\n>>> channel.send(chunker)\\n>>> channel.send(treebank_chunk.sents()[0])\\n>>> chunk_tree = pickle.loads(channel.receive())\\n>>> chunk_tree\\nTree(\\'S\\', [Tree(\\'PERSON\\', [(\\'Pierre\\', \\'NNP\\')]), Tree(\\'ORGANIZATION\\', \\n[(\\'Vinken\\', \\'NNP\\')]), (\\',\\', \\',\\'), (\\'61\\', \\'CD\\'), (\\'years\\', \\'NNS\\'), \\n(\\'old\\', \\'JJ\\'), (\\',\\', \\',\\'), (\\'will\\', \\'MD\\'), (\\'join\\', \\'VB\\'), (\\'the\\', \\n\\'DT\\'), (\\'board\\', \\'NN\\'), (\\'as\\', \\'IN\\'), (\\'a\\', \\'DT\\'), (\\'nonexecutive\\', \\n\\'JJ\\'), (\\'director\\', \\'NN\\'), (\\'Nov.\\', \\'NNP\\'), (\\'29\\', \\'CD\\'), (\\'.\\', \\'.\\')])\\n>>> gw.exit()\\nThe communication this time is slightly different.\\nDistributed Processing and Handling Large Datasets\\n208\\nHow it works...\\nThe remote_chunk.py module is just a little bit more complicated than the remote_tag.\\npy module from the previous recipe. In addition to receiving a pickled tagger, it also expects \\nto receive a pickled chunker that implements the ChunkerI interface. Once it has both a \\ntagger and a chunker, it expects to receive any number of tokenized sentences, which it \\ntags and parses into a Tree. This tree is then pickled and sent back over the channel.\\nimport cPickle as pickle\\nif __name__ == \\'__channelexec__\\':\\n  tagger = pickle.loads(channel.receive())\\n  chunker = pickle.loads(channel.receive())\\n  for sent in channel:\\n    tree = chunker.parse(tagger.tag(sent))\\n    channel.send(pickle.dumps(tree))\\nThe Tree must be pickled because it is not a simple built-in type.\\nThere\\'s more...\\nNote that the remote_chunk module is pure. Its only external dependency is the pickle \\n(or cPickle) module, which is part of the Python standard library. It doesn\\'t need to import \\nany NLTK modules in order to use the tagger or chunker, because all the necessary data is \\npickled and sent over the channel. As long as you structure your remote code like this, with \\nno external dependencies, you only need NLTK to be installed on a single machine—the one \\nthat starts the gateway and sends the objects over the channel.\\nPython subprocesses\\nIf you look at your task/system monitor (or top in *nix) while running the execnet code, \\nyou may notice a few extra python Processes. Every gateway spawns a new, self-contained, \\nshared-nothing Python interpreter process, which is killed when you call the exit() method. \\nUnlike with threads, there is no shared memory to worry about, and no global interpreter lock \\nto slow things down. All you have are separate communicating processes. This is true whether \\nthe processes are local or remote. Instead of locking and synchronization, all you have to \\nworry about is the order in which the messages are sent and received.\\nSee also\\nThe previous recipe explains execnet gateways and channels in detail. In the next recipe, \\nwe\\'ll use execnet to process a list in parallel.\\nChapter 8\\n209\\nParallel list processing with execnet\\nThis recipe presents a pattern for using execnet to process a list in parallel. It\\'s a function \\npattern for mapping each element in the list to a new value, using execnet to do the \\nmapping in parallel.\\nHow to do it...\\nFirst, we need to decide exactly what we want to do. In this example, we\\'ll just double integers, \\nbut we could do any pure computation. Following is the module remote_double.py, which \\nwill be executed by execnet. It receives a 2-tuple of (i, arg), assumes arg is a number, \\nand sends back (i, arg*2). The need for i will be explained in the next section.\\nif __name__ == \\'__channelexec__\\':\\n  for (i, arg) in channel:\\n    channel.send((i, arg * 2))\\nTo use this module to double every element in a list, we import the plists module (explained \\nin the next section) and call plists.map() with the remote_double module, and a list of \\nintegers to double.\\n>>> import plists, remote_double\\n>>> plists.map(remote_double, range(10))\\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\\nCommunication between channels is very simple, as shown in the following diagram:\\nHow it works...\\nThe map() function is deined in plists.py. It takes a pure module, a list of arguments, \\nand an optional list of 2-tuples consisting of (spec, count). The default specs are \\n[(\\'popen\\', 2)] , which means we\\'ll open two local gateways and channels. Once these \\nchannels are opened, we put them into an itertools cycle, which creates an ininite iterator \\nthat cycles back to the beginning once it hits the end.\\nDistributed Processing and Handling Large Datasets\\n210\\nNow we can send each argument in args to a channel for processing, and since the \\nchannels are cycled, each channel gets an almost even distribution of arguments. This is \\nwhere i comes in—we don\\'t know in what order we\\'ll get the results back, so i, as the index of \\neach arg in the list, is passed to the channel and back so we can combine the results in the \\noriginal order. We then wait for results with a MultiChannel receive queue and insert them \\ninto a pre-illed list that\\'s the same length as the original args. Once we have all the expected \\nresults, we can exit the gateways and return the results.\\nimport itertools, execnet\\ndef map(mod, args, specs=[(\\'popen\\', 2)]):\\n  gateways = []\\n  channels = []\\n  for spec, count in specs:\\n    for i in range(count):\\n      gw = execnet.makegateway(spec)\\n      gateways.append(gw)\\n      channels.append(gw.remote_exec(mod))\\n  cyc = itertools.cycle(channels)\\n  for i, arg in enumerate(args):\\n    channel = cyc.next()\\n    channel.send((i, arg))\\n  mch = execnet.MultiChannel(channels)\\n  queue = mch.make_receive_queue()\\n  l = len(args)\\n  results = [None] * l\\n  for j in range(l):\\n    channel, (i, result) = queue.get()\\n    results[i] = result\\n  for gw in gateways:\\n    gw.exit()\\n  return results\\nThere\\'s more...\\nYou can increase the parallelization by modifying the specs, as follows:\\n>>> plists.map(remote_double, range(10), [(\\'popen\\', 4)])\\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\\nHowever, more parallelization does not necessarily mean faster processing. It depends on the \\navailable resources, and the more gateways and channels you have open, the more overhead \\nis required. Ideally there should be one gateway and channel per CPU core.\\nChapter 8\\n211\\nYou can use plists.map() with any pure module as long as it receives and sends back \\n2-tuples where i is the irst element. This pattern is most useful when you have a bunch \\nof numbers to crunch, and want to process them as quickly as possible.\\nSee also\\nThe previous recipes cover execnet features in greater detail.\\nStoring a frequency distribution in Redis\\nThe nltk.probability.FreqDist class is used in many classes throughout NLTK for \\nstoring and managing frequency distributions. It\\'s quite useful, but it\\'s all in-memory, and \\ndoesn\\'t provide a way to persist the data. A single FreqDist is also not accessible to \\nmultiple processes. We can change all that by building a FreqDist on top of Redis.\\nRedis is a data structure server that is one of the more popular NoSQL databases. \\nAmong other things, it provides a network accessible database for storing dictionaries  \\n(also known as hash maps). Building a FreqDist interface to a Redis hash map will allow \\nus to create a persistent FreqDist that is accessible to multiple local and remote processes \\nat the same time.\\nMost Redis operations are atomic, so it\\'s even possible to have \\nmultiple processes write to the FreqDist concurrently.\\nGetting ready\\nFor this and subsequent recipes, we need to install both Redis and redis-py. A quick \\nstart install guide for Redis is available at http://code.google.com/p/redis/wiki/\\nQuickStart. To use hash maps, you should install at least version 2.0.0 (the latest version \\nas of this writing).\\nThe Redis Python driver redis-py can be installed using pip install redis or easy_\\ninstall redis. Ensure you install at least version 2.0.0 to use hash maps. The redis-\\npy homepage is at http://github.com/andymccurdy/redis-py/.\\nOnce both are installed and a redis-server process is running, you\\'re ready to go. Let\\'s \\nassume redis-server is running on localhost on port 6379 (the default host and port).\\nDistributed Processing and Handling Large Datasets\\n212\\nHow to do it...\\nThe FreqDist class extends the built-in dict class, which makes a FreqDist an enhanced \\ndictionary. The FreqDist class provides two additional key methods: inc() and N(). The \\ninc() method takes a single sample argument for the key, along with an optional count \\nkeyword argument that defaults to 1, and increments the value at sample by count. N() \\nreturns the number of sample outcomes, which is the sum of all the values in the frequency \\ndistribution.\\nWe can create an API-compatible class on top of Redis by extending a RedisHashMap (that \\nwill be explained in the next section), then implementing the inc() and N() methods. \\nSince the FreqDist only stores integers, we also override a few other methods to ensure \\nvalues are always integers. This RedisHashFreqDist (deined in redisprob.py) uses the \\nhincrby command for the inc() method to increment the sample value by count, and \\nsums all the values in the hash map for the N() method.\\nfrom rediscollections import RedisHashMap\\nclass RedisHashFreqDist(RedisHashMap):\\n  def inc(self, sample, count=1):\\n    self._r.hincrby(self._name, sample, count)\\n  def N(self):\\n    return int(sum(self.values()))\\n  def __getitem__(self, key):\\n    return int(RedisHashMap.__getitem__(self, key) or 0)\\n  def values(self):\\n    return [int(v) for v in RedisHashMap.values(self)]\\n  def items(self):\\n    return [(k, int(v)) for (k, v) in RedisHashMap.items(self)]\\nWe can use this class just like a FreqDist. To instantiate it, we must pass a Redis \\nconnection and the name of our hash map. The name should be a unique reference to \\nthis particular FreqDist so that it doesn\\'t clash with any other keys in Redis.\\n>>> from redis import Redis\\n>>> from redisprob import RedisHashFreqDist\\n>>> r = Redis()\\n>>> rhfd = RedisHashFreqDist(r, \\'test\\')\\n>>> len(rhfd)\\n0\\n>>> rhfd.inc(\\'foo\\')\\n>>> rhfd[\\'foo\\']\\n1\\n>>> rhfd.items()\\n>>> len(rhfd)\\n1\\nChapter 8\\n213\\nThe name of the hash map and the sample keys will be encoded to replace \\nwhitespace and & characters with _. This is because the Redis protocol \\nuses these characters for communication. It\\'s best if the name and keys don\\'t \\ninclude whitespace to begin with.\\nHow it works...\\nMost of the work is done in the RedisHashMap class, found in rediscollections.py, \\nwhich extends collections.MutableMapping, then overrides all methods that require \\nRedis-speciic commands. Here\\'s an outline of each method that uses a speciic Redis \\ncommand:\\n \\nf\\n__len__(): Uses the hlen command to get the number of elements in the hash \\nmap\\n \\nf\\n__contains__(): Uses the hexists command to check if an element exists in \\nthe hash map\\n \\nf\\n__getitem__(): Uses the hget command to get a value from the hash map\\n \\nf\\n__setitem__(): Uses the hset command to set a value in the hash map\\n \\nf\\n__delitem__(): Uses the hdel command to remove a value from the hash map\\n \\nf\\nkeys(): Uses the hkeys command to get all the keys in the hash map\\n \\nf\\nvalues(): Uses the hvals command to get all the values in the hash map\\n \\nf\\nitems(): Uses the hgetall command to get a dictionary containing all the keys \\nand values in the hash map\\n \\nf\\nclear(): Uses the delete command to remove the entire hash map from Redis\\nExtending collections.MutableMapping provides a number \\nof other dict compatible methods based on the previous methods, \\nsuch as update() and setdefault(), so we don\\'t have to \\nimplement them ourselves.\\nThe initialization used for the RedisHashFreqDist is actually implemented here, and \\nrequires a Redis connection and a name for the hash map. The connection and name \\nare both stored internally to use with all the subsequent commands. As mentioned before, \\nwhitespace is replaced by underscore in the name and all keys, for compatibility with the \\nRedis network protocol.\\nimport collections, re\\nwhite = r\\'[\\\\s&]+\\'\\ndef encode_key(key):\\nDistributed Processing and Handling Large Datasets\\n214\\n  return re.sub(white, \\'_\\', key.strip())\\nclass RedisHashMap(collections.MutableMapping):\\n  def __init__(self, r, name):\\n    self._r = r\\n    self._name = encode_key(name)\\n  def __iter__(self):\\n    return iter(self.items())\\n  def __len__(self):\\n    return self._r.hlen(self._name)\\n  def __contains__(self, key):\\n    return self._r.hexists(self._name, encode_key(key))\\n  def __getitem__(self, key):\\n    return self._r.hget(self._name, encode_key(key))\\n  \\n  def __setitem__(self, key, val):\\n    self._r.hset(self._name, encode_key(key), val)\\n  \\n  def __delitem__(self, key):\\n    self._r.hdel(self._name, encode_key(key))\\n  \\n  def keys(self):\\n    return self._r.hkeys(self._name)\\n  \\n  def values(self):\\n    return self._r.hvals(self._name)\\n  \\n  def items(self):\\n    return self._r.hgetall(self._name).items()\\n  \\n  def get(self, key, default=0):\\n    return self[key] or default\\n  \\n  def iteritems(self):\\n    return iter(self)\\n  \\n  def clear(self):\\n    self._r.delete(self._name)\\nThere\\'s more...\\nThe RedisHashMap can be used by itself as a persistent key-value dictionary. However, while \\nthe hash map can support a large number of keys and arbitrary string values, its storage \\nstructure is more optimal for integer values and smaller numbers of keys. However, don\\'t let \\nthat stop you from taking full advantage of Redis. It\\'s very fast (for a network server) and does \\nits best to eficiently encode whatever data you throw at it.\\nChapter 8\\n215\\nWhile Redis is quite fast for a network database, it will be \\nsigniicantly slower than the in-memory FreqDist. There\\'s no way \\naround this, but while you sacriice speed, you gain persistence and \\nthe ability to do concurrent processing.\\nSee also\\nIn the next recipe, we\\'ll create a conditional frequency distribution based on the Redis \\nfrequency distribution created here.\\nStoring a conditional frequency distribution \\nin Redis\\nThe nltk.probability.ConditionalFreqDist class is a container for FreqDist \\ninstances, with one FreqDist per condition. It is used to count frequencies that are \\ndependent on another condition, such as another word or a class label. We used this class \\nin the Calculating high information words recipe in Chapter 7, Text Classiication. Here, we\\'ll \\ncreate an API-compatible class on top of Redis using the RedisHashFreqDist from the \\nprevious recipe.\\nGetting ready\\nAs in the previous recipe, you\\'ll need to have Redis and redis-py installed with an instance \\nof redis-server running.\\nHow to do it...\\nWe deine a RedisConditionalHashFreqDist class in redisprob.py that extends \\nnltk.probability.ConditionalFreqDist and overrides the __contains__() \\nand __getitem__() methods. We then override __getitem__() so we can create an \\ninstance of RedisHashFreqDist instead of a FreqDist, and override __contains__() \\nso we can call encode_key() from the rediscollections module before checking if the \\nRedisHashFreqDist exists.\\nfrom nltk.probability import ConditionalFreqDist\\nfrom rediscollections import encode_key\\nclass RedisConditionalHashFreqDist(ConditionalFreqDist):\\n  def __init__(self, r, name, cond_samples=None):\\n    self._r = r\\n    self._name = name\\n    ConditionalFreqDist.__init__(self, cond_samples)\\nDistributed Processing and Handling Large Datasets\\n216\\n    # initialize self._fdists for all matching keys\\n    for key in self._r.keys(encode_key(\\'%s:*\\' % name)):\\n      condition = key.split(\\':\\')[1]\\n      self[condition] # calls self.__getitem__(condition)\\n  \\n  def __contains__(self, condition):\\n    return encode_key(condition) in self._fdists\\n  \\n  def __getitem__(self, condition):\\n    if condition not in self._fdists:\\n      key = \\'%s:%s\\' % (self._name, condition)\\n      self._fdists[condition] = RedisHashFreqDist(self._r, key)\\n  \\n    return self._fdists[condition]\\n  \\n  def clear(self):\\n    for fdist in self._fdists.values():\\n      fdist.clear()\\nAn instance of this class can be created by passing in a Redis connection and a base name. \\nAfter that, it works just like a ConditionalFreqDist.\\n>>> from redis import Redis\\n>>> from redisprob import RedisConditionalHashFreqDist\\n>>> r = Redis()\\n>>> rchfd = RedisConditionalHashFreqDist(r, \\'condhash\\')\\n>>> rchfd.N()\\n0\\n>>> rchfd.conditions()\\n[]\\n>>> rchfd[\\'cond1\\'].inc(\\'foo\\')\\n>>> rchfd.N()\\n1\\n>>> rchfd[\\'cond1\\'][\\'foo\\']\\n1\\n>>> rchfd.conditions()\\n[\\'cond1\\']\\n>>> rchfd.clear()\\nChapter 8\\n217\\nHow it works...\\nThe RedisConditionalHashFreqDist uses name preixes to \\nreference RedisHashFreqDist instances. The name passed in to the \\nRedisConditionalHashFreqDist is a base name that is combined with each condition to \\ncreate a unique name for each RedisHashFreqDist. For example, if the base name of the \\nRedisConditionalHashFreqDist is \\'condhash\\', and the condition is \\'cond1\\', then \\nthe inal name for the RedisHashFreqDist is \\'condhash:cond1\\'. This naming pattern is \\nused at initialization to ind all the existing hash maps using the keys command. By searching \\nfor all keys matching \\'condhash:*\\', we can identify all the existing conditions and create an \\ninstance of RedisHashFreqDist for each.\\nCombining strings with colons is a common naming convention \\nfor Redis keys as a way to deine namespaces. In our case, \\neach RedisConditionalHashFreqDist instance deines a \\nsingle namespace of hash maps.\\nThe ConditionalFreqDist class stores an internal dictionary at self._fdists that is \\na mapping of condition to FreqDist. The RedisConditionalHashFreqDist class \\nstill uses self._fdists, but the values are instances of RedisHashFreqDist instead of \\nFreqDist. self._fdists is created when we call ConditionalFreqDist.__init__(), \\nand values are initialized as necessary in the __getitem__() method.\\nThere\\'s more...\\nRedisConditionalHashFreqDist also deines a clear() method. This is a helper \\nmethod that calls clear() on all the internal RedisHashFreqDist instances. The \\nclear() method is not deined in ConditionalFreqDist.\\nSee also\\nThe previous recipe covers the RedisHashFreqDist in detail. Also see the Calculating \\nhigh information words recipe in Chapter 7, Text Classiication, for example usage of a \\nConditionalFreqDist.\\nDistributed Processing and Handling Large Datasets\\n218\\nStoring an ordered dictionary in Redis\\nAn ordered dictionary is like a normal dict, but the keys are ordered by an ordering \\nfunction. In the case of Redis, it supports ordered dictionaries whose keys are strings and \\nwhose values are loating point scores. This structure can come in handy for cases such as \\ncalculating information gain (covered in the Calculating high information words recipe in \\nChapter 7, Text Classiication) when you want to store all the words and scores for later use.\\nGetting ready\\nAgain, you\\'ll need Redis and redis-py installed, with an instance of redis-server \\nrunning.\\nHow to do it...\\nThe RedisOrderedDict class in rediscollections.py extends collections.\\nMutableMapping to get a number of dict compatible methods for free. Then it implements \\nall the key methods that require Redis ordered set (also known as Zset) commands.\\nclass RedisOrderedDict(collections.MutableMapping):\\n  def __init__(self, r, name):\\n    self._r = r\\n    self._name = encode_key(name)\\n  \\n  def __iter__(self):\\n    return iter(self.items())\\n  \\n  def __len__(self):\\n    return self._r.zcard(self._name)\\n  \\n  def __getitem__(self, key):\\n    val = self._r.zscore(self._name, encode_key(key))\\n  \\n    if val is None:\\n      raise KeyError\\n    else:\\n      return val\\n  \\n  def __setitem__(self, key, score):\\n    self._r.zadd(self._name, encode_key(key), score)\\n  \\n  def __delitem__(self, key):by brain feels dead\\n    self._r.zrem(self._name, encode_key(key))\\n  \\n  def keys(self, start=0, end=-1):\\n    # we use zrevrange to get keys sorted by high value instead of by \\nlowest\\nChapter 8\\n219\\n    return self._r.zrevrange(self._name, start, end)\\n  \\n  def values(self, start=0, end=-1):\\n    return [v for (k, v) in self.items(start=start, end=end)]\\n  \\n  def items(self, start=0, end=-1):\\n    return self._r.zrevrange(self._name, start, end, withscores=True)\\n  \\n  def get(self, key, default=0):\\n    return self[key] or default\\n  \\n  def iteritems(self):\\n    return iter(self)\\n  \\n  def clear(self):\\n    self._r.delete(self._name)\\nYou can create an instance of RedisOrderedDict by passing in a Redis connection and a \\nunique name.\\n>>> from redis import Redis\\n>>> from rediscollections import RedisOrderedDict\\n>>> r = Redis()\\n>>> rod = RedisOrderedDict(r, \\'test.txt\\')\\n>>> rod.get(\\'bar\\')\\n>>> len(rod)\\n0\\n>>> rod[\\'bar\\'] = 5.2\\n>>> rod[\\'bar\\']\\n5.2000000000000002\\n>>> len(rod)\\n1\\n>>> rod.items()\\n[(\\'bar\\', 5.2000000000000002)]\\n>>> rod.clear()\\nHow it works...\\nMuch of the code may look similar to the RedisHashMap, which is to be expected since \\nthey both extend collections.MutableMapping. The main difference here is that \\nRedisOrderedSet orders keys by loating point values, and so is not suited for arbitrary \\nkey-value storage like the RedisHashMap. Here\\'s an outline explaining each key method \\nand how it works with Redis:\\n \\nf\\n__len__(): Uses the zcard command to get the number of elements in the \\nordered set.\\nDistributed Processing and Handling Large Datasets\\n220\\n \\nf\\n__getitem__(): Uses the zscore command to get the score of a key, and returns \\n0 if the key does not exist.\\n \\nf\\n__setitem__(): Uses the zadd command to add a key to the ordered set with the \\ngiven score, or updates the score if the key already exists.\\n \\nf\\n__delitem__(): Uses the zrem command to remove a key from the ordered set.\\n \\nf\\nkeys(): Uses the zrevrange command to get all the keys in the ordered set, sorted \\nby highest score. It takes two optional keyword arguments start and end to more \\neficiently get a slice of the ordered keys.\\n \\nf\\nvalues(): Extracts all the scores from the items() method.\\n \\nf\\nitems(): Uses the zrevrange command to get the scores of each key in order to \\nreturn a list of 2-tuples ordered by highest score. Like keys(), it takes start and \\nend keyword arguments to eficiently get a slice.\\n \\nf\\nclear(): Uses the delete command to remove the entire ordered set from Redis.\\nThe default ordering of items in a Redis ordered set is low-to-high, so that \\nthe key with the lowest score comes irst. This is the same as Python\\'s default \\nlist ordering when you call sort() or sorted(), but it\\'s not what we want \\nwhen it comes to scoring. For storing scores, we expect items to be sorted \\nfrom high-to-low, which is why keys() and items() use zrevrange \\ninstead of zrange.\\nThere\\'s more...\\nAs mentioned previously, the keys() and items() methods take optional start and end \\nkeyword arguments to get a slice of the results. This makes the RedisOrderedDict optimal \\nfor storing scores, then getting the top N keys. Here\\'s a simple example where we assign three \\nword scores, then get the top two:\\n>>> from redis import Redis\\n>>> from rediscollections import RedisOrderedDict\\n>>> r = Redis()\\n>>> rod = RedisOrderedDict(r, \\'scores\\')\\n>>> rod[\\'best\\'] = 10\\n>>> rod[\\'worst\\'] = 0.1\\n>>> rod[\\'middle\\'] = 5\\n>>> rod.keys()\\n[\\'best\\', \\'middle\\', \\'worst\\']\\n>>> rod.keys(start=0, end=1)\\n[\\'best\\', \\'middle\\']\\n>>> rod.clear()\\nChapter 8\\n221\\nSee also\\nCalculating high information words recipe in Chapter 7, Text Classiication, describes \\nhow to calculate information gain, which is a good case for storing word scores in a \\nRedisOrderedDict. The Storing a frequency distribution in Redis recipe introduces \\nRedis and the RedisHashMap.\\nDistributed word scoring with Redis \\nand execnet\\nWe can use Redis and execnet together to do distributed word scoring. In the Calculating \\nhigh information words recipe in Chapter 7, Text Classiication, we calculated the \\ninformation gain of each word in the movie_reviews corpus using a FreqDist and \\nConditionalFreqDist. Now that we have Redis, we can do the same thing using a \\nRedisHashFreqDist and a RedisConditionalHashFreqDist, then store the scores in \\na RedisOrderedDict. We can use execnet to distribute the counting in order to get better \\nperformance out of Redis.\\nGetting ready\\nRedis, redis-py, and execnet must be installed, and an instance of redis-server must \\nbe running on localhost.\\nHow to do it...\\nWe start by getting a list of (label, words) tuples for each label in the movie_\\nreviews corpus (which only has pos and neg labels). Then we get the word_scores \\nusing score_words() from the dist_featx module. word_scores is an instance of \\nRedisOrderedDict, and we can see that the total number of words is 39,764. Using the \\nkeys() method, we can then get the top 1000 words, and inspect the top ive just to see \\nwhat they are. Once we have all we want from word_scores, we can delete the keys in \\nRedis as we no longer need the data.\\n>>> from dist_featx import score_words\\n>>> from nltk.corpus import movie_reviews\\n>>> labels = movie_reviews.categories()\\n>>> labelled_words = [(l, movie_reviews.words(categories=[l])) for l \\nin labels]\\n>>> word_scores = score_words(labelled_words)\\n>>> len(word_scores)\\n39764\\n>>> topn_words = word_scores.keys(end=1000)\\nDistributed Processing and Handling Large Datasets\\n222\\n>>> topn_words[0:5]\\n[\\'_\\', \\'bad\\', \\'?\\', \\'movie\\', \\'t\\']\\n>>> from redis import Redis\\n>>> r = Redis()\\n>>> [r.delete(key) for key in [\\'word_fd\\', \\'label_word_fd:neg\\', \\'label_\\nword_fd:pos\\', \\'word_scores\\']]\\n[True, True, True, True]\\nThe score_words() function in dist_featx can take a while to complete, so expect to \\nwait a couple of minutes. The overhead of using execnet and Redis means it will take \\nsigniicantly longer than a non-distributed in-memory version of the function.\\nHow it works...\\nThe dist_featx.py module contains the score_words() function, which does \\nthe following:\\n1. Opens gateways and channels, sending initialization data to each.\\n2. Sends each (label, words) tuple over a channel for counting.\\n3. Sends a done message to each channel, waits for a done reply back, \\nthen closes the channels and gateways.\\n4. Calculates the score of each word based on the counts and stores in a \\nRedisOrderedDict.\\nIn our case of counting words in the movie_reviews corpus, calling score_words() opens \\ntwo gateways and channels, one for counting the pos words, and the other for counting the \\nneg words. The communication is as follows:\\nChapter 8\\n223\\nOnce the counting is inished, we can score all the words and store the results. The code \\nitself is as follows:\\nimport itertools, execnet, remote_word_count\\nfrom nltk.metrics import BigramAssocMeasures\\nfrom redis import Redis\\nfrom redisprob import RedisHashFreqDist, RedisConditionalHashFreqDist\\nfrom rediscollections import RedisOrderedDict\\ndef score_words(labelled_words, score_fn=BigramAssocMeasures.chi_sq, \\nhost=\\'localhost\\', specs=[(\\'popen\\', 2)]):\\n  gateways = []\\n  channels = []\\n  \\n  for spec, count in specs:\\n    for i in range(count):\\n      gw = execnet.makegateway(spec)\\n      gateways.append(gw)\\n      channel = gw.remote_exec(remote_word_count)\\n      channel.send((host, \\'word_fd\\', \\'label_word_fd\\'))\\n      channels.append(channel)\\n  \\n  cyc = itertools.cycle(channels)\\n  \\n  for label, words in labelled_words:\\n    channel = cyc.next()\\n    channel.send((label, list(words)))\\n  \\n  for channel in channels:\\n    channel.send(\\'done\\')\\n    assert \\'done\\' == channel.receive()\\n    channel.waitclose(5)\\n  \\n  for gateway in gateways:\\n    gateway.exit()\\n  \\n  r = Redis(host)\\n  fd = RedisHashFreqDist(r, \\'word_fd\\')\\n  cfd = RedisConditionalHashFreqDist(r, \\'label_word_fd\\')\\n  word_scores = RedisOrderedDict(r, \\'word_scores\\')\\n  n_xx = cfd.N()\\n  \\n  for label in cfd.conditions():\\n    n_xi = cfd[label].N()\\n  \\n    for word, n_ii in cfd[label].iteritems():\\n      n_ix = fd[word]\\n  \\n      if n_ii and n_ix and n_xi and n_xx:\\n        score = score_fn(n_ii, (n_ix, n_xi), n_xx)\\n        word_scores[word] = score\\n  \\n  return word_scores\\nNote that this scoring method will only be accurate when there are two labels. \\nIf there are more than two labels, then word scores for each label should be \\nstored in separate RedisOrderedDict instances, one per label.\\nDistributed Processing and Handling Large Datasets\\n224\\nThe remote_word_count.py module looks as follows:\\nfrom redis import Redis\\nfrom redisprob import RedisHashFreqDist, RedisConditionalHashFreqDist\\n  \\nif __name__ == \\'__channelexec__\\':\\n  host, fd_name, cfd_name = channel.receive()\\n  r = Redis(host)\\n  fd = RedisHashFreqDist(r, fd_name)\\n  cfd = RedisConditionalHashFreqDist(r, cfd_name)\\n  \\n  for data in channel:\\n    if data == \\'done\\':\\n      channel.send(\\'done\\')\\n      break\\n  \\n    label, words = data\\n  \\n    for word in words:\\n      fd.inc(word)\\n      cfd[label].inc(word)\\nYou\\'ll notice this is not a pure module as it requires being able to import both \\nredis and redisprob. The reason is that instances of RedisHashFreqDist and \\nRedisConditionalHashFreqDist cannot be pickled and sent over the channel. Instead, \\nwe send the host name and key names over the channel so we can create the instances in \\nthe remote module. Once we have the instances, there are two kinds of data we can receive \\nover the channel:\\n1. A done message, which signals that there is no more data coming in over the \\nchannel. We reply back with another done message, then exit the loop to close \\nthe channel.\\n2. A 2-tuple of (label, words), which we then iterate over to increment counts in \\nboth the RedisHashFreqDist and RedisConditionalHashFreqDist.\\nThere\\'s more...\\nIn this particular case, it would be faster to compute the scores without using Redis or \\nexecnet. However, by using Redis, we can store the scores persistently for later examination \\nand usage. Being able to inspect all the word counts and scores manually is a great way to learn \\nabout your data. We can also tweak feature extraction without having to re-compute the scores. \\nFor example, you could use featx.bag_of_words_in_set() (found in Chapter 7, Text \\nClassiication) with the top N words from the RedisOrderedDict, where N could be 1,000, \\n2,000, or whatever number you want. If our data size is much greater, the beneits of execnet \\nwill be much more apparent. Horizontal scalability using execnet or some other method to \\ndistribute computations across many nodes becomes more valuable, as the size of the data you \\nneed to process increases. \\nChapter 8\\n225\\nSee also\\nThe Calculating high information words recipe in Chapter 7, Text Classiication introduces \\ninformation gain scoring of words for feature extraction and classiication. The irst three \\nrecipes of this chapter show how to use execnet, while the next three recipes describe \\nRedisHashFreqDist, RedisConditionalHashFreqDist, and RedisOrderedDict \\nrespectively.\\n9\\nParsing Speciic Data\\nIn this chapter, we will cover:\\n \\nf\\nParsing dates and times with Dateutil\\n \\nf\\nTime zone lookup and conversion\\n \\nf\\nTagging temporal expressions with Timex\\n \\nf\\nExtracting URLs from HTML with lxml\\n \\nf\\nCleaning and stripping HTML\\n \\nf\\nConverting HTML entities with BeautifulSoup\\n \\nf\\nDetecting and converting character encodings\\nIntroduction\\nThis chapter covers parsing speciic kinds of data, focusing primarily on dates, times, and \\nHTML. Luckily, there are a number of useful libraries for accomplishing this, so we don\\'t have \\nto delve into tricky and overly complicated regular expressions. These libraries can be great \\ncomplements to the NLTK:\\n \\nf\\ndateutil: Provides date/time parsing and time zone conversion\\n \\nf\\ntimex: Can identify time words in text\\n \\nf\\nlxml and BeautifulSoup: Can parse, clean, and convert HTML\\n \\nf\\nchardet: Detects the character encoding of text\\nThe libraries can be useful for pre-processing text before passing it to an NLTK object, or  \\npost-processing text that has been processed and extracted using NLTK. Here\\'s an example \\nthat ties many of these tools together.\\nParsing Speciic Data\\n228\\nLet\\'s say you need to parse a blog article about a restaurant. You can use lxml or \\nBeautifulSoup to extract the article text, outbound links, and the date and time when the \\narticle was written. The date and time can then be parsed to a Python datetime object with \\ndateutil. Once you have the article text, you can use chardet to ensure it\\'s UTF-8 before \\ncleaning out the HTML and running it through NLTK-based part-of-speech tagging, chunk \\nextraction, and/or text classiication, to create additional metadata about the article. If there\\'s \\nan event happening at the restaurant, you may be able to discover that by looking at the \\ntime words identiied by timex. The point of this example is that real-world text processing \\noften requires more than just NLTK-based natural language processing, and the functionality \\ncovered in this chapter can help with those additional requirements.\\nParsing dates and times with Dateutil\\nIf you need to parse dates and times in Python, there is no better library than dateutil. The \\nparser module can parse datetime strings in many more formats than can be shown here, \\nwhile the tz module provides everything you need for looking up time zones. Combined, these \\nmodules make it quite easy to parse strings into time zone aware datetime objects.\\nGetting ready\\nYou can install dateutil using pip or easy_install, that is sudo pip install \\ndateutil or sudo easy_install dateutil. Complete documentation can be found at \\nhttp://labix.org/python-dateutil.\\nHow to do it...\\nLet\\'s dive into a few parsing examples:\\n>>> from dateutil import parser\\n>>> parser.parse(\\'Thu Sep 25 10:36:28 2010\\')\\ndatetime.datetime(2010, 9, 25, 10, 36, 28)\\n>>> parser.parse(\\'Thursday, 25. September 2010 10:36AM\\')\\ndatetime.datetime(2010, 9, 25, 10, 36)\\n>>> parser.parse(\\'9/25/2010 10:36:28\\')\\ndatetime.datetime(2010, 9, 25, 10, 36, 28)\\n>>> parser.parse(\\'9/25/2010\\')\\ndatetime.datetime(2010, 9, 25, 0, 0)\\n>>> parser.parse(\\'2010-09-25T10:36:28Z\\')\\ndatetime.datetime(2010, 9, 25, 10, 36, 28, tzinfo=tzutc())\\nAs you can see, all it takes is importing the parser module and calling the parse() function \\nwith a datetime string. The parser will do its best to return a sensible datetime object, but \\nif it cannot parse the string, it will raise a ValueError.\\nChapter 9\\n229\\nHow it works...\\nThe parser does not use regular expressions. Instead, it looks for recognizable tokens and \\ndoes its best to guess what those tokens refer to. The order of these tokens matters, for \\nexample, some cultures use a date format that looks like Month/Day/Year (the default order) \\nwhile others use a Day/Month/Year format. To deal with this, the parse() function takes an \\noptional keyword argument dayfirst, which defaults to False. If you set it to True, it can \\ncorrectly parse dates in the latter format.\\n>>> parser.parse(\\'25/9/2010\\', dayfirst=True)\\ndatetime.datetime(2010, 9, 25, 0, 0)\\nAnother ordering issue can occur with two-digit years. For example, \\'10-9-25\\' is ambiguous. \\nSince dateutil defaults to the Month-Day-Year format, \\'10-9-25\\' is parsed to the year \\n2025. But if you pass yearfirst=True into parse(), it will be parsed to the year 2010.\\n>>> parser.parse(\\'10-9-25\\')\\ndatetime.datetime(2025, 10, 9, 0, 0)\\n>>> parser.parse(\\'10-9-25\\', yearfirst=True)\\ndatetime.datetime(2010, 9, 25, 0, 0)\\nThere\\'s more...\\nThe dateutil parser can also do fuzzy parsing, which allows it to ignore extraneous \\ncharacters in a datetime string. With the default value of False, parse() will raise a \\nValueError when it encounters unknown tokens. But if fuzzy=True, then a datetime \\nobject can usually be returned.\\n>>> try:\\n...    parser.parse(\\'9/25/2010 at about 10:36AM\\')\\n... except ValueError:\\n...    \\'cannot parse\\'\\n\\'cannot parse\\'\\n>>> parser.parse(\\'9/25/2010 at about 10:36AM\\', fuzzy=True)\\ndatetime.datetime(2010, 9, 25, 10, 36)\\nSee also\\nIn the next recipe, we\\'ll use the tz module from dateutil to do time zone lookup and \\nconversion.\\nParsing Speciic Data\\n230\\nTime zone lookup and conversion\\nMost datetime objects returned from the dateutil parser are naive, meaning they don\\'t \\nhave an explicit tzinfo, which speciies the time zone and UTC offset. In the previous recipe, \\nonly one of the examples had a tzinfo, and that\\'s because it\\'s in the standard ISO format \\nfor UTC date and time strings. UTC is the coordinated universal time, and is the same as \\nGMT. ISO is the International Standards Organization, which among other things, speciies \\nstandard date and time formatting.\\nPython datetime objects can either be naive or aware. If a datetime object has a tzinfo, \\nthen it is aware. Otherwise the datetime is naive. To make a naive datetime object time \\nzone aware, you must give it an explicit tzinfo. However, the Python datetime library \\nonly deines an abstract base class for tzinfo, and leaves it up to the others to actually \\nimplement tzinfo creation. This is where the tz module of dateutil comes in—it provides \\neverything you need to lookup time zones from your OS time zone data.\\nGetting ready\\ndateutil should be installed using pip or easy_install. You should also make sure \\nyour operating system has time zone data. On Linux, this is usually found in /usr/share/\\nzoneinfo, and the Ubuntu package is called tzdata. If you have a number of iles and \\ndirectories in /usr/share/zoneinfo, such as America/, Europe/, and so on, then you \\nshould be ready to proceed. The following examples show directory paths for Ubuntu Linux.\\nHow to do it...\\nLet\\'s start by getting a UTC tzinfo object. This can be done by calling tz.tzutc(), \\nand you can check that the offset is 0 by calling the utcoffset() method with a UTC \\ndatetime object.\\n>>> from dateutil import tz\\n>>> tz.tzutc()\\ntzutc()\\n>>> import datetime\\n>>> tz.tzutc().utcoffset(datetime.datetime.utcnow())\\ndatetime.timedelta(0)\\nTo get tzinfo objects for other time zones, you can pass in a time zone ile path to the \\ngettz() function.\\n>>> tz.gettz(\\'US/Pacific\\')\\ntzfile(\\'/usr/share/zoneinfo/US/Pacific\\')\\n>>> tz.gettz(\\'US/Pacific\\').utcoffset(datetime.datetime.utcnow())\\ndatetime.timedelta(-1, 61200)\\nChapter 9\\n231\\n>>> tz.gettz(\\'Europe/Paris\\')\\ntzfile(\\'/usr/share/zoneinfo/Europe/Paris\\')\\n>>> tz.gettz(\\'Europe/Paris\\').utcoffset(datetime.datetime.utcnow())\\ndatetime.timedelta(0, 7200)\\nYou can see the UTC offsets are timedelta objects, where the irst number is days, and the \\nsecond number is seconds.\\nIf you\\'re storing datetimes in a database, it\\'s a good idea to store them \\nall in UTC to eliminate any time zone ambiguity. Even if the database can \\nrecognize time zones, it\\'s still a good practice.\\nTo convert a non-UTC datetime object to UTC, it must be made time zone aware. If you try \\nto convert a naive datetime to UTC, you\\'ll get a ValueError exception. To make a naive \\ndatetime time zone aware, you simply call the replace() method with the correct tzinfo. \\nOnce a datetime object has a tzinfo, then UTC conversion can be performed by calling the \\nastimezone() method with tz.tzutc().\\n>>> pst = tz.gettz(\\'US/Pacific\\')\\n>>> dt = datetime.datetime(2010, 9, 25, 10, 36)\\n>>> dt.tzinfo\\n>>> dt.astimezone(tz.tzutc())\\nTraceback (most recent call last):\\n  File \"/usr/lib/python2.6/doctest.py\", line 1248, in __run\\n  compileflags, 1) in test.globs\\n  File \"<doctest __main__[22]>\", line 1, in <module>\\n  dt.astimezone(tz.tzutc())\\nValueError: astimezone() cannot be applied to a naive datetime\\n>>> dt.replace(tzinfo=pst)\\ndatetime.datetime(2010, 9, 25, 10, 36, tzinfo=tzfile(\\'/usr/share/\\nzoneinfo/US/Pacific\\'))\\n>>> dt.replace(tzinfo=pst).astimezone(tz.tzutc())\\ndatetime.datetime(2010, 9, 25, 17, 36, tzinfo=tzutc())\\nHow it works...\\nThe tzutc and tzfile objects are both subclasses of tzinfo. As such, they know the \\ncorrect UTC offset for time zone conversion (which is 0 for tzutc). A tzfile object knows \\nhow to read your operating system\\'s zoneinfo iles to get the necessary offset data. The \\nreplace() method of a datetime object does what its name implies—it replaces attributes. \\nOnce a datetime has a tzinfo, the astimezone() method will be able to convert the time \\nusing the UTC offsets, and then replace the current tzinfo with the new tzinfo.\\nParsing Speciic Data\\n232\\nNote that both replace() and astimezone() return \\nnew datetime objects. They do not modify the current \\nobject.\\nThere\\'s more...\\nYou can pass a tzinfos keyword argument into the dateutil parser to detect otherwise \\nunrecognized time zones.\\n>>> parser.parse(\\'Wednesday, Aug 4, 2010 at 6:30 p.m. (CDT)\\', \\nfuzzy=True)\\ndatetime.datetime(2010, 8, 4, 18, 30)\\n>>> tzinfos = {\\'CDT\\': tz.gettz(\\'US/Central\\')}\\n>>> parser.parse(\\'Wednesday, Aug 4, 2010 at 6:30 p.m. (CDT)\\', \\nfuzzy=True, tzinfos=tzinfos)\\ndatetime.datetime(2010, 8, 4, 18, 30, tzinfo=tzfile(\\'/usr/share/\\nzoneinfo/US/Central\\'))\\nIn the irst instance, we get a naive datetime since the time zone is not recognized. However, \\nwhen we pass in the tzinfos mapping, we get a time zone aware datetime.\\nLocal time zone\\nIf you want to lookup your local time zone, you can call tz.tzlocal(), which will use \\nwhatever your operating system thinks is the local time zone. In Ubuntu Linux, this is usually \\nspeciied in the /etc/timezone ile.\\nCustom offsets\\nYou can create your own tzinfo object with a custom UTC offset using the tzoffset object. \\nA custom offset of one hour can be created as follows:\\n>>> tz.tzoffset(\\'custom\\', 3600)\\ntzoffset(\\'custom\\', 3600)\\nYou must provide a name as the irst argument, and the offset time in seconds as the  \\nsecond argument.\\nSee also\\nThe previous recipe covers parsing datetime strings with dateutil.parser.\\nChapter 9\\n233\\nTagging temporal expressions with Timex\\nThe NLTK project has a little known contrib repository that contains, among other things, \\na module called timex.py that can tag temporal expressions. A temporal expression is \\njust one or more time words, such as \"this week\", or \"next month\". These are ambiguous \\nexpressions that are relative to some other point in time, like when the text was written. The \\ntimex module provides a way to annotate text so these expressions can be extracted for \\nfurther analysis. More on TIMEX can be found at http://timex2.mitre.org/.\\nGetting ready\\nThe timex.py module is part of the nltk_contrib package, which is separate from the \\ncurrent version of NLTK. This means you need to install it yourself, or use the timex.py \\nmodule that is included with the book\\'s code download. You can also download timex.\\npy directly from http://code.google.com/p/nltk/source/browse/trunk/nltk_\\ncontrib/nltk_contrib/timex.py.\\nIf you want to install the entire nltk_contrib package, you can check out the source at \\nhttp://nltk.googlecode.com/svn/trunk/ and do sudo python setup.py install \\nfrom within the nltk_contrib folder. If you do this, you\\'ll need to do from nltk_contrib \\nimport timex instead of just import timex as done in the following How to do it... section.\\nFor this recipe, you have to download the timex.py module into the same folder as the rest \\nof the code, so that import timex does not cause an ImportError.\\nYou\\'ll also need to get the egenix-mx-base package installed. This is a C extension library \\nfor Python, so if you have all the correct Python development headers installed, you should \\nbe able to do sudo pip install egenix-mx-base or sudo easy_install egenix-mx-\\nbase. If you\\'re running Ubuntu Linux, you can instead do sudo apt-get install python-\\negenix-mxdatetime. If none of those work, you can go to http://www.egenix.com/\\nproducts/python/mxBase/ to download the package and ind installation instructions.\\nHow to do it...\\nUsing timex is very simple: pass a string into the timex.tag() function and get back \\nan annotated string. The annotations will be XML TIMEX tags surrounding each temporal \\nexpression.\\n>>> import timex\\n>>> timex.tag(\"Let\\'s go sometime this week\")\\n\"Let\\'s go sometime <TIMEX2>this week</TIMEX2>\"\\n>>> timex.tag(\"Tomorrow I\\'m going to the park.\")\\n\"<TIMEX2>Tomorrow</TIMEX2> I\\'m going to the park.\"\\nParsing Speciic Data\\n234\\nHow it works...\\nThe implementation of timex.py is essentially over 300 lines of conditional \\nregular expression matches. When one of the known expressions match, it creates a \\nRelativeDateTime object (from the mx.DateTime module). This RelativeDateTime \\nis then converted back to a string with surrounding TIMEX tags and replaces the original \\nmatched string in the text.\\nThere\\'s more...\\ntimex is smart enough not to tag expressions that have already been tagged, so it\\'s ok to \\npass TIMEX tagged text into the tag() function.\\n>>> timex.tag(\"Let\\'s go sometime <TIMEX2>this week</TIMEX2>\")\\n\"Let\\'s go sometime <TIMEX2>this week</TIMEX2>\"\\nSee also\\nIn the next recipe, we\\'ll be extracting URLs from HTML, but the same modules and techniques \\ncan be used to extract the TIMEX tagged expressions for further processing.\\nExtracting URLs from HTML with lxml\\nA common task when parsing HTML is extracting links. This is one of the core functions of \\nevery general web crawler. There are a number of Python libraries for parsing HTML, and lxml \\nis one of the best. As you\\'ll see, it comes with some great helper functions geared speciically \\ntowards link extraction.\\nGetting ready\\nlxml is a Python binding for the C libraries libxml2 and libxslt. This makes it a very fast \\nXML and HTML parsing library, while still being pythonic. However, that also means you need \\nto install the C libraries for it to work. Installation instructions are at http://codespeak.\\nnet/lxml/installation.html. However, if you\\'re running Ubuntu Linux, installation is as \\neasy as sudo apt-get install python-lxml.\\nChapter 9\\n235\\nHow to do it...\\nlxml comes with an html module designed speciically for parsing HTML. Using the \\nfromstring() function, we can parse an HTML string, then get a list of all the links. The \\niterlinks() method generates four-tuples of the form (element, attr, link, pos):\\n \\nf\\nelement: This is the parsed node of the anchor tag from which the link is \\nextracted. If you\\'re just interested in the link, you can ignore this.\\n \\nf\\nattr: This is the attribute the link came from, which is usually href.\\n \\nf\\nlink: This is the actual URL extracted from the anchor tag.\\n \\nf\\npos: This is the numeric index of the anchor tag in the document. The irst tag has a \\npos of 0, the second has a pos of 1, and so on.\\nFollowing is some code to demonstrate:\\n>>> from lxml import html\\n>>> doc = html.fromstring(\\'Hello <a href=\"/world\">world</a>\\')\\n>>> links = list(doc.iterlinks())\\n>>> len(links)\\n1\\n>>> (el, attr, link, pos) = links[0]\\n>>> attr\\n\\'href\\'\\n>>> link\\n\\'/world\\'\\n>>> pos\\n0\\nHow it works...\\nlxml parses the HTML into an ElementTree. This is a tree structure of parent nodes and \\nchild nodes, where each node represents an HTML tag, and contains all the corresponding \\nattributes of that tag. Once the tree is created, it can be iterated on to ind elements, such \\nas the a or anchor tag. The core tree handling code is in the lxml.etree module, while \\nthe lxml.html module contains only HTML-speciic functions for creating and iterating a \\ntree. For complete documentation, see the lxml tutorial: http://codespeak.net/lxml/\\ntutorial.html.\\nThere\\'s more...\\nYou\\'ll notice in the previous code that the link is relative, meaning it\\'s not an absolute URL. \\nWe can make it absolute by calling the make_links_absolute() method with a base URL \\nbefore extracting the links.\\n>>> doc.make_links_absolute(\\'http://hello\\')\\n>>> abslinks = list(doc.iterlinks())\\nParsing Speciic Data\\n236\\n>>> (el, attr, link, pos) = abslinks[0]\\n>>> link\\n\\'http://hello/world\\'\\nExtracting links directly\\nIf you don\\'t want to do anything other than extract links, you can call the iterlinks() \\nfunction with an HTML string.\\n>>> links = list(html.iterlinks(\\'Hello <a href=\"/world\">world</a>\\'))\\n>>> links[0][2]\\n\\'/world\\'\\nParsing HTML from URLs or iles\\nInstead of parsing an HTML string using the fromstring() function, you can call the \\nparse() function with a URL or ile name. For example, html.parse(\"http://my/url\") \\nor html.parse(\"/path/to/file\"). The result will be the same as if you loaded the URL \\nor ile into a string yourself, then called fromstring().\\nExtracting links with XPaths\\nInstead of using the iterlinks() method, you can also get links using the xpath() \\nmethod, which is a general way to extract whatever you want from HTML or XML parse trees.\\n>>> doc.xpath(\\'//a/@href\\')[0]\\n\\'http://hello/world\\'\\nFor more on XPath syntax, see http://www.w3schools.com/XPath/xpath_syntax.\\nasp.\\nSee also\\nIn the next recipe, we\\'ll cover cleaning and stripping HTML.\\nCleaning and stripping HTML\\nCleaning up text is one of the unfortunate but entirely necessary aspects of text processing. \\nWhen it comes to parsing HTML, you probably don\\'t want to deal with any embedded \\nJavaScript or CSS, and are only interested in the tags and text. Or you may want to remove  \\nthe HTML entirely, and process only the text. This recipe covers how to do both of these  \\npre-processing actions.\\nChapter 9\\n237\\nGetting ready\\nYou\\'ll need to install lxml. See the previous recipe or http://codespeak.net/lxml/\\ninstallation.html for installation instructions. You\\'ll also need NLTK installed for \\nstripping HTML.\\nHow to do it...\\nWe can use the clean_html() function in the lxml.html.clean module to remove \\nunnecessary HTML tags and embedded JavaScript from an HTML string.\\n>>> import lxml.html.clean\\n>>> lxml.html.clean.clean_html(\\'<html><head></head><body \\nonload=loadfunc()>my text</body></html>\\')\\n\\'<div><body>my text</body></div>\\'\\nThe result is much cleaner and easier to deal with. The full module path to the  \\nclean_html() function is used because there\\'s also has a clean_html() function \\nin the nltk.util module, but its purpose is different. The nltk.util.clean_html() \\nfunction removes all HTML tags when you just want the text.\\n>>> import nltk.util\\n>>> nltk.util.clean_html(\\'<div><body>my text</body></div>\\')\\n\\'my text\\'\\nHow it works...\\nThe lxml.html.clean_html() function parses the HTML string into a tree, then iterates \\nover and removes all nodes that should be removed. It also cleans nodes of unnecessary \\nattributes (such as embedded JavaScript) using regular expression matching and substitution.\\nThe nltk.util.clean_html() function performs a bunch of regular expression \\nsubstitutions to remove HTML tags. To be safe, it\\'s best to strip the HTML after cleaning it to \\nensure the regular expressions will match.\\nThere\\'s more...\\nThe lxml.html.clean module deines a default Cleaner class that\\'s used when you \\ncall clean_html(). You can customize the behavior of this class by creating your own \\ninstance and calling its clean_html() method. For more details on this class, see  \\nhttp://codespeak.net/lxml/lxmlhtml.html.\\nParsing Speciic Data\\n238\\nSee also\\nThe lxml.html module was introduced in the previous recipe for parsing HTML and \\nextracting links. In the next recipe, we\\'ll cover un-escaping HTML entities.\\nConverting HTML entities with \\nBeautifulSoup\\nHTML entities are strings such as &amp; or &lt;. These are encodings of normal ASCII \\ncharacters that have special uses in HTML. For example, &lt; is the entity for <. You can\\'t \\njust have < within HTML tags because it is the beginning character for an HTML tag, hence the \\nneed to escape it and deine the &lt; entity. The entity code for & is &amp; which, as we\\'ve \\njust seen, is the beginning character for an entity code. If you need to process the text within \\nan HTML document, then you\\'ll want to convert these entities back to their normal characters \\nso you can recognize them and handle them appropriately.\\nGetting ready\\nYou\\'ll need to install BeautifulSoup, which you should be able to do with sudo pip \\ninstall BeautifulSoup or sudo easy_install BeautifulSoup. You can read more \\nabout BeautifulSoup at http://www.crummy.com/software/BeautifulSoup/.\\nHow to do it...\\nBeautifulSoup is an HTML parser library that also contains an XML parser called \\nBeautifulStoneSoup. This is what we can use for entity conversion. It\\'s quite simple: \\ncreate an instance of BeautifulStoneSoup given a string containing HTML entities and \\nspecify the keyword argument convertEntities=\\'html\\'. Convert this instance to a string, \\nand you\\'ll get the ASCII representation of the HTML entities.\\n>>> from BeautifulSoup import BeautifulStoneSoup\\n>>> unicode(BeautifulStoneSoup(\\'&lt;\\', convertEntities=\\'html\\'))\\nu\\'<\\'\\n>>> unicode(BeautifulStoneSoup(\\'&amp;\\', convertEntities=\\'html\\'))\\nu\\'&\\'\\nIt\\'s ok to run the string through multiple times, as long as the ASCII characters are not by \\nthemselves. If your string is just a single ASCII character for an HTML entity, that character will \\nbe lost.\\n>>> unicode(BeautifulStoneSoup(\\'<\\', convertEntities=\\'html\\'))\\nu\\'\\'\\nChapter 9\\n239\\n>>> unicode(BeautifulStoneSoup(\\'< \\', convertEntities=\\'html\\'))\\nu\\'< \\'\\nTo make sure the character isn\\'t lost, all that\\'s required is to have another character in the \\nstring that is not part of an entity code.\\nHow it works...\\nTo convert the HTML entities, BeautifulStoneSoup looks for tokens that look like \\nan entity and replaces them with their corresponding value in the htmlentitydefs.\\nname2codepoint dictionary from the Python standard library. It can do this if the entity \\ntoken is within an HTML tag, or when it\\'s in a normal string.\\nThere\\'s more...\\nBeautifulSoup is an excellent HTML and XML parser in its own right, and can be a \\ngreat alternative to lxml. It\\'s particularly good at handling malformed HTML. You can read \\nmore about how to use it at http://www.crummy.com/software/BeautifulSoup/\\ndocumentation.html.\\nExtracting URLs with BeautifulSoup\\nHere\\'s an example of using BeautifulSoup to extract URLs, like we did in the Extracting \\nURLs from HTML with lxml recipe. You irst create the soup with an HTML string, call the \\nfindAll() method with \\'a\\' to get all anchor tags, and pull out the \\'href\\' attribute to get \\nthe URLs.\\n>>> from BeautifulSoup import BeautifulSoup\\n>>> soup = BeautifulSoup(\\'Hello <a href=\"/world\">world</a>\\')\\n>>> [a[\\'href\\'] for a in soup.findAll(\\'a\\')]\\n[u\\'/world\\']\\nSee also\\nIn the Extracting URLs from HTML with lxml recipe, we covered how to use lxml to extract \\nURLs from an HTML string, and we covered Cleaning and stripping HTML after that recipe.\\nParsing Speciic Data\\n240\\nDetecting and converting character \\nencodings\\nA common occurrence with text processing is inding text that has a non-standard character \\nencoding. Ideally, all text would be ASCII or UTF-8, but that\\'s just not the reality. In cases when \\nyou have non-ASCII or non-UTF-8 text and you don\\'t know what the character encoding is, you\\'ll \\nneed to detect it and convert the text to a standard encoding before further processing it.\\nGetting ready\\nYou\\'ll need to install the chardet module, using sudo pip install chardet or sudo \\neasy_install chardet. You can learn more about chardet at http://chardet.\\nfeedparser.org/.\\nHow to do it...\\nEncoding detection and conversion functions are provided in encoding.py. These are \\nsimple wrapper functions around the chardet module. To detect the encoding of a string, \\ncall encoding.detect(). You\\'ll get back a dict containing two attributes: confidence \\nand encoding. confidence is a probability of how conident chardet is that the value for \\nencoding is correct.\\n# -*- coding: utf-8 -*-\\nimport chardet\\ndef detect(s):\\n  try:\\n    return chardet.detect(s)\\n  except UnicodeDecodeError:\\n    return chardet.detect(s.encode(\\'utf-8\\'))\\n  def convert(s):\\n    encoding = detect(s)[\\'encoding\\']\\n    \\n    if encoding == \\'utf-8\\':\\n      return unicode(s)\\n    else:\\n      return unicode(s, encoding)\\nHere\\'s some example code using detect() to determine character encoding:\\n>>> import encoding\\n>>> encoding.detect(\\'ascii\\')\\n{\\'confidence\\': 1.0, \\'encoding\\': \\'ascii\\'}\\nChapter 9\\n241\\n>>> encoding.detect(u\\'abcdé\\')\\n{\\'confidence\\': 0.75249999999999995, \\'encoding\\': \\'utf-8\\'}\\n>>> encoding.detect(\\'\\\\222\\\\222\\\\223\\\\225\\')\\n{\\'confidence\\': 0.5, \\'encoding\\': \\'windows-1252\\'}\\nTo convert a string to a standard unicode encoding, call encoding.convert(). This will \\ndecode the string from its original encoding, then re-encode it as UTF-8.\\n>>> encoding.convert(\\'ascii\\')\\nu\\'ascii\\' \\n>>> encoding.convert(u\\'abcdé\\')\\nu\\'abcd\\\\\\\\xc3\\\\\\\\xa9\\'\\n>>> encoding.convert(\\'\\\\222\\\\222\\\\223\\\\225\\')\\nu\\'\\\\u2019\\\\u2019\\\\u201c\\\\u2022\\'\\nHow it works...\\nThe detect() function is a wrapper around chardet.detect() which can handle \\nUnicodeDecodeError exceptions. In these cases, the string is encoded in UTF-8 before \\ntrying to detect the encoding.\\nThe convert() function irst calls detect() to get the encoding, then returns a \\nunicode string with the encoding as the second argument. By passing the encoding into \\nunicode(), the string is decoded from the original encoding, allowing it to be re-encoded \\ninto a standard encoding.\\nThere\\'s more...\\nThe comment at the top of the module, # -*- coding: utf-8 -*-, is a hint to the Python \\ninterpreter, telling it which encoding to use for the strings in the code. This is helpful for when \\nyou have non-ASCII strings in your source code, and is documented in detail at http://www.\\npython.org/dev/peps/pep-0263/.\\nConverting to ASCII\\nIf you want pure ASCII text, with non-ASCII characters converted to ASCII equivalents, \\nor dropped if there is no equivalent character, then you can use the unicodedata.\\nnormalize() function.\\n>>> import unicodedata\\n>>> unicodedata.normalize(\\'NFKD\\', u\\'abcd\\\\xe9\\').encode(\\'ascii\\', \\n\\'ignore\\')\\n\\'abcde\\'\\nParsing Speciic Data\\n242\\nSpecifying \\'NFKD\\' as the irst argument ensures the non-ASCII characters are replaced with \\ntheir equivalent ASCII versions, and the inal call to encode() with \\'ignore\\' as the second \\nargument will remove any extraneous unicode characters.\\nSee also\\nEncoding detection and conversion is a recommended irst step before doing HTML \\nprocessing with lxml or BeautifulSoup, covered in the Extracting URLs from HTML with \\nlxml and Converting HTML entities with BeautifulSoup recipes.\\nPenn Treebank  \\nPart-of-Speech Tags\\nFollowing is a table of all the part-of-speech tags that occur in the treebank corpus \\ndistributed with NLTK. The tags and counts shown here were acquired using the  \\nfollowing code:\\n>>> from nltk.probability import FreqDist\\n>>> from nltk.corpus import treebank\\n>>> fd = FreqDist()\\n>>> for word, tag in treebank.tagged_words():\\n...   fd.inc(tag)\\n>>> fd.items()\\nThe FreqDist fd contains all the counts shown here for every tag in the treebank corpus. \\nYou can inspect each tag count individually by doing fd[tag], as in fd[\\'DT\\']. Punctuation \\ntags are also shown, along with special tags such as -NONE-, which signiies that the part-\\nof-speech tag is unknown. Descriptions of most of the tags can be found at http://www.\\nling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html.\\nPenn Treebank Part-of-Speech Tags\\n244\\nPart-of-speech tag\\nFrequency of occurrence\\n#\\n16\\n$\\n724\\n\\'\\'\\n694\\n,\\n4,886\\n-LRB-\\n120\\n-NONE-\\n6,592\\n-RRB-\\n126\\n.\\n384\\n:\\n563\\n``\\n712\\nCC\\n2,265\\nCD\\n3,546\\nDT\\n8,165\\nEX\\n88\\nFW\\n4\\nIN\\n9,857\\nJJ\\n5,834\\nJJR\\n381\\nJJS\\n182\\nLS\\n13\\nMD\\n927\\nNN\\n13,166\\nNNP\\n9,410\\nNNPS\\n244\\nNNS\\n6,047\\nPDT\\n27\\nPOS\\n824\\nPRP\\n1,716\\nAppendix\\n245\\nPart-of-speech tag\\nFrequency of occurrence\\nPRP$\\n766\\nRB\\n2,822\\nRBR\\n136\\nRBS\\n35\\nRP\\n216\\nSYM\\n1\\nTO\\n2,179\\nUH\\n3\\nVB\\n2,554\\nVBD\\n3,043\\nVBG\\n1,460\\nVBN\\n2,134\\nVBP\\n1,321\\nVBZ\\n2,125\\nWDT\\n445\\nWP\\n241\\nWP$\\n14\\nIndex\\nSymbols\\n__contains__() method  213\\n__delitem__() method  213, 220\\n__getitem__() method  213, 220\\n__len__() method  213, 219\\n__setitem__() method  213, 220\\nA\\nAbstractLazySequence class\\nworking  76\\naccuracy() function  184\\nACE  133\\nAfixTagger\\nabout  96\\nmin_stem_length keyword argument  97\\nworking  96\\nanchor tag  235\\nAntonymReplacer class  43\\nantonyms  \\nabout  18, 41\\nnegations, replacing  41, 43\\nantonyms() method  19\\nappend_line() function  78\\naspell  36\\nAutomatic Content Extraction. See  ACE\\navailable_languages attribute  31\\nB\\nBabelish\\nabout  30\\ntext, translating  30, 31\\nbabelish.translate() function  30\\nbabelize() function  31\\nbackoff_tagger function  91\\nbackoff tagging\\nabout  88\\ntaggers, combining  88\\nbackreference  34\\nbag_of_bigrams_words() function  170\\nbag_of_words() function\\nformat  168\\nworking  168\\nBag of Words model  168\\nbag_of_words_not_in_set() function\\nabout  169\\nexample  169\\nbatch_tag() method  84\\nBayes Theorem\\nusing  170\\nBeautifulSoup library\\nabout  227, 238, 239\\nHTML entities, converting  238\\nURLs, extracting  239\\nBigramCollocationFinder constructs\\nbigrams, inding  22\\nBigramTagger\\nabout  90\\nworking  90, 91\\nbinary classiier  167, 171\\nbinary named entity\\nextracting  135\\nblock  70\\nblock reader functions\\nread_blankline_block()  73\\nread_line_block()  73\\nread_regexp_block()  73\\nread_whitespace_block()  73\\nread_wordpunct_block()  73\\nBrillTagger\\nabout  98\\n248\\ntraining  98, 99\\nworking  99\\nC\\ncapitalization\\nneed for  102\\ncardinal word  150\\ncategorized chunk corpus reader\\ncreating  61-64\\ncategorized text corpus\\ncreating  58, 59\\ncategory ile  61\\ncess_cat corpora\\nand cess_esp corpora  160\\ncess_esp corpora\\nand cess_cat corpora  160\\nchannel\\nabout  202\\nmultiple channels, creating  205\\ncharacter encodings\\nconverting  240, 241\\ndetecting  240, 241\\nchardet.detect() function  241\\nchardet library  227\\nchinking\\nabout  112\\nwith, regular expressions  112-114\\nChinkRule class  112\\nchinks  112\\nchoose_tag() method\\nabout  83\\narguments  83\\nChunkedCorpusReader\\nabout  55\\nworking  56\\nchunked phrase corpus\\ncreating  54\\nchunk extraction  111\\nchunking\\nabout  112\\nclassiication-based  129-132\\nlooping  125\\ntracing  126\\nwith, regular expressions  112-114\\nchunk patterns\\nabout  112\\nworking  113, 114\\nchunk rule\\nwith, context  116, 117\\nChunkRule class  112\\nchunks\\nabout  54, 111, 144\\nexpanding, with regular expressions  121-123\\nmerging, with regular expressions  117-119\\nremoving, with regular expressions  121-123\\nsplitting, with regular expressions  117-119\\ntypes  115\\nChunkScore\\nmetrics  125\\nchunk transformations\\nchaining  154\\nchunk tree\\nconverting, to text  155, 156\\nchunk_tree_to_sent() function\\nabout  156\\nworking  156\\nChunkWithContext class\\nexample  116\\nclassiication probability\\ngetting  174, 175\\nClassiierBasedPOSTagger\\nabout  106\\nworking  106\\nClassiierBasedTagger class  129-132\\nClassiierChunker class  129-132\\nclassiiers\\ncombining, with voting  191, 192\\nclassify() method  174, 175\\nclass-imbalance problem  199\\nclean_html() function  237\\nclear() method  213, 220\\ncollocations  21\\nconditional exponential classiier. See  \\nMaxentClassiier\\nconditional frequency distribution\\nstoring, in Redis  215, 217\\nConference on Computational Natural  \\nLanguage Learning. See  CoNLL\\nCoNLL  58\\nConll chunk corpus reader\\ncategorizing  65, 66\\ncontext model\\noverriding  87\\n249\\nconvert() function  241\\nconvert_tree_nodes() function\\nabout  163\\ntree nodes, converting  163, 164\\nworking  164\\ncorpora\\nabout  46\\ncategorizing  61\\ncorpus  46\\ncorpus editing\\nand ile locking  77, 78\\ncorpus view  70\\ncorrect_verbs() function\\nabout  147\\nverb forms, correcting  146, 148\\nworking  148\\nCSV synonym replacement  40\\nCsvWordReplacer class  40\\ncustom corpus\\nsetting up  46, 47\\ncustom corpus view\\ncreating  70\\nD\\ndates\\nparsing, with dateutil library  228, 229\\ndateutil library\\nabout  227\\ndates, parsing  228, 229\\ninstalling  230\\ntimes, parsing  228, 229\\ndecision tree classiier\\ntraining  177, 178\\nDecisionTreeClassiier\\nabout  177\\ndepth cutoff  179\\nentropy cutoff  178, 179\\nevaluating  190, 191\\nsupport cutoff  179\\nworking  178\\ndeep tree\\nlattening  157-160\\nDefaultTagger\\nworking  83\\ndefault tagging  82\\ndepth_cutoff  179\\ndetect() function  241\\ndict style feature sets  168\\ndistributed chunking\\nexecnet, used  206, 208\\ndistributed tagging\\nexecnet, used  202, 204\\nE\\nedit distance  37\\nEnchant\\nabout  36\\npersonal word list  38\\nen_GB Dictionary  38\\nentity tags  133\\nentropy\\nabout  178\\ncalculating  179\\nentropy_cutoff  178, 179\\nestimator\\ntraining  175, 176\\nevaluate() method  82, 84\\nexclusive lock  77\\nexecnet\\nabout  202\\ndistributed chunking  206, 208\\ndistributed tagging  202, 204\\ndistributed word scoring  221-223\\nparallel list processing  209, 210\\nexecnet.makegateway() function  203\\nExpandLeftRule rule  121\\nExpandRightRule rule  121\\nF\\nfalse negatives  183\\nfalse positives  183\\nfeature detector  106\\nfeature_detector() function  173\\nfeatures  106\\nfeature set  106, 167\\nilter_insigniicant() function\\nabout  145\\ninsigniicant words, iltering  144, 145\\nworking  145\\nirst_chunk_index() function\\nabout  147, 148\\nusing  149\\n250\\nlatten_childtrees() function\\nabout  157\\ndeep tree, lattening  159\\nworking  159\\nlatten_deeptree() function\\nabout  157, 162\\ndeep tree, lattening  158, 159\\nworking  159\\nF-measure  186\\nfrequency distribution\\nabout  22\\nstoring, in Redis  211-214\\nfull parsing  111\\nfuzzy parsing  229\\nG\\ngateways\\nabout  202\\nlocal, comparing with remote  206\\nGeneral Iterative Scaling. See  gis\\ngis  181\\ngrammar  112\\nH\\nheight() function  160\\nhigh information word\\nabout  187\\nmeasuring  188, 189\\nhigh_information_words() function\\nworking  189\\nHTML entities\\nabout  238\\nconverting, with BeautifulSoup library  238\\nhypernyms  15\\nhyponyms  15\\nI\\nieer_chunked_sents() function\\nabout  141\\nusing  140\\nieer corpus  140\\nieertree2conlltags() function\\nusing  140\\niis  181\\nImproved Iterative Scaling. See  iis\\nininitive phrase\\nabout  151\\nswapping  151\\nInformation Extraction—Entity Recognition. \\nSee  ieer corpus\\ninformation gain  175\\ninpred lambda  152\\ninstallation, NLTK  8\\ninstance  168\\nInternational Standards Organization. See  ISO\\nIOB tags  56\\nISO  230\\nitems() method  213, 220\\niterlinks() function  235, 236\\nJ\\njaccard() function  189\\nJaccard index  189\\nK\\nkeys() method  213, 220\\nL\\nlabel_feats_from_corpus() function  171, 173\\nlabelled feature sets  167\\nLancasterStemmer class  26, 27\\nLancaster Stemming Algorithm  26\\nLazyCorpusLoader class\\nabout  68\\narguments  68\\nworking  69\\nLeacock Chodorow (LCH) similarity  21\\nleaves() method  58\\nlemma\\nabout  17, 28\\ninding, in WordNet  17, 18\\nlemmatization\\nabout  28\\ncombining, with stemming  29, 30\\nlinks\\nextracting  236\\nlocal gateways\\nversus remote gateways  206\\n251\\nLocationChunker class\\nworking  139\\nlocation chunks\\nextracting  137, 139\\nlog likelihood  181\\nlow information words  187\\nlxml.html.clean_html() function  237\\nlxml library\\nabout  227, 234\\nURLs, extracting from HTML  234\\nworking  235\\nM\\nmap() function  209\\nmasi distance\\nusing  196\\nMaxentClassiier\\nabout  180\\nevaluating  190\\nrequisites  180\\nusing  107\\nworking  181\\nmaximum entropy classiier\\ntraining  180, 181\\nMaxVoteClassiier\\nworking  193\\nmegam algorithm\\nabout  183\\nworking  183\\nMergeRule class\\nabout  117\\nworking  119\\nMongoDB\\nabout  74\\nworking  76\\nmorphy() function  29\\nmulti-label classiier  167\\nabout  193\\nclassifying with  195, 196\\nmultiple binary classiiers\\ncreating  193, 194\\nmultiple channels\\ncreating  205\\nN\\nnaive Bayes classiier\\ntraining  170-174\\nNaiveBayesClassiier\\nabout  170\\nmanual training  176\\nworking  173, 174\\nNaiveBayesClassiier classiier, methods\\nmost_informative_features()  175\\nshow_most_informative_features()  175\\nNaiveBayesClassiier.train() method  173\\nNAME chunker  136, 139\\nnamed entities\\nextracting  133, 134\\nnamed entity chunker\\ntraining  140, 141\\nnamed entity recognition  133, 134\\nnames corpus  49\\nNamesTagger\\nabout  105\\nworking  105\\nNational Institute of Standards and  \\nTechnology. See  NIST\\nNatural Language Toolkit. See  NLTK\\nnegations\\nreplacing, with antonyms  41, 43\\nnegative feature sets  198\\nngram  89\\nNgramTagger  90\\nNIST  133\\nNLTK\\nabout  7, 14, 202\\ninstalling  8\\nURL  8\\nnltk.data.ind() function\\ndata directories, searching  65\\nnltk.data.load() function  47\\nNLTK data package\\ninstalling  46\\nnltk.metrics package  23\\nnltk.tag package  82\\nnltk.tag.pos_tag() function  202\\nnltk.tag.untag() function  84\\nnltk.tokenize.punkt module  9\\nnltk.util.clean_html() function  237\\n252\\nnon-UTC datetime object\\nconverting, to UTC datetime object  231\\nnoun cardinals\\nswapping  150\\nO\\nordered dictionary\\nabout  218, 219\\nstoring, in Redis  218, 219\\nP\\nparagraph block reader\\ncustomizing  53\\nparallel list processing\\nexecnet, used  209, 210\\nparse() method  120, 139\\npartial parsing\\nabout  111\\nwith, regular expressions  123, 124\\npart-of-speech tagged word corpus\\ncreating  50, 51\\npart-of-speech tagging\\nabout  50, 82\\nneed for  82\\npart-of-speech tags\\n  244\\n``  244\\n,  244\\n.  244\\n’’  244\\n#  244\\n$  244\\nabout  82\\nCC  244\\nCD  244\\nDT  244\\nEX  244\\nFW  244\\nIN  244\\nJJ  244\\nJJR  244\\nJJS  244\\n-LRB-  244\\nLS  244\\nMD  244\\nNN  244\\nNNP  244\\nNNPS  244\\nNNS  244\\n-NONE-  244\\nPDT  244\\nPOS  244\\nPRP  244\\nPRP$  245\\nRB  245\\nRBR  245\\nRBS  245\\nRP  245\\n-RRB-  244\\nSYM  245\\nTO  245\\nUH  245\\nVB  245\\nVBD  245\\nVBG  245\\nVBN  245\\nVBP  245\\nVBZ  245\\nWDT  245\\nWP  245\\nWP$  245\\npath similarity  21\\nPenn Treebank corpus  111\\nP(features | label) probability  171\\nP(features) probability  171\\nphrases  111\\nPickleCorpusView  73\\nP(label | features) probability  171\\nP(label) probability  171\\nplural nouns\\nsingularizing  153\\nPorterStemmer class  26, 27\\nPorter Stemming Algorithm  26\\npositive feature sets  198\\nPOS tag\\nabout  16\\nexample  16\\nprecision\\nabout  183, 185\\nmeasuring  183, 184\\nprecision_recall() function  184, 185\\nprob_classify() method  174\\n253\\nproper names\\ntagging  105, 106\\nproper noun chunks\\nextracting  135\\nPunktSentenceTokenizer  9\\nPunktWordTokenizer  11\\npyenchant library  36\\nPython subprocesses  208\\nQ\\nQuadgramTagger\\nabout  91\\nworking  92\\nR\\nread_blankline_block() function  73\\nread_line_block() function  73\\nread_regexp_block() function  73\\nread_whitespace_block() function  73\\nread_wordpunct_block() function  73\\nrecall  183, 185\\nRedis\\nabout  211\\nconditional frequency distribution, storing  \\n215, 217\\ndistributed word scoring  221-223\\nfrequency distribution, storing  211-214\\nordered dictionary, storing  218, 219\\nreference set  185\\nre.indall() method  12\\nRegexpReplacer.replace()  33\\nRegexpStemmer class  27\\nRegexpTagger\\nabout  95\\nworking  95\\nRegexpTokenizer\\ntokenizing, on whitespace  13\\nworking  12\\nregular expressions\\nchinking  112-114\\nchunking  112-114\\npartial parsing  123, 124\\nsentences, tokenizing  11, 12\\nused, in tagging  94, 95\\nremote_exec() method\\nabout  203\\narguments, types  204\\nremote gateways\\nversus local gateways  206\\nremove_line() function  78\\nrepeatitive characters\\nremoving  34, 35\\nRepeatReplacer class  35\\nreplace() method  32, 43\\nreplace_negations() method  43\\nre.subn() function  32\\nreuters corpus  194\\nreuters_high_info_words() function  194\\nworking  198\\nS\\nscoring functions  23\\nscoring ngrams  23\\nsentences\\ntagged sentence, untagging  84\\ntagging  84\\ntokenizing, into words  9, 10\\ntokenizing, regular expressions used  11, 12\\nsentence tokenizer\\ncustomizing  53\\nSequentialBackoffTagger\\nabout  88\\nworking  88\\nshallow tree\\ncreating  161, 162\\nshallow_tree() function\\nabout  161\\nshallow tree, creating  161, 162\\nworking  162\\nshow_most_informative_features() method  \\n182, 187\\nsigniicant bigrams\\nabout  170\\nincluding  170\\nsingularize_plural_noun() function\\nabout  153\\nplural nouns, singularizing  153\\nworking  153\\nSnowballStemmer class  28\\nSpaceTokenizer  10\\n254\\nSpellingReplacer class  37\\nspellings\\ncorrecting, with Enchant  36, 37\\nspicy algorithms\\nabout  182\\nBroyden-Fletcher-Goldfarb-Shanno algorithm \\n(BFGS)  182\\nConjugate gradient (CG) algorithm  182\\nLBFGSB (limited memory version of BFGS)  \\n182\\nNelder-Mead  182\\nPowell  182\\nsplit_label_feats() function  173\\nSplitRule class\\nabout  117\\nworking  119\\nStemmerI interface  27\\nstem() method  27, 28\\nstemming\\nabout  25\\ncombining, with lemmatization  29, 30\\nstopile keyword  169\\nstopwords\\nabout  13, 144\\niltering  169\\niltering, in tokenized sentence  13\\nsupport_cutoff  179\\nswap_ininitive_phrase() function\\nabout  151\\nininitive phrases, swapping  152\\nworking  152\\nswap_noun_cardinal() function\\nabout  150\\nnoun cardinals, swapping  151\\nworking  151\\nswap_verb_phrase() function\\nabout  149\\nverb phrases, swapping  149\\nsynonyms\\nreplacing  39, 40\\ninding, in WordNet  17, 18\\nsynsets\\nabout  15\\ninding, for word in WordNet  14, 15\\nT\\ntag  82\\ntagged corpus reader\\ncategorizing  61\\nTaggedCorpusReader\\ncreating  51\\nworking  52\\ntagged sentence\\nuntagging  84\\ntagged_sents() function  82\\ntagged token  52, 83\\ntagger-based chunker\\ntraining  126-128\\ntaggers\\nabout  82\\nAfixTagger  96\\nBigramTagger  89-91\\nBrillTagger  98, 99\\nClassiierBasedPOSTagger  106, 107\\ncombining, with backoff tagging  88\\nContextTagger  85\\nDefaultTagger  82, 83\\nNamesTagger  105\\nNgramTagger  85-91\\nQuadgram  91, 92\\nRegexpTagger  95\\nSequentialBackoffTagger  83, 88\\nTnT tagger  100, 101\\nTrigramTagger  89\\nUnigramTagger  85, 86, 93\\nWordNetTagger  104\\ntagging\\nregular expressions, used  94, 95\\nWordNet, used  103, 104\\ntag mapping function\\ntags, simplifying  53\\ntag() method  82\\ntag_pattern2re_pattern() function  112\\ntag separator\\ncustomizing  53\\nt.draw() method  113\\ntemporal expression\\nabout  233\\ntagging, with timex library  233\\ntest set  185\\n255\\ntext\\ntokenizing, into sentences  8, 9\\ntranslating, with Bableish  30, 31\\ntext classiication  167\\ntext feature extraction  168\\ntime\\nparsing, with dateutil library  228, 229\\ntimex library\\nabout  227\\ntemporal expressions, tagging  233\\nusing  233\\nTnT tagger\\nabout  100\\nworking  101\\ntokenization  7, 8\\ntokenized sentence\\nstopwords, iltering  13\\ntokens  70\\ntrained tagger\\npreserving  89\\nunpreserving  89\\ntransform_chunk() function\\nabout  154\\nchunk transformations, chaining  154\\nworking  154\\ntreebank_chunk corpus  125\\ntreebank corpus  144, 243\\nTreebankWordTokenizer  10, 11\\ntree nodes\\nconverting  163, 164\\nTree.pos() method  157\\nTrigramCollocationFinder  23\\nTrigrams’n’Tags. See  TnT tagger\\nTrigramTagger\\nabout  89\\nworking  90, 91\\ntrue negative  183\\ntrue positive  183\\ntzile object  231\\ntzinfo object\\ngetting  230\\ntz.tzlocal() function  232\\nU\\nUnChunkRule rule  121\\nunicodedata.normalize() function  241\\nunigram  85\\nUnigramTagger\\nabout  85, 93\\ntraining  85\\nworking  86\\nunlabelled feature set  167\\nurllib request  31\\nUTC  230\\nV\\nvalues() method  213, 220\\nverb forms\\ncorrecting  146, 148\\nverb phrases\\nswapping  149\\nverbs\\ncomparing  21\\nvoting\\nclassiiers, combining with  191, 192\\nW\\nWhitespaceTokenizer  10\\nword list corpus\\ncreating  48, 49\\nWordListCorpusReader\\nabout  48\\nworking  49\\nWordNet\\nabout  8, 14\\nlemma, inding  17, 18\\nPOS tag  16\\nsynonyms, inding  17, 18\\nsynset similarity, comparing  19, 20\\nused, for tagging  103, 104\\nwords, lemmatising  28\\nWordNetCorpusReader class  29\\nWordNetLemmatizer class  29\\nWordNetTagger\\nworking  104\\nWordPunctTokenizer  11\\nWordReplacer class  40\\nwords\\nlemmatising, with WordNet  28\\nreplacing, based on regular expressions \\n  32, 33\\nstemming  25, 26\\n256\\ninsigniicant words, iltering  144, 145\\nword_tag_model() function  94\\nword_tokenize() function\\nabout  10\\nworking  10\\nword tokenizer\\ncustomizing  52\\nWu-Palmer Similarity method. See  \\nwup_similarity() method\\nwup_similarity() method  20, 21\\nX\\nxpath() method\\nlinks, extracting  236\\nY\\nYAML ile\\nloading  47\\nYAML synonym replacement  41\\nThank you for buying \\nPython Text Processing with  \\nNLTK 2.0 Cookbook\\nAbout Packt Publishing\\nPackt, pronounced \\'packed\\', published its irst book \"Mastering phpMyAdmin for Effective MySQL \\nManagement\" in April 2004 and subsequently continued to specialize in publishing highly focused \\nbooks on speciic technologies and solutions.\\nOur books and publications share the experiences of your fellow IT professionals in adapting and \\ncustomizing today\\'s systems, applications, and frameworks. Our solution based books give you the \\nknowledge and power to customize the software and technologies you\\'re using to get the job done. \\nPackt books are more speciic and less general than the IT books you have seen in the past. Our \\nunique business model allows us to bring you more focused information, giving you more of what \\nyou need to know, and less of what you don\\'t.\\nPackt is a modern, yet unique publishing company, which focuses on producing quality, cutting-\\nedge books for communities of developers, administrators, and newbies alike. For more \\ninformation, please visit our website: www.packtpub.com.\\nAbout Packt Open Source\\nIn 2010, Packt launched two new brands, Packt Open Source and Packt Enterprise, in order to \\ncontinue its focus on specialization. This book is part of the Packt Open Source brand, home \\nto books published on software built around Open Source licences, and offering information to \\nanybody from advanced developers to budding web designers. The Open Source brand also runs \\nPackt\\'s Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project \\nabout whose software a book is sold.\\nWriting for Packt\\nWe welcome all inquiries from people who are interested in authoring. Book proposals should \\nbe sent to author@packtpub.com. If your book idea is still at an early stage and you would like to \\ndiscuss it irst before writing a formal book proposal, contact us; one of our commissioning editors \\nwill get in touch with you. \\nWe\\'re not just looking for published authors; if you have strong technical skills but no writing \\nexperience, our experienced editors can help you develop a writing career, or simply get some \\nadditional reward for your expertise.\\nExpert Python Programming\\nISBN: 978-1-847194-94-7             Paperback: 372 pages\\nBest practices for designing, coding, and distributing \\nyour Python software\\n1. \\nLearn Python development best practices from \\nan expert, with detailed coverage of naming and \\ncoding conventions \\n2. \\nApply object-oriented principles, design patterns, \\nand advanced syntax tricks \\n3. \\nManage your code with distributed version control \\n4. \\nProile and optimize your code \\nMySQL for Python\\nISBN: 978-1-849510-18-9            Paperback: 440 pages\\nIntegrate the lexibility of Python and the power of MySQL \\nto boost the productivity of your Python applications\\n1. \\nImplement the outstanding features of Python\\'s \\nMySQL library to their full potential \\n2. \\nSee how to make MySQL take the processing \\nburden from your programs \\n3. \\nLearn how to employ Python with MySQL to power \\nyour websites and desktop applications \\n4. \\nApply your knowledge of MySQL and Python \\nto real-world problems instead of hypothetical \\nscenarios \\n \\n \\n \\n \\nPlease check www.PacktPub.com for information on our titles\\nPython 3 Object Oriented \\nProgramming\\nISBN:  978-1-849511-26-1            Paperback: 404 pages\\nHarness the power of Python 3 objects\\n1. \\nLearn how to do Object Oriented Programming in \\nPython using this step-by-step tutorial\\n2. \\nDesign public interfaces using abstraction, \\nencapsulation, and information hiding\\n3. \\nTurn your designs into working software by \\nstudying the Python syntax \\n4. \\nRaise, handle, deine, and manipulate exceptions \\nusing special error objects \\nPython Testing: Beginner\\'s \\nGuide\\nISBN: 978-1-847198-84-6            Paperback: 256 pages\\nAn easy and convenient approach to testing your \\npowerful Python projects\\n1. \\nCovers everything you need to test your code in \\nPython \\n2. \\nEasiest and enjoyable approach to learn Python \\ntesting \\n3. \\nWrite, execute, and understand the result of tests \\nin the unit test framework \\n \\n \\nPlease check www.PacktPub.com for information on our titles\\nSpring Python 1.1\\nISBN: 978-1-849510-66-0             Paperback: 264 pages\\nCreate powerful and versatile Spring Python applications \\nusing pragmatic libraries and useful abstractions\\n1. \\nMaximize the use of Spring features in Python and \\ndevelop impressive Spring Python applications\\n2. \\nExplore the versatility of Spring Python by \\nintegrating it with frameworks, libraries, and tools \\n3. \\nDiscover the non-intrusive Spring way of wiring \\ntogether Python components \\n4. \\nPacked with hands-on-examples, case studies, \\nand clear explanations for better understanding  \\nPython Multimedia\\nISBN: 978-1-849510-16-5            Paperback: 292 pages\\nLearn how to develop Multimedia applications using \\nPython with this practical step-by-step guide\\n1. \\nUse Python Imaging Library for digital image \\nprocessing. \\n2. \\nCreate exciting 2D cartoon characters using Pyglet \\nmultimedia framework\\n3. \\nCreate GUI-based audio and video players using \\nQT Phonon framework. \\n4. \\nGet to grips with the primer on GStreamer \\nmultimedia framework and use this API for audio \\nand video processing.\\n \\n \\n \\n \\nPlease check www.PacktPub.com for information on our titles\\nMatplotlib for Python \\nDevelopers\\nISBN:  978-1-847197-90-0            Paperback: 308 pages\\nBuild remarkable publication-quality plots the easy way\\n1. \\nCreate high quality 2D plots by using Matplotlib \\nproductively \\n2. \\nIncremental introduction to Matplotlib, from the \\nground up to advanced levels\\n3. \\nEmbed Matplotlib in GTK+, Qt, and wxWidgets \\napplications as well as web sites to utilize them in \\nPython applications \\n4. \\nDeploy Matplotlib in web applications and expose \\nit on the Web using popular web frameworks such \\nas Pylons and Django \\n \\n \\nPlease check www.PacktPub.com for information on our titles\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to delete specific lines from a text file\n",
        "def delete_lines(file_name, lines_to_delete):\n",
        "    # Read all lines from the file\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Filter out the lines to be deleted\n",
        "    remaining_lines = [line for index, line in enumerate(lines) if index + 1 not in lines_to_delete]\n",
        "\n",
        "    # Write the remaining lines back to the file\n",
        "    with open(file_name, 'w', encoding='utf-8') as file:\n",
        "        file.writelines(remaining_lines)\n",
        "\n",
        "# Specify the lines to delete (1-480 and 8257-9313)\n",
        "lines_to_delete = list(range(1, 500)) + list(range(8257, 9313))\n",
        "\n",
        "# Call the function with your file name\n",
        "delete_lines('extracted_book.txt', lines_to_delete)\n",
        "\n",
        "print(\"Specified lines have been deleted from 'extracted_book.txt'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY2rnhGavLWg",
        "outputId": "6628b3b0-6515-4ef8-adc5-49465d8973d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Specified lines have been deleted from 'extracted_book.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_file = 'extracted_book.txt'\n",
        "extract_text = fitz.open(extract_file)\n",
        "\n",
        "# Extract text from each page\n",
        "text = \"\"\n",
        "for page in extract_text:\n",
        "    text += page.get_text()\n",
        "\n",
        "# Close the document\n",
        "extract_text.close()\n",
        "\n",
        "# Display the first 500 characters of the book\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V4xmNX19rG_",
        "outputId": "961e867d-b9f2-48ad-8bf9-6528c8d7845f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Much better—we can clearly see four of the most\n",
            "common bigrams in Monty Python and the \n",
            "Holy Grail. If you'd like to see more than four,\n",
            "simply increase the number to whatever you \n",
            "want, and the collocation inder will do its best.\n",
            "How it works...\n",
            "The BigramCollocationFinder constructs two frequency\n",
            "distributions: one for each \n",
            "word, and another for bigrams. A frequency\n",
            "distribution, or FreqDist in NLTK, is basically \n",
            "an enhanced dictionary where the keys are what's\n",
            "being counted, and the values \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "exbp3okkzp8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "J1D42emD32Ro"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "uThBuxIpz6Xf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "wyXljxh-0ArK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR4V3zG70NV2",
        "outputId": "13d3b22f-ef84-447b-cad4-b6678b9acbe3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3926"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentence in text.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "MOtOQgyq0QOc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwAPK_3NAV49",
        "outputId": "4cb99c75-3f09-4a92-f90f-205bf77765f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[331, 2502],\n",
              " [331, 2502, 11],\n",
              " [331, 2502, 11, 1421],\n",
              " [331, 2502, 11, 1421, 66],\n",
              " [331, 2502, 11, 1421, 66, 720],\n",
              " [331, 2502, 11, 1421, 66, 720, 4],\n",
              " [331, 2502, 11, 1421, 66, 720, 4, 1],\n",
              " [331, 2502, 11, 1421, 66, 720, 4, 1, 138],\n",
              " [293, 524],\n",
              " [293, 524, 5],\n",
              " [293, 524, 5, 2503],\n",
              " [293, 524, 5, 2503, 209],\n",
              " [293, 524, 5, 2503, 209, 6],\n",
              " [293, 524, 5, 2503, 209, 6, 1],\n",
              " [2504, 2505],\n",
              " [2504, 2505, 30],\n",
              " [2504, 2505, 30, 1644],\n",
              " [2504, 2505, 30, 1644, 158],\n",
              " [2504, 2505, 30, 1644, 158, 3],\n",
              " [2504, 2505, 30, 1644, 158, 3, 66],\n",
              " [2504, 2505, 30, 1644, 158, 3, 66, 43],\n",
              " [2504, 2505, 30, 1644, 158, 3, 66, 43, 162],\n",
              " [2504, 2505, 30, 1644, 158, 3, 66, 43, 162, 720],\n",
              " [606, 676],\n",
              " [606, 676, 1],\n",
              " [606, 676, 1, 159],\n",
              " [606, 676, 1, 159, 3],\n",
              " [606, 676, 1, 159, 3, 784],\n",
              " [606, 676, 1, 159, 3, 784, 13],\n",
              " [120, 6],\n",
              " [120, 6, 1],\n",
              " [120, 6, 1, 1645],\n",
              " [120, 6, 1, 1645, 1646],\n",
              " [120, 6, 1, 1645, 1646, 29],\n",
              " [120, 6, 1, 1645, 1646, 29, 55],\n",
              " [120, 6, 1, 1645, 1646, 29, 55, 150],\n",
              " [120, 6, 1, 1645, 1646, 29, 55, 150, 449],\n",
              " [33, 10],\n",
              " [33, 10, 107],\n",
              " [1, 1102],\n",
              " [1, 1102, 997],\n",
              " [1, 1102, 997, 87],\n",
              " [1, 1102, 997, 87, 314],\n",
              " [998, 101],\n",
              " [998, 101, 8],\n",
              " [998, 101, 8, 50],\n",
              " [19, 6],\n",
              " [19, 6, 400],\n",
              " [19, 6, 400, 8],\n",
              " [19, 6, 400, 8, 524],\n",
              " [19, 6, 400, 8, 524, 2],\n",
              " [19, 6, 400, 8, 524, 2, 314],\n",
              " [677, 45],\n",
              " [677, 45, 225],\n",
              " [677, 45, 225, 5],\n",
              " [677, 45, 225, 5, 23],\n",
              " [677, 45, 225, 5, 23, 7],\n",
              " [677, 45, 225, 5, 23, 7, 1946],\n",
              " [37, 1947],\n",
              " [37, 1947, 284],\n",
              " [37, 1947, 284, 142],\n",
              " [37, 1947, 284, 142, 1],\n",
              " [37, 1947, 284, 142, 1, 221],\n",
              " [37, 1947, 284, 142, 1, 221, 25],\n",
              " [37, 1947, 284, 142, 1, 221, 25, 1948],\n",
              " [637, 2506],\n",
              " [637, 2506, 6],\n",
              " [637, 2506, 6, 1],\n",
              " [637, 2506, 6, 1, 243],\n",
              " [637, 2506, 6, 1, 243, 25],\n",
              " [637, 2506, 6, 1, 243, 25, 1],\n",
              " [607, 160],\n",
              " [607, 160, 1103],\n",
              " [607, 160, 1103, 218],\n",
              " [607, 160, 1103, 218, 15],\n",
              " [607, 160, 1103, 218, 15, 25],\n",
              " [607, 160, 1103, 218, 15, 25, 843],\n",
              " [1236, 1],\n",
              " [1236, 1, 844],\n",
              " [1236, 1, 844, 4],\n",
              " [1236, 1, 844, 4, 94],\n",
              " [1236, 1, 844, 4, 94, 87],\n",
              " [1236, 1, 844, 4, 94, 87, 2507],\n",
              " [1236, 1, 844, 4, 94, 87, 2507, 36],\n",
              " [1237, 160],\n",
              " [1237, 160, 14],\n",
              " [1237, 160, 14, 15],\n",
              " [1237, 160, 14, 15, 332],\n",
              " [1237, 160, 14, 15, 332, 147],\n",
              " [1237, 160, 14, 15, 332, 147, 1],\n",
              " [1237, 160, 14, 15, 332, 147, 1, 845],\n",
              " [1237, 160, 14, 15, 332, 147, 1, 845, 36],\n",
              " [39, 2],\n",
              " [39, 2, 1103],\n",
              " [39, 2, 1103, 53],\n",
              " [39, 2, 1103, 53, 3],\n",
              " [39, 2, 1103, 53, 3, 1422],\n",
              " [39, 2, 1103, 53, 3, 1422, 51],\n",
              " [14, 15],\n",
              " [14, 15, 25],\n",
              " [14, 15, 25, 101],\n",
              " [14, 15, 25, 101, 45],\n",
              " [14, 15, 25, 101, 45, 87],\n",
              " [14, 15, 25, 101, 45, 87, 315],\n",
              " [14, 15, 25, 101, 45, 87, 315, 6],\n",
              " [14, 15, 25, 101, 45, 87, 315, 6, 51],\n",
              " [14, 15, 25, 101, 45, 87, 315, 6, 51, 608],\n",
              " [500, 9],\n",
              " [500, 9, 11],\n",
              " [500, 9, 11, 69],\n",
              " [500, 9, 11, 69, 2],\n",
              " [500, 9, 11, 69, 2, 331],\n",
              " [500, 9, 11, 69, 2, 331, 1238],\n",
              " [285, 401],\n",
              " [285, 401, 1103],\n",
              " [285, 401, 1103, 1],\n",
              " [285, 401, 1103, 1, 1645],\n",
              " [285, 401, 1103, 1, 1645, 1646],\n",
              " [285, 401, 1103, 1, 1645, 1646, 7],\n",
              " [222, 3],\n",
              " [222, 3, 1647],\n",
              " [222, 3, 1647, 2],\n",
              " [222, 3, 1647, 2, 1949],\n",
              " [222, 3, 1647, 2, 1949, 565],\n",
              " [222, 3, 1647, 2, 1949, 565, 53],\n",
              " [222, 3, 1647, 2, 1949, 565, 53, 8],\n",
              " [1239, 917],\n",
              " [1239, 917, 846],\n",
              " [1239, 917, 846, 565],\n",
              " [1239, 917, 846, 565, 218],\n",
              " [1239, 917, 846, 565, 218, 25],\n",
              " [234, 5],\n",
              " [234, 5, 1],\n",
              " [234, 5, 1, 565],\n",
              " [234, 5, 1, 565, 218],\n",
              " [234, 5, 1, 565, 218, 721],\n",
              " [918, 5],\n",
              " [918, 5, 20],\n",
              " [918, 5, 20, 38],\n",
              " [38, 67],\n",
              " [115, 43],\n",
              " [5, 847],\n",
              " [5, 847, 3],\n",
              " [5, 847, 3, 1102],\n",
              " [5, 847, 3, 1102, 115],\n",
              " [5, 847, 3, 1102, 115, 42],\n",
              " [8, 1239],\n",
              " [8, 1239, 2509],\n",
              " [8, 1239, 2509, 148],\n",
              " [8, 1239, 2509, 148, 4],\n",
              " [8, 1239, 2509, 148, 4, 1423],\n",
              " [8, 1239, 2509, 148, 4, 1423, 20],\n",
              " [8, 1239, 2509, 148, 4, 1423, 20, 121],\n",
              " [8, 1239, 2509, 148, 4, 1423, 20, 121, 204],\n",
              " [301, 8],\n",
              " [301, 8, 1424],\n",
              " [301, 8, 1424, 5],\n",
              " [301, 8, 1424, 5, 2510],\n",
              " [301, 8, 1424, 5, 2510, 2511],\n",
              " [301, 8, 1424, 5, 2510, 2511, 2512],\n",
              " [17, 23],\n",
              " [17, 23, 917],\n",
              " [17, 23, 917, 31],\n",
              " [17, 23],\n",
              " [17, 23, 278],\n",
              " [17, 23, 278, 31],\n",
              " [17, 23, 278, 31, 1950],\n",
              " [14, 316],\n",
              " [14, 316, 638],\n",
              " [14, 316, 638, 8],\n",
              " [14, 316, 638, 8, 316],\n",
              " [14, 316, 638, 8, 316, 5],\n",
              " [2513, 14],\n",
              " [2513, 14, 2514],\n",
              " [2513, 14, 2514, 377],\n",
              " [1425, 1648],\n",
              " [1425, 1648, 17],\n",
              " [1425, 1648, 17, 14],\n",
              " [1425, 1648, 17, 14, 14],\n",
              " [1425, 420],\n",
              " [1425, 420, 19],\n",
              " [1425, 420, 19, 549],\n",
              " [1425, 420, 19, 549, 549],\n",
              " [1425, 420, 19, 549, 549, 1649],\n",
              " [1425, 420],\n",
              " [1425, 420, 1426],\n",
              " [1425, 420, 1426, 549],\n",
              " [1425, 420, 1426, 549, 135],\n",
              " [1425, 1650],\n",
              " [1425, 1650, 1950],\n",
              " [1425, 1650, 1950, 722],\n",
              " [1425, 1650, 1950, 722, 1951],\n",
              " [2515, 2516],\n",
              " [2515, 2516, 2517],\n",
              " [302, 9],\n",
              " [302, 9, 332],\n",
              " [302, 9, 332, 303],\n",
              " [302, 9, 332, 303, 723],\n",
              " [302, 9, 332, 303, 723, 1427],\n",
              " [302, 9, 332, 303, 723, 1427, 25],\n",
              " [302, 9, 332, 303, 723, 1427, 25, 378],\n",
              " [302, 9, 332, 303, 723, 1427, 25, 378, 8],\n",
              " [302, 9, 332, 303, 723, 1427, 25, 378, 8, 2],\n",
              " [566, 1428],\n",
              " [566, 1428, 2518],\n",
              " [566, 1428, 2518, 45],\n",
              " [566, 1428, 2518, 45, 61],\n",
              " [566, 1428, 2518, 45, 61, 72],\n",
              " [566, 1428, 2518, 45, 61, 72, 1421],\n",
              " [179, 37],\n",
              " [179, 37, 1240],\n",
              " [179, 37, 1240, 2519],\n",
              " [179, 37, 1240, 2519, 5],\n",
              " [179, 37, 1240, 2519, 5, 847],\n",
              " [179, 37, 1240, 2519, 5, 847, 3],\n",
              " [179, 37, 1240, 2519, 5, 847, 3, 1],\n",
              " [179, 37, 1240, 2519, 5, 847, 3, 1, 1651],\n",
              " [845, 9],\n",
              " [845, 9, 42],\n",
              " [845, 9, 42, 843],\n",
              " [845, 9, 42, 843, 2],\n",
              " [845, 9, 42, 843, 2, 314],\n",
              " [845, 9, 42, 843, 2, 314, 845],\n",
              " [40, 999],\n",
              " [40, 999, 160],\n",
              " [40, 999, 160, 1424],\n",
              " [40, 999, 160, 1424, 15],\n",
              " [40, 999, 160, 1424, 15, 1000],\n",
              " [40, 999, 160, 1424, 15, 1000, 421],\n",
              " [40, 999, 160, 1424, 15, 1000, 421, 162],\n",
              " [235, 678],\n",
              " [235, 678, 20],\n",
              " [235, 678, 20, 7],\n",
              " [235, 678, 20, 7, 1241],\n",
              " [235, 678, 20, 7, 1241, 117],\n",
              " [235, 678, 20, 7, 1241, 117, 101],\n",
              " [235, 678, 20, 7, 1241, 117, 101, 285],\n",
              " [317, 361],\n",
              " [317, 361, 123],\n",
              " [317, 361, 123, 9],\n",
              " [317, 361, 123, 9, 2520],\n",
              " [317, 361, 123, 9, 2520, 8],\n",
              " [317, 361, 123, 9, 2520, 8, 2521],\n",
              " [317, 361, 123, 9, 2520, 8, 2521, 149],\n",
              " [317, 361, 123, 9, 2520, 8, 2521, 149, 317],\n",
              " [117, 101],\n",
              " [117, 101, 285],\n",
              " [117, 101, 285, 15],\n",
              " [117, 101, 285, 15, 1000],\n",
              " [117, 101, 285, 15, 1000, 43],\n",
              " [162, 1952],\n",
              " [565, 218],\n",
              " [149, 25],\n",
              " [149, 25, 265],\n",
              " [149, 25, 265, 43],\n",
              " [149, 25, 265, 43, 565],\n",
              " [149, 25, 265, 43, 565, 218],\n",
              " [149, 25, 265, 43, 565, 218, 567],\n",
              " [1652, 722],\n",
              " [1652, 722, 1951],\n",
              " [1652, 722, 1951, 72],\n",
              " [1652, 722, 1951, 72, 109],\n",
              " [162, 785],\n",
              " [162, 785, 1426],\n",
              " [162, 785, 1426, 13],\n",
              " [162, 785, 1426, 13, 343],\n",
              " [162, 785, 1426, 13, 343, 125],\n",
              " [162, 785, 1426, 13, 343, 125, 2],\n",
              " [162, 785, 1426, 13, 343, 125, 2, 919],\n",
              " [162, 785, 1426, 13, 343, 125, 2, 919, 4],\n",
              " [162, 785, 1426, 13, 343, 125, 2, 919, 4, 2],\n",
              " [162, 785, 1426, 13, 343, 125, 2, 919, 4, 2, 2522],\n",
              " [2523, 3],\n",
              " [2523, 3, 1653],\n",
              " [2523, 3, 1653, 33],\n",
              " [2523, 3, 1653, 33, 249],\n",
              " [476, 2524],\n",
              " [476, 2524, 1],\n",
              " [476, 2524, 1, 23],\n",
              " [476, 2524, 1, 23, 786],\n",
              " [476, 2524, 1, 23, 786, 848],\n",
              " [476, 2524, 1, 23, 786, 848, 8],\n",
              " [2525, 5],\n",
              " [2525, 5, 1],\n",
              " [2525, 5, 1, 23],\n",
              " [2525, 5, 1, 23, 278],\n",
              " [639, 3],\n",
              " [639, 3, 66],\n",
              " [639, 3, 66, 51],\n",
              " [639, 3, 66, 51, 1],\n",
              " [639, 3, 66, 51, 1, 436],\n",
              " [639, 3, 66, 51, 1, 436, 565],\n",
              " [639, 3, 66, 51, 1, 436, 565, 218],\n",
              " [565, 1104],\n",
              " [5, 847],\n",
              " [5, 847, 3],\n",
              " [5, 847, 3, 1],\n",
              " [5, 847, 3, 1, 1650],\n",
              " [5, 847, 3, 1, 1650, 76],\n",
              " [5, 847, 3, 1, 1650, 76, 149],\n",
              " [5, 847, 3, 1, 1650, 76, 149, 25],\n",
              " [5, 847, 3, 1, 1650, 76, 149, 25, 87],\n",
              " [109, 1654],\n",
              " [109, 1654, 3],\n",
              " [109, 1654, 3, 69],\n",
              " [109, 1654, 3, 69, 1104],\n",
              " [109, 1654, 3, 69, 1104, 2],\n",
              " [109, 1654, 3, 69, 1104, 2, 1949],\n",
              " [109, 1654, 3, 69, 1104, 2, 1949, 1428],\n",
              " [8, 2526],\n",
              " [8, 2526, 524],\n",
              " [8, 2526, 524, 6],\n",
              " [8, 2526, 524, 6, 1424],\n",
              " [8, 2526, 524, 6, 1424, 17],\n",
              " [8, 2526, 524, 6, 1424, 17, 2],\n",
              " [1645, 1646],\n",
              " [67, 2527],\n",
              " [67, 2527, 99],\n",
              " [67, 2527, 99, 99],\n",
              " [67, 2527, 99, 99, 640],\n",
              " [67, 2527, 99, 99, 640, 450],\n",
              " [67, 2527, 99, 99, 640, 450, 99],\n",
              " [67, 2527, 99, 99, 640, 450, 99, 11],\n",
              " [67, 2527, 99, 99, 640, 450, 99, 11, 24],\n",
              " [67, 2527, 99, 99, 640, 450, 99, 11, 24, 103],\n",
              " [67, 2527, 99, 99, 640, 450, 99, 11, 24, 103, 3],\n",
              " [69, 51],\n",
              " [69, 51, 1104],\n",
              " [69, 51, 1104, 16],\n",
              " [69, 51, 1104, 16, 318],\n",
              " [15, 25],\n",
              " [15, 25, 77],\n",
              " [15, 25, 77, 501],\n",
              " [15, 25, 77, 501, 450],\n",
              " [15, 25, 77, 501, 450, 99],\n",
              " [15, 25, 77, 501, 450, 99, 1],\n",
              " [15, 25, 77, 501, 450, 99, 1, 450],\n",
              " [15, 25, 77, 501, 450, 99, 1, 450, 99],\n",
              " [15, 25, 77, 501, 450, 99, 1, 450, 99, 15],\n",
              " [15, 25, 77, 501, 450, 99, 1, 450, 99, 15, 13],\n",
              " [451, 29],\n",
              " [451, 29, 1655],\n",
              " [451, 29, 1655, 1953],\n",
              " [451, 29, 1655, 1953, 44],\n",
              " [1, 99],\n",
              " [1, 99, 640],\n",
              " [1, 99, 640, 13],\n",
              " [1, 99, 640, 13, 48],\n",
              " [95, 99],\n",
              " [95, 99, 1104],\n",
              " [95, 99, 1104, 99],\n",
              " [95, 99, 1104, 99, 640],\n",
              " [95, 99, 1104, 99, 640, 29],\n",
              " [95, 99, 1104, 99, 640, 29, 52],\n",
              " [95, 99, 1104, 99, 640, 29, 52, 2],\n",
              " [95, 99, 1104, 99, 640, 29, 52, 2, 46],\n",
              " [95, 99, 1104, 99, 640, 29, 52, 2, 46, 16],\n",
              " [609, 1423],\n",
              " [609, 1423, 4],\n",
              " [609, 1423, 4, 641],\n",
              " [609, 1423, 4, 641, 99],\n",
              " [20, 11],\n",
              " [20, 11, 24],\n",
              " [20, 11, 24, 103],\n",
              " [20, 11, 24, 103, 3],\n",
              " [20, 11, 24, 103, 3, 2528],\n",
              " [20, 11, 24, 103, 3, 2528, 126],\n",
              " [20, 11, 24, 103, 3, 2528, 126, 1242],\n",
              " [20, 11, 24, 103, 3, 2528, 126, 1242, 8],\n",
              " [20, 11, 24, 103, 3, 2528, 126, 1242, 8, 450],\n",
              " [20, 11, 24, 103, 3, 2528, 126, 1242, 8, 450, 99],\n",
              " [5, 1],\n",
              " [5, 1, 93],\n",
              " [5, 1, 93, 379],\n",
              " [66, 42],\n",
              " [1, 23],\n",
              " [1, 23, 278],\n",
              " [1, 23, 278, 143],\n",
              " [1, 23, 278, 143, 29],\n",
              " [1, 23, 278, 143, 29, 24],\n",
              " [1, 23, 278, 143, 29, 24, 103],\n",
              " [1, 23, 278, 143, 29, 24, 103, 1656],\n",
              " [1, 23, 278, 143, 29, 24, 103, 1656, 5],\n",
              " [1, 23, 278, 143, 29, 24, 103, 1656, 5, 38],\n",
              " [319, 58],\n",
              " [319, 58, 151],\n",
              " [333, 6],\n",
              " [550, 14],\n",
              " [5, 20],\n",
              " [5, 20, 38],\n",
              " [5, 20, 38, 9],\n",
              " [5, 20, 38, 9, 29],\n",
              " [5, 20, 38, 9, 29, 244],\n",
              " [422, 14],\n",
              " [1657, 14],\n",
              " [1657, 14, 16],\n",
              " [1657, 14, 16, 136],\n",
              " [1954, 58],\n",
              " [1954, 58, 16],\n",
              " [1954, 58, 16, 1658],\n",
              " [333, 14],\n",
              " [333, 14, 679],\n",
              " [333, 14, 679, 184],\n",
              " [333, 14, 679, 184, 254],\n",
              " [849, 1001],\n",
              " [849, 1001, 315],\n",
              " [568, 502],\n",
              " [568, 502, 16],\n",
              " [568, 502, 16, 402],\n",
              " [333, 569],\n",
              " [333, 787],\n",
              " [333, 787, 16],\n",
              " [333, 787, 16, 642],\n",
              " [5, 20],\n",
              " [5, 20, 38],\n",
              " [5, 20, 38, 9],\n",
              " [5, 20, 38, 9, 29],\n",
              " [5, 20, 38, 9, 29, 850],\n",
              " [5, 20, 38, 9, 29, 850, 239],\n",
              " [5, 20, 38, 9, 29, 850, 239, 1105],\n",
              " [5, 20, 38, 9, 29, 850, 239, 1105, 19],\n",
              " [320, 6],\n",
              " [320, 6, 502],\n",
              " [320, 6, 502, 1002],\n",
              " [320, 6, 502, 1002, 1],\n",
              " [191, 244],\n",
              " [191, 244, 1],\n",
              " [191, 244, 1, 2529],\n",
              " [191, 244, 1, 2529, 4],\n",
              " [191, 244, 1, 2529, 4, 1955],\n",
              " [191, 244, 1, 2529, 4, 1955, 1429],\n",
              " [568, 502],\n",
              " [568, 502, 6],\n",
              " [568, 502, 6, 58],\n",
              " [568, 502, 6, 58, 1659],\n",
              " [51, 4],\n",
              " [51, 4, 94],\n",
              " [51, 4, 94, 210],\n",
              " [51, 4, 94, 210, 11],\n",
              " [51, 4, 94, 210, 11, 24],\n",
              " [51, 4, 94, 210, 11, 24, 279],\n",
              " [51, 4, 94, 210, 11, 24, 279, 437],\n",
              " [51, 4, 94, 210, 11, 24, 279, 437, 8],\n",
              " [51, 4, 94, 210, 11, 24, 279, 437, 8, 724],\n",
              " [255, 58],\n",
              " [255, 58, 192],\n",
              " [255, 58, 192, 851],\n",
              " [255, 58, 192, 851, 1430],\n",
              " [852, 151],\n",
              " [852, 151, 6],\n",
              " [852, 151, 6, 58],\n",
              " [852, 151, 6, 58, 1431],\n",
              " [422, 14],\n",
              " [422, 7],\n",
              " [422, 7, 2],\n",
              " [422, 7, 2, 1660],\n",
              " [422, 7, 2, 1660, 8],\n",
              " [422, 7, 2, 1660, 8, 849],\n",
              " [422, 7, 2, 1660, 8, 849, 1956],\n",
              " [422, 7, 2, 1660, 8, 849, 1956, 17],\n",
              " [422, 7, 2, 1660, 8, 849, 1956, 17, 2],\n",
              " [19, 1243],\n",
              " [19, 1243, 199],\n",
              " [19, 1243, 199, 16],\n",
              " [19, 1243, 199, 16, 1],\n",
              " [19, 1243, 199, 16, 1, 294],\n",
              " [19, 1243, 199, 16, 1, 294, 8],\n",
              " [110, 1],\n",
              " [110, 1, 294],\n",
              " [110, 1, 294, 4],\n",
              " [110, 1, 294, 4, 1661],\n",
              " [110, 1, 294, 4, 1661, 7],\n",
              " [110, 1, 294, 4, 1661, 7, 2530],\n",
              " [110, 1, 294, 4, 1661, 7, 2530, 6],\n",
              " [110, 1, 294, 4, 1661, 7, 2530, 6, 2],\n",
              " [110, 1, 294, 4, 1661, 7, 2530, 6, 2, 286],\n",
              " [422, 403],\n",
              " [422, 403, 921],\n",
              " [422, 403, 921, 15],\n",
              " [422, 403, 921, 15, 1],\n",
              " [1432, 1244],\n",
              " [1432, 1244, 11],\n",
              " [1432, 1244, 11, 24],\n",
              " [1432, 1244, 11, 24, 999],\n",
              " [1432, 1244, 11, 24, 999, 422],\n",
              " [1432, 1244, 11, 24, 999, 422, 7],\n",
              " [1432, 1244, 11, 24, 999, 422, 7, 138],\n",
              " [1432, 1244, 11, 24, 999, 422, 7, 138, 1662],\n",
              " [103, 36],\n",
              " [103, 36, 851],\n",
              " [103, 36, 851, 2531],\n",
              " [103, 36, 851, 2531, 8],\n",
              " [103, 36, 851, 2531, 8, 1430],\n",
              " [14, 148],\n",
              " [14, 148, 4],\n",
              " [14, 148, 4, 570],\n",
              " [14, 148, 4, 570, 51],\n",
              " [14, 148, 4, 570, 51, 680],\n",
              " [14, 148, 4, 570, 51, 680, 4],\n",
              " [14, 148, 4, 570, 51, 680, 4, 2],\n",
              " [14, 148, 4, 570, 51, 680, 4, 2, 19],\n",
              " [14, 148, 4, 570, 51, 680, 4, 2, 19, 2],\n",
              " [851, 1957],\n",
              " [851, 1957, 11],\n",
              " [851, 1957, 11, 681],\n",
              " [851, 1957, 11, 681, 117],\n",
              " [851, 1957, 11, 681, 117, 1],\n",
              " [851, 1957, 11, 681, 117, 1, 2532],\n",
              " [851, 1957, 11, 681, 117, 1, 2532, 1663],\n",
              " [1664, 1],\n",
              " [1664, 1, 844],\n",
              " [1664, 1, 844, 4],\n",
              " [1664, 1, 844, 4, 211],\n",
              " [1664, 1, 844, 4, 211, 182],\n",
              " [1664, 1, 844, 4, 211, 182, 1106],\n",
              " [1664, 1, 844, 4, 211, 182, 1106, 2533],\n",
              " [333, 6],\n",
              " [333, 6, 550],\n",
              " [333, 6, 550, 14],\n",
              " [101, 4],\n",
              " [101, 4, 1],\n",
              " [101, 4, 1, 138],\n",
              " [101, 4, 1, 138, 293],\n",
              " [101, 4, 1, 138, 293, 422],\n",
              " [101, 4, 1, 138, 293, 422, 725],\n",
              " [101, 4, 1, 138, 293, 422, 725, 7],\n",
              " [101, 4, 1, 138, 293, 422, 725, 7, 1],\n",
              " [1245, 422],\n",
              " [1245, 422, 403],\n",
              " [1245, 422, 403, 36],\n",
              " [1245, 422, 403, 36, 2535],\n",
              " [1245, 10],\n",
              " [1245, 10, 7],\n",
              " [1245, 10, 7, 1246],\n",
              " [1245, 10, 7, 1246, 3],\n",
              " [1245, 10, 7, 1246, 3, 452],\n",
              " [1245, 10, 7, 1246, 3, 452, 6],\n",
              " [1245, 10, 7, 1246, 3, 452, 6, 144],\n",
              " [1245, 10, 7, 1246, 3, 452, 6, 144, 610],\n",
              " [390, 726],\n",
              " [390, 726, 4],\n",
              " [390, 726, 4, 608],\n",
              " [390, 726, 4, 608, 14],\n",
              " [390, 726, 4, 608, 14, 6],\n",
              " [390, 726, 4, 608, 14, 6, 150],\n",
              " [477, 5],\n",
              " [477, 5, 23],\n",
              " [477, 5, 23, 29],\n",
              " [477, 5, 23, 29, 24],\n",
              " [477, 5, 23, 29, 24, 234],\n",
              " [477, 5, 23, 29, 24, 234, 105],\n",
              " [1, 643],\n",
              " [1, 643, 294],\n",
              " [1, 643, 294, 7],\n",
              " [1, 643, 294, 7, 61],\n",
              " [1, 643, 294, 7, 61, 465],\n",
              " [1, 643, 294, 7, 61, 465, 2],\n",
              " [1, 643, 294, 7, 61, 465, 2, 1247],\n",
              " [1, 643, 294, 7, 61, 465, 2, 1247, 19],\n",
              " [1, 643, 294, 7, 61, 465, 2, 1247, 19, 8],\n",
              " [110, 1],\n",
              " [294, 4],\n",
              " [294, 4, 2536],\n",
              " [294, 4, 2536, 7],\n",
              " [294, 4, 2536, 7, 2537],\n",
              " [294, 4, 2536, 7, 2537, 20],\n",
              " [294, 4, 2536, 7, 2537, 20, 7],\n",
              " [294, 4, 2536, 7, 2537, 20, 7, 2],\n",
              " [294, 4, 2536, 7, 2537, 20, 7, 2, 49],\n",
              " [61, 2],\n",
              " [61, 2, 1958],\n",
              " [33, 3],\n",
              " [33, 3, 55],\n",
              " [33, 3, 55, 10],\n",
              " [23, 423],\n",
              " [23, 423, 16],\n",
              " [23, 423, 16, 37],\n",
              " [23, 423, 16, 37, 1433],\n",
              " [23, 423, 16, 37, 1433, 4],\n",
              " [23, 423, 16, 37, 1433, 4, 1],\n",
              " [23, 423, 16, 37, 1433, 4, 1, 1245],\n",
              " [422, 403],\n",
              " [422, 403, 40],\n",
              " [422, 403, 40, 7],\n",
              " [422, 403, 40, 7, 279],\n",
              " [422, 403, 40, 7, 279, 551],\n",
              " [3, 48],\n",
              " [3, 48, 606],\n",
              " [3, 48, 606, 1434],\n",
              " [3, 48, 606, 1434, 1],\n",
              " [3, 48, 606, 1434, 1, 788],\n",
              " [3, 48, 606, 1434, 1, 788, 82],\n",
              " [6, 188],\n",
              " [6, 188, 1],\n",
              " [6, 188, 1, 294],\n",
              " [6, 188, 1, 294, 76],\n",
              " [6, 188, 1, 294, 76, 16],\n",
              " [6, 188, 1, 294, 76, 16, 1],\n",
              " [19, 13],\n",
              " [19, 13, 120],\n",
              " [19, 13, 120, 3],\n",
              " [19, 13, 120, 3, 294],\n",
              " [17, 23],\n",
              " [17, 23, 294],\n",
              " [17, 23, 294, 31],\n",
              " [17, 23, 294, 31, 788],\n",
              " [503, 788],\n",
              " [503, 294],\n",
              " [503, 294, 1107],\n",
              " [503, 294],\n",
              " [503, 294, 1248],\n",
              " [33, 10],\n",
              " [33, 10, 107],\n",
              " [1, 788],\n",
              " [1, 788, 921],\n",
              " [1, 788, 921, 2],\n",
              " [1, 788, 921, 2, 159],\n",
              " [1, 788, 921, 2, 159, 4],\n",
              " [1, 788, 921, 2, 159, 4, 184],\n",
              " [1, 788, 921, 2, 159, 4, 184, 19],\n",
              " [680, 6],\n",
              " [680, 6, 726],\n",
              " [680, 6, 726, 6],\n",
              " [680, 6, 726, 6, 155],\n",
              " [15, 1665],\n",
              " [15, 1665, 3],\n",
              " [15, 1665, 3, 362],\n",
              " [15, 1665, 3, 362, 126],\n",
              " [15, 1665, 3, 362, 126, 1959],\n",
              " [15, 1665, 3, 362, 126, 1959, 19],\n",
              " [15, 1665, 3, 362, 126, 1959, 19, 3],\n",
              " [15, 1665, 3, 362, 126, 1959, 19, 3, 2],\n",
              " [15, 1665, 3, 362, 126, 1959, 19, 3, 2, 478],\n",
              " [294, 571],\n",
              " [294, 571, 2],\n",
              " [294, 571, 2, 1960],\n",
              " [294, 571, 2, 1960, 4],\n",
              " [294, 571, 2, 1960, 4, 2539],\n",
              " [294, 571, 2, 1960, 4, 2539, 1],\n",
              " [643, 294],\n",
              " [643, 294, 7],\n",
              " [643, 294, 7, 438],\n",
              " [643, 294, 7, 438, 2],\n",
              " [643, 294, 7, 438, 2, 1249],\n",
              " [643, 294, 7, 438, 2, 1249, 19],\n",
              " [643, 294, 7, 438, 2, 1249, 19, 45],\n",
              " [643, 294, 7, 438, 2, 1249, 19, 45, 77],\n",
              " [643, 294, 7, 438, 2, 1249, 19, 45, 77, 501],\n",
              " [643, 294, 7, 438, 2, 1249, 19, 45, 77, 501, 2],\n",
              " [293, 266],\n",
              " [293, 266, 4],\n",
              " [293, 266, 4, 1],\n",
              " [293, 266, 4, 1, 19],\n",
              " [293, 266, 4, 1, 19, 15],\n",
              " [293, 266, 4, 1, 19, 15, 152],\n",
              " [293, 266, 4, 1, 19, 15, 152, 1],\n",
              " [130, 853],\n",
              " [130, 853, 682],\n",
              " [115, 43],\n",
              " [149, 25],\n",
              " [149, 25, 109],\n",
              " [149, 25, 109, 422],\n",
              " [149, 25, 109, 422, 725],\n",
              " [149, 25, 109, 422, 725, 525],\n",
              " [149, 25, 109, 422, 725, 525, 149],\n",
              " [149, 25, 109, 422, 725, 525, 149, 1652],\n",
              " [1, 1245],\n",
              " [1, 1245, 422],\n",
              " [1, 1245, 422, 403],\n",
              " [1, 1245, 422, 403, 139],\n",
              " [18, 1],\n",
              " [18, 1, 1961],\n",
              " [18, 1, 1961, 422],\n",
              " [18, 1, 1961, 422, 403],\n",
              " [18, 1, 1961, 422, 403, 2540],\n",
              " [18, 1, 1961, 422, 403, 2540, 77],\n",
              " [1961, 2541],\n",
              " [1961, 2541, 23],\n",
              " [1961, 2541, 23, 1962],\n",
              " [10, 18],\n",
              " [10, 18, 1],\n",
              " [10, 18, 1, 1108],\n",
              " [10, 18, 1, 1108, 82],\n",
              " [10, 18, 1, 1108, 82, 77],\n",
              " [10, 18, 1, 1108, 82, 77, 1],\n",
              " [10, 18, 1, 1108, 82, 77, 1, 121],\n",
              " [10, 18, 1, 1108, 82, 77, 1, 121, 4],\n",
              " [1250, 149],\n",
              " [1250, 149, 7],\n",
              " [1250, 149, 7, 175],\n",
              " [1250, 149, 7, 175, 2542],\n",
              " [1250, 149, 7, 175, 2542, 2543],\n",
              " [1963, 1],\n",
              " [1963, 1, 2544],\n",
              " [1963, 1, 2544, 4],\n",
              " [1963, 1, 2544, 4, 101],\n",
              " [1963, 1, 2544, 4, 101, 403],\n",
              " [1963, 1, 2544, 4, 101, 403, 239],\n",
              " [1, 109],\n",
              " [1, 109, 526],\n",
              " [1, 109, 526, 1245],\n",
              " [1, 109, 526, 1245, 422],\n",
              " [7, 1003],\n",
              " [7, 1003, 1],\n",
              " [7, 1003, 1, 131],\n",
              " [7, 1003, 1, 131, 1242],\n",
              " [38, 95],\n",
              " [51, 1],\n",
              " [51, 1, 2546],\n",
              " [51, 1, 2546, 234],\n",
              " [51, 1, 2546, 234, 105],\n",
              " [51, 1, 2546, 234, 105, 1004],\n",
              " [51, 1, 2546, 234, 105, 1004, 17],\n",
              " [51, 1, 2546, 234, 105, 1004, 17, 1],\n",
              " [2547, 789],\n",
              " [2547, 789, 40],\n",
              " [2547, 789, 40, 683],\n",
              " [2547, 789, 40, 683, 1],\n",
              " [294, 76],\n",
              " [294, 76, 1],\n",
              " [294, 76, 1, 132],\n",
              " [294, 76, 1, 132, 7],\n",
              " [294, 76, 1, 132, 7, 37],\n",
              " [294, 76, 1, 132, 7, 37, 684],\n",
              " [350, 854],\n",
              " [350, 854, 20],\n",
              " [1, 1108],\n",
              " [1, 1108, 218],\n",
              " [1, 1108, 218, 236],\n",
              " [1, 1108, 218, 236, 158],\n",
              " [1, 1108, 218, 236, 158, 1],\n",
              " [788, 72],\n",
              " [788, 72, 11],\n",
              " [788, 72, 11, 572],\n",
              " [788, 72, 11, 572, 790],\n",
              " [321, 304],\n",
              " [321, 304, 10],\n",
              " [321, 304, 10, 7],\n",
              " [321, 304, 10, 7, 390],\n",
              " [321, 304, 10, 7, 390, 3],\n",
              " [321, 304, 10, 7, 390, 3, 24],\n",
              " [321, 304, 10, 7, 390, 3, 24, 790],\n",
              " [321, 304, 10, 7, 390, 3, 24, 790, 43],\n",
              " [2548, 162],\n",
              " [2548, 162, 1],\n",
              " [2548, 162, 1, 788],\n",
              " [17, 23],\n",
              " [17, 23, 294],\n",
              " [17, 23, 294, 31],\n",
              " [17, 23, 294, 31, 1108],\n",
              " [503, 1108],\n",
              " [503, 294],\n",
              " [503, 294, 1107],\n",
              " [503, 294],\n",
              " [503, 294, 1248],\n",
              " [13, 11],\n",
              " [13, 11, 42],\n",
              " [13, 11, 42, 685],\n",
              " [13, 11, 42, 685, 126],\n",
              " [13, 11, 42, 685, 126, 305],\n",
              " [13, 11, 42, 685, 126, 305, 503],\n",
              " [13, 11, 42, 685, 126, 305, 503, 39],\n",
              " [13, 11, 42, 685, 126, 305, 503, 39, 1],\n",
              " [1251, 10],\n",
              " [1251, 10, 212],\n",
              " [1251, 10, 212, 2],\n",
              " [1251, 10, 212, 2, 166],\n",
              " [1251, 10, 212, 2, 166, 184],\n",
              " [391, 439],\n",
              " [391, 439, 2549],\n",
              " [391, 439, 2549, 45],\n",
              " [391, 439, 2549, 45, 18],\n",
              " [391, 439, 2549, 45, 18, 2],\n",
              " [391, 439, 2549, 45, 18, 2, 200],\n",
              " [391, 439, 2549, 45, 18, 2, 200, 6],\n",
              " [391, 439, 2549, 45, 18, 2, 200, 6, 29],\n",
              " [452, 160],\n",
              " [452, 160, 1964],\n",
              " [452, 160, 1964, 45],\n",
              " [452, 160, 1964, 45, 1244],\n",
              " [452, 160, 1964, 45, 1244, 15],\n",
              " [452, 160, 1964, 45, 1244, 15, 527],\n",
              " [17, 23],\n",
              " [17, 23, 294],\n",
              " [17, 23, 294, 31],\n",
              " [17, 23, 294, 31, 1251],\n",
              " [503, 1251],\n",
              " [503, 1251, 2550],\n",
              " [503, 294],\n",
              " [503, 294, 1107],\n",
              " [503, 294],\n",
              " [503, 294, 1248],\n",
              " [503, 294],\n",
              " [503, 294, 2551],\n",
              " [2, 1251],\n",
              " [2, 1251, 163],\n",
              " [2, 1251, 163, 117],\n",
              " [2, 1251, 163, 117, 24],\n",
              " [2, 1251, 163, 117, 24, 103],\n",
              " [2, 1251, 163, 117, 24, 103, 5],\n",
              " [2, 1251, 163, 117, 24, 103, 5, 279],\n",
              " [2, 1251, 163, 117, 24, 103, 5, 279, 380],\n",
              " [727, 15],\n",
              " [727, 15, 25],\n",
              " [727, 15, 25, 61],\n",
              " [727, 15, 25, 61, 234],\n",
              " [727, 15, 25, 61, 234, 36],\n",
              " [727, 15, 25, 61, 234, 36, 1],\n",
              " [788, 45],\n",
              " [788, 45, 1108],\n",
              " [333, 6],\n",
              " [333, 6, 550],\n",
              " [333, 6, 550, 14],\n",
              " [334, 5],\n",
              " [334, 5, 23],\n",
              " [334, 5, 23, 95],\n",
              " [334, 5, 23, 95, 1666],\n",
              " [334, 5, 23, 95, 1666, 7],\n",
              " [334, 5, 23, 95, 1666, 7, 1],\n",
              " [334, 5, 23, 95, 1666, 7, 1, 1109],\n",
              " [334, 5, 23, 95, 1666, 7, 1, 1109, 40],\n",
              " [1253, 1667],\n",
              " [1253, 1667, 528],\n",
              " [1253, 1667, 528, 608],\n",
              " [1253, 1667, 528, 608, 686],\n",
              " [3, 48],\n",
              " [3, 48, 10],\n",
              " [3, 48, 10, 13],\n",
              " [3, 48, 10, 13, 104],\n",
              " [3, 48, 10, 13, 104, 37],\n",
              " [3, 48, 10, 13, 104, 37, 205],\n",
              " [3, 48, 10, 13, 104, 37, 205, 16],\n",
              " [3, 48, 10, 13, 104, 37, 205, 16, 1],\n",
              " [3, 48, 10, 13, 104, 37, 205, 16, 1, 100],\n",
              " [3, 48, 10, 13, 104, 37, 205, 16, 1, 100, 4],\n",
              " [1, 335],\n",
              " [1, 335, 13],\n",
              " [1, 335, 13, 25],\n",
              " [1, 335, 13, 25, 39],\n",
              " [1, 335, 13, 25, 39, 6],\n",
              " [1, 335, 13, 25, 39, 6, 54],\n",
              " [1, 335, 13, 25, 39, 6, 54, 188],\n",
              " [1, 294],\n",
              " [1, 294, 76],\n",
              " [1, 294, 76, 440],\n",
              " [1, 294, 76, 440, 7],\n",
              " [1, 294, 76, 440, 7, 2],\n",
              " [1, 294, 76, 440, 7, 2, 46],\n",
              " [1, 294, 76, 440, 7, 2, 46, 4],\n",
              " [1, 294, 76, 440, 7, 2, 46, 4, 51],\n",
              " [1, 294, 76, 440, 7, 2, 46, 4, 51, 1],\n",
              " [1668, 686],\n",
              " [1668, 686, 6],\n",
              " [1668, 686, 6, 37],\n",
              " [1668, 686, 6, 37, 110],\n",
              " [1668, 686, 6, 37, 110, 39],\n",
              " [1668, 686, 6, 37, 110, 39, 1],\n",
              " [1669, 1109],\n",
              " [17, 23],\n",
              " [17, 23, 294],\n",
              " [17, 23, 294, 31],\n",
              " [17, 23, 294, 31, 1109],\n",
              " [1109, 686],\n",
              " [2553, 2554],\n",
              " [2553, 2554, 2555],\n",
              " [2553, 2554, 2555, 1965],\n",
              " [2553, 2554, 2555, 1965, 1436],\n",
              " [1966, 2557],\n",
              " [1966, 2557, 1967],\n",
              " [1966, 2557, 1967, 2558],\n",
              " [1110, 2559],\n",
              " [1669, 503],\n",
              " [1669, 503, 1109],\n",
              " [1669, 503, 1109, 1110],\n",
              " [1669, 503],\n",
              " [1669, 503, 294],\n",
              " [1669, 503, 294, 2560],\n",
              " [66, 42],\n",
              " [5, 1],\n",
              " [5, 1, 105],\n",
              " [5, 1, 105, 41],\n",
              " [5, 1, 105, 41, 9],\n",
              " [5, 1, 105, 41, 9, 29],\n",
              " [5, 1, 105, 41, 9, 29, 244],\n",
              " [5, 1, 105, 41, 9, 29, 244, 1005],\n",
              " [40, 7],\n",
              " [40, 7, 504],\n",
              " [40, 7, 504, 552],\n",
              " [40, 7, 504, 552, 3],\n",
              " [40, 7, 504, 552, 3, 422],\n",
              " [40, 7, 504, 552, 3, 422, 72],\n",
              " [2562, 321],\n",
              " [1657, 14],\n",
              " [1657, 14, 16],\n",
              " [1657, 14, 16, 136],\n",
              " [1005, 7],\n",
              " [1005, 7, 279],\n",
              " [1005, 7, 279, 552],\n",
              " [1005, 7, 279, 552, 3],\n",
              " [1005, 7, 279, 552, 3, 422],\n",
              " [1005, 7, 279, 552, 3, 422, 72],\n",
              " [1005, 7, 279, 552, 3, 422, 72, 7],\n",
              " [43, 2563],\n",
              " [43, 2563, 3],\n",
              " [43, 2563, 3, 728],\n",
              " [43, 2563, 3, 728, 320],\n",
              " [43, 2563, 3, 728, 320, 2],\n",
              " [729, 7],\n",
              " [729, 7, 2],\n",
              " [729, 7, 2, 853],\n",
              " [729, 7, 2, 853, 19],\n",
              " [729, 7, 2, 853, 19, 18],\n",
              " [729, 7, 2, 853, 19, 18, 1670],\n",
              " [729, 7, 2, 853, 19, 18, 1670, 3],\n",
              " [729, 7, 2, 853, 19, 18, 1670, 3, 1],\n",
              " [729, 7, 2, 853, 19, 18, 1670, 3, 1, 853],\n",
              " [729, 7, 2, 853, 19, 18, 1670, 3, 1, 853, 294],\n",
              " [729, 7, 2, 853, 19, 18, 1670, 3, 1, 853, 294, 78],\n",
              " [1111, 422],\n",
              " [1111, 422, 13],\n",
              " [1111, 422, 13, 25],\n",
              " [1111, 422, 13, 25, 465],\n",
              " [573, 16],\n",
              " [573, 16, 2],\n",
              " [573, 16, 2, 1247],\n",
              " [573, 16, 2, 1247, 19],\n",
              " [573, 16, 2, 1247, 19, 40],\n",
              " [573, 16, 2, 1247, 19, 40, 322],\n",
              " [573, 16, 2, 1247, 19, 40, 322, 1],\n",
              " [573, 16, 2, 1247, 19, 40, 322, 1, 130],\n",
              " [573, 16, 2, 1247, 19, 40, 322, 1, 130, 922],\n",
              " [72, 1],\n",
              " [72, 1, 19],\n",
              " [72, 1, 19, 13],\n",
              " [72, 1, 19, 13, 213],\n",
              " [72, 1, 19, 13, 213, 199],\n",
              " [72, 1, 19, 13, 213, 199, 16],\n",
              " [72, 1, 19, 13, 213, 199, 16, 11],\n",
              " [72, 1, 19, 13, 213, 199, 16, 11, 24],\n",
              " [2564, 321],\n",
              " [2564, 321, 2],\n",
              " [2564, 321, 2, 574],\n",
              " [2564, 321, 2, 574, 855],\n",
              " [2564, 321, 2, 574, 855, 29],\n",
              " [2564, 321, 2, 574, 855, 29, 2565],\n",
              " [193, 222],\n",
              " [24, 730],\n",
              " [24, 730, 13],\n",
              " [24, 730, 13, 56],\n",
              " [24, 730, 13, 56, 2566],\n",
              " [24, 730, 13, 56, 2566, 1],\n",
              " [24, 730, 13, 56, 2566, 1, 136],\n",
              " [24, 730, 13, 56, 2566, 1, 136, 28],\n",
              " [24, 730, 13, 56, 2566, 1, 136, 28, 5],\n",
              " [23, 68],\n",
              " [23, 68, 173],\n",
              " [23, 68, 173, 136],\n",
              " [23, 68, 173, 136, 20],\n",
              " [23, 68, 173, 136, 20, 29],\n",
              " [1112, 1],\n",
              " [1112, 1, 1113],\n",
              " [1112, 1, 1113, 3],\n",
              " [1112, 1, 1113, 3, 529],\n",
              " [1112, 1, 1113, 3, 529, 136],\n",
              " [1112, 1, 1113, 3, 529, 136, 13],\n",
              " [163, 42],\n",
              " [163, 42, 24],\n",
              " [163, 42, 24, 1437],\n",
              " [163, 42, 24, 1437, 1671],\n",
              " [16, 1],\n",
              " [16, 1, 60],\n",
              " [16, 1, 60, 4],\n",
              " [16, 1, 60, 4, 70],\n",
              " [16, 1, 60, 4, 70, 73],\n",
              " [16, 1, 60, 4, 70, 73, 234],\n",
              " [16, 1, 60, 4, 70, 73, 234, 5],\n",
              " [16, 1, 60, 4, 70, 73, 234, 5, 1],\n",
              " [16, 1, 60, 4, 70, 73, 234, 5, 1, 378],\n",
              " [199, 575],\n",
              " [199, 575, 8],\n",
              " [199, 575, 8, 2],\n",
              " [199, 575, 8, 2, 19],\n",
              " [199, 575, 8, 2, 19, 5],\n",
              " [199, 575, 8, 2, 19, 5, 136],\n",
              " [199, 575, 8, 2, 19, 5, 136, 41],\n",
              " [199, 575, 8, 2, 19, 5, 136, 41, 4],\n",
              " [38, 67],\n",
              " [38, 67, 441],\n",
              " [38, 67, 441, 58],\n",
              " [38, 67, 441, 58, 6],\n",
              " [38, 67, 441, 58, 6, 136],\n",
              " [38, 67, 441, 58, 6, 136, 404],\n",
              " [33, 3],\n",
              " [33, 3, 55],\n",
              " [33, 3, 55, 10],\n",
              " [9, 29],\n",
              " [9, 29, 48],\n",
              " [9, 29, 48, 1],\n",
              " [9, 29, 48, 1, 1113],\n",
              " [9, 29, 48, 1, 1113, 3],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "aUNVfHHeAYkw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')"
      ],
      "metadata": {
        "id": "pLJML_oxAiAN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6loolQW2Ak0z",
        "outputId": "06c9057c-75f6-4c40-f563-5e9bbdf0da43"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,  331, 2502],\n",
              "       [   0,    0,    0, ...,  331, 2502,   11],\n",
              "       [   0,    0,    0, ..., 2502,   11, 1421],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  578,    8,  176],\n",
              "       [   0,    0,    0, ...,    8,  176,   44],\n",
              "       [   0,    0,    0, ...,  176,   44,  185]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:,:-1]"
      ],
      "metadata": {
        "id": "ujv8KJU4AnES"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "RteAu74XApjY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bnhe8uIMArT4",
        "outputId": "fb530cf0-684f-4895-c590-9558166a529a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46354, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZZVZa6KBUL6",
        "outputId": "c209f1d4-7217-4373-8c6c-0c35aca45ce0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46354,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y,num_classes=3926)"
      ],
      "metadata": {
        "id": "PEZbHfRfBWkW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tiKDa6nBZdG",
        "outputId": "17b5dbfa-1056-4c35-96f8-97bb3cd1430c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46354, 3926)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "iTyNLbsEBwf7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(3926, 100, input_length=14))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(3926, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z18UzlcWBzG4",
        "outputId": "cb2dc55c-2344-4f90-9c1c-f9a475543d4c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jhMLxDnFCFGF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "wDx4ThrzCIFZ",
        "outputId": "bd47317b-7fa9-4666-eb4b-005cea1e5fb7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yCIBGeGCKrb",
        "outputId": "68549f57-d9d6-4982-c7e6-4adfdd445b6e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 38ms/step - accuracy: 0.0569 - loss: 6.6577\n",
            "Epoch 2/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 37ms/step - accuracy: 0.0984 - loss: 5.8207\n",
            "Epoch 3/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 37ms/step - accuracy: 0.1667 - loss: 5.0765\n",
            "Epoch 4/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 37ms/step - accuracy: 0.2105 - loss: 4.6011\n",
            "Epoch 5/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 37ms/step - accuracy: 0.2504 - loss: 4.2029\n",
            "Epoch 6/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 37ms/step - accuracy: 0.2788 - loss: 3.8995\n",
            "Epoch 7/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 37ms/step - accuracy: 0.3081 - loss: 3.6041\n",
            "Epoch 8/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 37ms/step - accuracy: 0.3366 - loss: 3.3526\n",
            "Epoch 9/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 38ms/step - accuracy: 0.3681 - loss: 3.1089\n",
            "Epoch 10/10\n",
            "\u001b[1m1449/1449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 37ms/step - accuracy: 0.3921 - loss: 2.9045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f6394c76500>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L5UEwE-CCVRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "text = \"what is nltk\"\n",
        "\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=56, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGeYGwCMfTus",
        "outputId": "0ab860aa-152f-4119-aff7-a79cb8d62a09"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
            "what is nltk as\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "what is nltk as a\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "what is nltk as a single\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "what is nltk as a single word\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "what is nltk as a single word is\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "what is nltk as a single word is a\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "what is nltk as a single word is a single\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "what is nltk as a single word is a single token\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "what is nltk as a single word is a single token of\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "what is nltk as a single word is a single token of a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_k6STvnFshq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}