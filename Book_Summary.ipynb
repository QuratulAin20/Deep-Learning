{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPX7apG2bq++qWBjw39Wfka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuratulAin20/Deep-Learning/blob/main/Book_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "-1CoeklCnlo9",
        "outputId": "f3ed9c17-36d9-4193-a146-c8692e45286e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ce8040b-b570-4cc9-9ce2-7bd96ffd4610\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ce8040b-b570-4cc9-9ce2-7bd96ffd4610\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving text_data.txt to text_data (1).txt\n",
            "Uploaded file: text_data (1).txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the book file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# If you want to check the names of uploaded files\n",
        "for filename in uploaded.keys():\n",
        "    print(f'Uploaded file: {filename}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers\n",
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install streamlit\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr1GBqh8J8dU",
        "outputId": "a4ce011c-da8d-452e-b41f-164b6955ce5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.39.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.1)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Extract text from PDF"
      ],
      "metadata": {
        "id": "JWm5ro0cLy62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_txt(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# Specify the path to your .txt file\n",
        "file_path = '/content/text_data.txt'\n",
        "text = extract_text_from_txt(file_path)"
      ],
      "metadata": {
        "id": "Opm_PSqLLON6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "p7_v96ezk8kh",
        "outputId": "fab39358-2ce3-45bf-be5e-482ac3fa32c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIntroduction\\nNLTK is the Natural Language Toolkit, a comprehensive Python library for natural language \\nprocessing and text analytics. Originally designed for teaching, it has been adopted in the \\nindustry for research and development due to its usefulness and breadth of coverage.\\nThis chapter will cover the basics of tokenizing text and using WordNet. Tokenization is a \\nmethod of breaking up a piece of text into many pieces, and is an essential irst step for \\nrecipes in later chapters.\\nTokenizing Text and WordNet Basics\\n\\nWordNet is a dictionary designed for programmatic access by natural language processing \\nsystems. NLTK includes a WordNet corpus reader, which we will use to access and explore \\nWordNet. We\\'ll be using WordNet again in later chapters, so it\\'s important to familiarize \\nyourself with the basics irst.\\nTokenizing text into sentences\\nTokenization is the process of splitting a string into a list of pieces, or tokens. We\\'ll start by \\nsplitting a paragraph into a list of sentences.\\nGetting ready\\nInstallation instructions for NLTK are available at http://www.nltk.org/download and \\nthe latest version as of this writing is 2.0b9. NLTK requires Python 2.4 or higher, but is not \\ncompatible with Python 3.0. The recommended Python version is 2.6.\\nOnce you\\'ve installed NLTK, you\\'ll also need to install the data by following the instructions \\nat http://www.nltk.org/data. We recommend installing everything, as we\\'ll be using \\na number of corpora and pickled objects. The data is installed in a data directory, which on \\nMac and Linux/Unix is usually /usr/share/nltk_data, or on Windows is C:\\\\nltk_data. \\nMake sure that tokenizers/punkt.zip is in the data directory and has been unpacked so \\nthat there\\'s a ile at tokenizers/punkt/english.pickle.\\nFinally, to run the code examples, you\\'ll need to start a Python console. Instructions on  \\nhow to do so are available at http://www.nltk.org/getting-started. For Mac \\nwith Linux/Unix users, you can open a terminal and type python.\\nHow to do it...\\nOnce NLTK is installed and you have a Python console running, we can start by creating a \\nparagraph of text:\\n>>> para = \"Hello World. It\\'s good to see you. Thanks for buying this \\nbook.\"\\nNow we want to split para into sentences. First we need to import the sentence tokenization \\nfunction, and then we can call it with the paragraph as an argument.\\n>>> from nltk.tokenize import sent_tokenize\\n>>> sent_tokenize(para)\\n[\\'Hello World.\\', \"It\\'s good to see you.\", \\'Thanks for buying this \\nbook.\\']\\nSo now we have a list of sentences that we can use for further processing.\\nChapter 1\\n9\\nHow it works...\\nsent_tokenize uses an instance of PunktSentenceTokenizer from the nltk.\\ntokenize.punkt module. This instance has already been trained on and works well for \\nmany European languages. So it knows what punctuation and characters mark the end of a \\nsentence and the beginning of a new sentence.\\nThere\\'s more...\\nThe instance used in sent_tokenize() is actually loaded on demand from a pickle \\nile. So if you\\'re going to be tokenizing a lot of sentences, it\\'s more eficient to load the \\nPunktSentenceTokenizer once, and call its tokenize() method instead.\\n>>> import nltk.data\\n>>> tokenizer = nltk.data.load(\\'tokenizers/punkt/english.pickle\\')\\n>>> tokenizer.tokenize(para)\\n[\\'Hello World.\\', \"It\\'s good to see you.\", \\'Thanks for buying this \\nbook.\\']\\nOther languages\\nIf you want to tokenize sentences in languages other than English, you can load one of the \\nother pickle iles in tokenizers/punkt and use it just like the English sentence tokenizer. \\nHere\\'s an example for Spanish:\\n>>> spanish_tokenizer = nltk.data.load(\\'tokenizers/punkt/spanish.\\npickle\\')\\n>>> spanish_tokenizer.tokenize(\\'Hola amigo. Estoy bien.\\')\\nSee also\\nIn the next recipe, we\\'ll learn how to split sentences into individual words. After that, we\\'ll \\ncover how to use regular expressions for tokenizing text.\\nTokenizing sentences into words\\nIn this recipe, we\\'ll split a sentence into individual words. The simple task of creating a list of \\nwords from a string is an essential part of all text processing.\\nwww.allitebooks.com\\nTokenizing Text and WordNet Basics\\n10\\nHow to do it...\\nBasic word tokenization is very simple: use the word_tokenize() function:\\n>>> from nltk.tokenize import word_tokenize\\n>>> word_tokenize(\\'Hello World.\\')\\n[\\'Hello\\', \\'World\\', \\'.\\']\\nHow it works...\\nword_tokenize() is a wrapper function that calls tokenize() on an instance of the \\nTreebankWordTokenizer. It\\'s equivalent to the following:\\n>>> from nltk.tokenize import TreebankWordTokenizer\\n>>> tokenizer = TreebankWordTokenizer()\\n>>> tokenizer.tokenize(\\'Hello World.\\')\\n[\\'Hello\\', \\'World\\', \\'.\\']\\nIt works by separating words using spaces and punctuation. And as you can see, it does not \\ndiscard the punctuation, allowing you to decide what to do with it.\\nThere\\'s more...\\nIgnoring the obviously named WhitespaceTokenizer and SpaceTokenizer, there are two \\nother word tokenizers worth looking at: PunktWordTokenizer and WordPunctTokenizer. \\nThese differ from the TreebankWordTokenizer by how they handle punctuation and \\ncontractions, but they all inherit from TokenizerI. The inheritance tree looks like this:\\nChapter 1\\n11\\nContractions\\nTreebankWordTokenizer uses conventions found in the Penn Treebank corpus, which we\\'ll \\nbe using for training in Chapter 4, Part-of-Speech Tagging and Chapter 5, Extracting Chunks. \\nOne of these conventions is to separate contractions. For example:\\n>>> word_tokenize(\"can\\'t\")\\n[\\'ca\\', \"n\\'t\"]\\nIf you ind this convention unacceptable, then read on for alternatives, and see the next recipe \\nfor tokenizing with regular expressions.\\nPunktWordTokenizer\\nAn alternative word tokenizer is the PunktWordTokenizer. It splits on punctuation, but \\nkeeps it with the word instead of creating separate tokens.\\n>>> from nltk.tokenize import PunktWordTokenizer\\n>>> tokenizer = PunktWordTokenizer()\\n>>> tokenizer.tokenize(\"Can\\'t is a contraction.\")\\n[\\'Can\\', \"\\'t\", \\'is\\', \\'a\\', \\'contraction.\\']\\nWordPunctTokenizer\\nAnother alternative word tokenizer is WordPunctTokenizer. It splits all punctuations into \\nseparate tokens.\\n>>> from nltk.tokenize import WordPunctTokenizer\\n>>> tokenizer = WordPunctTokenizer()\\n>>> tokenizer.tokenize(\"Can\\'t is a contraction.\")\\n[\\'Can\\', \"\\'\", \\'t\\', \\'is\\', \\'a\\', \\'contraction\\', \\'.\\']\\nSee also\\nFor more control over word tokenization, you\\'ll want to read the next recipe to learn how to use \\nregular expressions and the RegexpTokenizer for tokenization.\\nTokenizing sentences using regular \\nexpressions\\nRegular expression can be used if you want complete control over how to tokenize text. As \\nregular expressions can get complicated very quickly, we only recommend using them if the \\nword tokenizers covered in the previous recipe are unacceptable.\\nTokenizing Text and WordNet Basics\\n12\\nGetting ready\\nFirst you need to decide how you want to tokenize a piece of text, as this will determine how \\nyou construct your regular expression. The choices are:\\n \\nf\\nMatch on the tokens\\n \\nf\\nMatch on the separators, or gaps\\nWe\\'ll start with an example of the irst, matching alphanumeric tokens plus single quotes so \\nthat we don\\'t split up contractions.\\nHow to do it...\\nWe\\'ll create an instance of the RegexpTokenizer, giving it a regular expression string to \\nuse for matching tokens.\\n>>> from nltk.tokenize import RegexpTokenizer\\n>>> tokenizer = RegexpTokenizer(\"[\\\\w\\']+\")\\n>>> tokenizer.tokenize(\"Can\\'t is a contraction.\")\\n[\"Can\\'t\", \\'is\\', \\'a\\', \\'contraction\\']\\nThere\\'s also a simple helper function you can use in case you don\\'t want to instantiate  \\nthe class.\\n>>> from nltk.tokenize import regexp_tokenize\\n>>> regexp_tokenize(\"Can\\'t is a contraction.\", \"[\\\\w\\']+\")\\n[\"Can\\'t\", \\'is\\', \\'a\\', \\'contraction\\']\\nNow we inally have something that can treat contractions as whole words, instead of splitting \\nthem into tokens.\\nHow it works...\\nThe RegexpTokenizer works by compiling your pattern, then calling re.findall() on \\nyour text. You could do all this yourself using the re module, but the RegexpTokenizer \\nimplements the TokenizerI interface, just like all the word tokenizers from the previous \\nrecipe. This means it can be used by other parts of the NLTK package, such as corpus \\nreaders, which we\\'ll cover in detail in Chapter 3, Creating Custom Corpora. Many corpus \\nreaders need a way to tokenize the text they\\'re reading, and can take optional keyword \\narguments specifying an instance of a TokenizerI subclass. This way, you have the ability to \\nprovide your own tokenizer instance if the default tokenizer is unsuitable.\\nChapter 1\\n13\\nThere\\'s more...\\nRegexpTokenizer can also work by matching the gaps, instead of the tokens. Instead \\nof using re.findall(), the RegexpTokenizer will use re.split(). This is how the \\nBlanklineTokenizer in nltk.tokenize is implemented.\\nSimple whitespace tokenizer\\nHere\\'s a simple example of using the RegexpTokenizer to tokenize on whitespace:\\n>>> tokenizer = RegexpTokenizer(\\'\\\\s+\\', gaps=True)\\n>>> tokenizer.tokenize(\"Can\\'t is a contraction.\")\\n [\"Can\\'t\", \\'is\\', \\'a\\', \\'contraction.\\']\\nNotice that punctuation still remains in the tokens.\\nSee also\\nFor simpler word tokenization, see the previous recipe.\\nFiltering stopwords in a tokenized sentence\\nStopwords are common words that generally do not contribute to the meaning of a sentence, \\nat least for the purposes of information retrieval and natural language processing. Most \\nsearch engines will ilter stopwords out of search queries and documents in order to save \\nspace in their index.\\nGetting ready\\nNLTK comes with a stopwords corpus that contains word lists for many languages. Be sure to \\nunzip the dataile so NLTK can ind these word lists in nltk_data/corpora/stopwords/.\\nHow to do it...\\nWe\\'re going to create a set of all English stopwords, then use it to ilter stopwords from a \\nsentence.\\n>>> from nltk.corpus import stopwords\\n>>> english_stops = set(stopwords.words(\\'english\\'))\\n>>> words = [\"Can\\'t\", \\'is\\', \\'a\\', \\'contraction\\']\\n>>> [word for word in words if word not in english_stops]\\n[\"Can\\'t\", \\'contraction\\']\\nTokenizing Text and WordNet Basics\\n14\\nHow it works...\\nThe stopwords corpus is an instance of nltk.corpus.reader.WordListCorpusReader. \\nAs such, it has a words() method that can take a single argument for the ile ID, which in this \\ncase is \\'english\\', referring to a ile containing a list of English stopwords. You could also \\ncall stopwords.words() with no argument to get a list of all stopwords in every language \\navailable.\\nThere\\'s more...\\nYou can see the list of all English stopwords using stopwords.words(\\'english\\') or by \\nexamining the word list ile at nltk_data/corpora/stopwords/english. There are also \\nstopword lists for many other languages. You can see the complete list of languages using the \\nfileids() method:\\n>>> stopwords.fileids()\\n[\\'danish\\', \\'dutch\\', \\'english\\', \\'finnish\\', \\'french\\', \\'german\\', \\n\\'hungarian\\', \\'italian\\', \\'norwegian\\', \\'portuguese\\', \\'russian\\', \\n\\'spanish\\', \\'swedish\\', \\'turkish\\']\\nAny of these fileids can be used as an argument to the words() method to get a list of \\nstopwords for that language.\\nSee also\\nIf you\\'d like to create your own stopwords corpus, see the Creating a word list corpus recipe \\nin Chapter 3, Creating Custom Corpora, to learn how to use the WordListCorpusReader. \\nWe\\'ll also be using stopwords in the Discovering word collocations recipe, later in this chapter.\\nLooking up synsets for a word in WordNet\\nWordNet is a lexical database for the English language. In other words, it\\'s a dictionary \\ndesigned speciically for natural language processing.\\nNLTK comes with a simple interface for looking up words in WordNet. What you get is a list of \\nsynset instances, which are groupings of synonymous words that express the same concept. \\nMany words have only one synset, but some have several. We\\'ll now explore a single synset, \\nand in the next recipe, we\\'ll look at several in more detail.\\nChapter 1\\n15\\nGetting ready\\nBe sure you\\'ve unzipped the wordnet corpus in nltk_data/corpora/wordnet. This will \\nallow the WordNetCorpusReader to access it.\\nHow to do it...\\nNow we\\'re going to lookup the synset for cookbook, and explore some of the properties and \\nmethods of a synset.\\n>>> from nltk.corpus import wordnet\\n>>> syn = wordnet.synsets(\\'cookbook\\')[0]\\n>>> syn.name\\n\\'cookbook.n.01\\'\\n>>> syn.definition\\n\\'a book of recipes and cooking directions\\'\\nHow it works...\\nYou can look up any word in WordNet using wordnet.synsets(word) to get a list of \\nsynsets. The list may be empty if the word is not found. The list may also have quite a few \\nelements, as some words can have many possible meanings and therefore many synsets.\\nThere\\'s more...\\nEach synset in the list has a number of attributes you can use to learn more about it.  \\nThe name attribute will give you a unique name for the synset, which you can use to get \\nthe synset directly.\\n>>> wordnet.synset(\\'cookbook.n.01\\')\\nSynset(\\'cookbook.n.01\\')\\nThe definition attribute should be self-explanatory. Some synsets also have an examples \\nattribute, which contains a list of phrases that use the word in context.\\n>>> wordnet.synsets(\\'cooking\\')[0].examples\\n[\\'cooking can be a great art\\', \\'people are needed who have experience \\nin cookery\\', \\'he left the preparation of meals to his wife\\']\\nHypernyms\\nSynsets are organized in a kind of inheritance tree. More abstract terms are known as \\nhypernyms and more speciic terms are hyponyms. This tree can be traced all the way up \\nto a root hypernym.\\nTokenizing Text and WordNet Basics\\n16\\nHypernyms provide a way to categorize and group words based on their similarity to each \\nother. The synset similarity recipe details the functions used to calculate similarity based on \\nthe distance between two words in the hypernym tree.\\n>>> syn.hypernyms()\\n[Synset(\\'reference_book.n.01\\')]\\n>>> syn.hypernyms()[0].hyponyms()\\n[Synset(\\'encyclopedia.n.01\\'), Synset(\\'directory.n.01\\'), \\nSynset(\\'source_book.n.01\\'), Synset(\\'handbook.n.01\\'), \\nSynset(\\'instruction_book.n.01\\'), Synset(\\'cookbook.n.01\\'), \\nSynset(\\'annual.n.02\\'), Synset(\\'atlas.n.02\\'), Synset(\\'wordbook.n.01\\')]\\n>>> syn.root_hypernyms()\\n[Synset(\\'entity.n.01\\')]\\nAs you can see, reference book is a hypernym of cookbook, but cookbook is only one of \\nmany hyponyms of reference book. All these types of books have the same root hypernym, \\nentity, one of the most abstract terms in the English language. You can trace the entire \\npath from entity down to cookbook using the hypernym_paths() method.\\n>>> syn.hypernym_paths()\\n[[Synset(\\'entity.n.01\\'), Synset(\\'physical_entity.n.01\\'), \\nSynset(\\'object.n.01\\'), Synset(\\'whole.n.02\\'), Synset(\\'artifact.n.01\\'), \\nSynset(\\'creation.n.02\\'), Synset(\\'product.n.02\\'), Synset(\\'work.n.02\\'), \\nSynset(\\'publication.n.01\\'), Synset(\\'book.n.01\\'), Synset(\\'reference_\\nbook.n.01\\'), Synset(\\'cookbook.n.01\\')]]\\nThis method returns a list of lists, where each list starts at the root hypernym and ends with \\nthe original Synset. Most of the time you\\'ll only get one nested list of synsets.\\nPart-of-speech (POS)\\nYou can also look up a simpliied part-of-speech tag.\\n>>> syn.pos\\n\\'n\\'\\nThere are four common POS found in WordNet.\\nPart-of-speech\\nTag\\nNoun\\nn\\nAdjective\\na\\nAdverb\\nr\\nVerb\\nv\\nThese POS tags can be used for looking up speciic synsets for a word. For example, the \\nword great can be used as a noun or an adjective. In WordNet, great has one noun synset \\nand six adjective synsets.\\nChapter 1\\n17\\n>>> len(wordnet.synsets(\\'great\\'))\\n7\\n>>> len(wordnet.synsets(\\'great\\', pos=\\'n\\'))\\n1\\n>>> len(wordnet.synsets(\\'great\\', pos=\\'a\\'))\\n6\\nThese POS tags will be referenced more in the Using WordNet for Tagging recipe of \\nChapter 4, Part-of-Speech Tagging.\\nSee also\\nIn the next two recipes, we\\'ll explore lemmas and how to calculate synset similarity. In \\nChapter 2, Replacing and Correcting Words, we\\'ll use WordNet for lemmatization, synonym \\nreplacement, and then explore the use of antonyms.\\nLooking up lemmas and synonyms \\nin WordNet\\nBuilding on the previous recipe, we can also look up lemmas in WordNet to ind synonyms of a \\nword. A lemma (in linguistics) is the canonical form, or morphological form, of a word.\\nHow to do it...\\nIn the following block of code, we\\'ll ind that there are two lemmas for the cookbook synset \\nby using the lemmas attribute:\\n>>> from nltk.corpus import wordnet\\n>>> syn = wordnet.synsets(\\'cookbook\\')[0]\\n>>> lemmas = syn.lemmas\\n>>> len(lemmas)\\n2\\n>>> lemmas[0].name\\n\\'cookbook\\'\\n>>> lemmas[1].name\\n\\'cookery_book\\'\\n>>> lemmas[0].synset == lemmas[1].synset\\nTrue\\nTokenizing Text and WordNet Basics\\n18\\nHow it works...\\nAs you can see, cookery_book and cookbook are two distinct lemmas in the same \\nsynset. In fact, a lemma can only belong to a single synset. In this way, a synset represents \\na group of lemmas that all have the same meaning, while a lemma represents a distinct  \\nword form.\\nThere\\'s more...\\nSince lemmas in a synset all have the same meaning, they can be treated as synonyms. So if \\nyou wanted to get all synonyms for a synset, you could do:\\n>>> [lemma.name for lemma in syn.lemmas]\\n[\\'cookbook\\', \\'cookery_book\\']\\nAll possible synonyms\\nAs mentioned before, many words have multiple synsets because the word can have \\ndifferent meanings depending on the context. But let\\'s say you didn\\'t care about the context, \\nand wanted to get all possible synonyms for a word.\\n>>> synonyms = []\\n>>> for syn in wordnet.synsets(\\'book\\'):\\n...     for lemma in syn.lemmas:\\n...         synonyms.append(lemma.name)\\n>>> len(synonyms)\\n38\\nAs you can see, there appears to be 38 possible synonyms for the word book. But in fact, \\nsome are verb forms, and many are just different usages of book. Instead, if we take the set \\nof synonyms, there are fewer unique words.\\n>>> len(set(synonyms))\\n25\\nAntonyms\\nSome lemmas also have antonyms. The word good, for example, has 27 synsets, ive of \\nwhich have lemmas with antonyms.\\n>>> gn2 = wordnet.synset(\\'good.n.02\\')\\n>>> gn2.definition\\n\\'moral excellence or admirableness\\'\\n>>> evil = gn2.lemmas[0].antonyms()[0]\\n>>> evil.name\\n\\'evil\\'\\n>>> evil.synset.definition\\nChapter 1\\n19\\n\\'the quality of being morally wrong in principle or practice\\'\\n>>> ga1 = wordnet.synset(\\'good.a.01\\')\\n>>> ga1.definition\\n\\'having desirable or positive qualities especially those suitable for \\na thing specified\\'\\n>>> bad = ga1.lemmas[0].antonyms()[0]\\n>>> bad.name\\n\\'bad\\'\\n>>> bad.synset.definition\\n\\'having undesirable or negative qualities\\'\\nThe antonyms() method returns a list of lemmas. In the irst case here, we see that the \\nsecond synset for good as a noun is deined as moral excellence, and its irst antonym \\nis evil, deined as morally wrong. In the second case, when good is used as an adjective \\nto describe positive qualities, the irst antonym is bad, which describes negative qualities.\\nSee also\\nIn the next recipe, we\\'ll learn how to calculate synset similarity. Then in Chapter 2, Replacing \\nand Correcting Words, we\\'ll revisit lemmas for lemmatization, synonym replacement, and \\nantonym replacement.\\nCalculating WordNet synset similarity\\nSynsets are organized in a hypernym tree. This tree can be used for reasoning about the \\nsimilarity between the synsets it contains. Two synsets are more similar, the closer they are  \\nin the tree.\\nHow to do it...\\nIf you were to look at all the hyponyms of reference book (which is the hypernym of \\ncookbook) you\\'d see that one of them is instruction_book. These seem intuitively very \\nsimilar to cookbook, so let\\'s see what WordNet similarity has to say about it.\\n>>> from nltk.corpus import wordnet\\n>>> cb = wordnet.synset(\\'cookbook.n.01\\')\\n>>> ib = wordnet.synset(\\'instruction_book.n.01\\')\\n>>> cb.wup_similarity(ib)\\n0.91666666666666663\\nSo they are over 91% similar!\\nwww.allitebooks.com\\nTokenizing Text and WordNet Basics\\n20\\nHow it works...\\nwup_similarity is short for Wu-Palmer Similarity, which is a scoring method based on \\nhow similar the word senses are and where the synsets occur relative to each other in the \\nhypernym tree. One of the core metrics used to calculate similarity is the shortest path \\ndistance between the two synsets and their common hypernym.\\n>>> ref = cb.hypernyms()[0]\\n>>> cb.shortest_path_distance(ref)\\n1\\n>>> ib.shortest_path_distance(ref)\\n1\\n>>> cb.shortest_path_distance(ib)\\n2\\nSo cookbook and instruction book must be very similar, because they are only one step \\naway from the same hypernym, reference book, and therefore only two steps away from \\neach other.\\nThere\\'s more...\\nLet\\'s look at two dissimilar words to see what kind of score we get. We\\'ll compare dog with \\ncookbook, two seemingly very different words.\\n>>> dog = wordnet.synsets(\\'dog\\')[0]\\n>>> dog.wup_similarity(cb)\\n0.38095238095238093\\nWow, dog and cookbook are apparently 38% similar! This is because they share common \\nhypernyms farther up the tree.\\n>>> dog.common_hypernyms(cb)\\n[Synset(\\'object.n.01\\'), Synset(\\'whole.n.02\\'), Synset(\\'physical_\\nentity.n.01\\'), Synset(\\'entity.n.01\\')]\\nComparing verbs\\nThe previous comparisons were all between nouns, but the same can be done for verbs  \\nas well.\\n>>> cook = wordnet.synset(\\'cook.v.01\\')\\n>>> bake = wordnet.synset(\\'bake.v.02\\')\\n>>> cook.wup_similarity(bake)\\n0.75\\nChapter 1\\n21\\nThe previous synsets were obviously handpicked for demonstration, and the reason is that \\nthe hypernym tree for verbs has a lot more breadth and a lot less depth. While most nouns \\ncan be traced up to object, thereby providing a basis for similarity, many verbs do not share \\ncommon hypernyms, making WordNet unable to calculate similarity. For example, if you were \\nto use the synset for bake.v.01 here, instead of bake.v.02, the return value would be \\nNone. This is because the root hypernyms of the two synsets are different, with no overlapping \\npaths. For this reason, you also cannot calculate similarity between words with different parts \\nof speech.\\nPath and LCH similarity\\nTwo other similarity comparisons are the path similarity and Leacock Chodorow (LCH) \\nsimilarity.\\n>>> cb.path_similarity(ib)\\n0.33333333333333331\\n>>> cb.path_similarity(dog)\\n0.071428571428571425\\n>>> cb.lch_similarity(ib)\\n2.5389738710582761\\n>>> cb.lch_similarity(dog)\\n0.99852883011112725\\nAs you can see, the number ranges are very different for these scoring methods, which is why \\nwe prefer the wup_similarity() method.\\nSee also\\nThe recipe on Looking up synsets for a word in WordNet, discussed earlier in this chapter, has \\nmore details about hypernyms and the hypernym tree.\\nDiscovering word collocations\\nCollocations are two or more words that tend to appear frequently together, such as \"United \\nStates\". Of course, there are many other words that can come after \"United\", for example \\n\"United Kingdom\", \"United Airlines\", and so on. As with many aspects of natural language \\nprocessing, context is very important, and for collocations, context is everything!\\nIn the case of collocations, the context will be a document in the form of a list of words. \\nDiscovering collocations in this list of words means that we\\'ll ind common phrases that occur \\nfrequently throughout the text. For fun, we\\'ll start with the script for Monty Python and the \\nHoly Grail.\\nTokenizing Text and WordNet Basics\\n22\\nGetting ready\\nThe script for Monty Python and the Holy Grail is found in the webtext corpus, so be sure \\nthat it\\'s unzipped in nltk_data/corpora/webtext/.\\nHow to do it...\\nWe\\'re going to create a list of all lowercased words in the text, and then produce a \\nBigramCollocationFinder, which we can use to ind bigrams, which are pairs of words. \\nThese bigrams are found using association measurement functions found in the nltk.\\nmetrics package.\\n>>> from nltk.corpus import webtext\\n>>> from nltk.collocations import BigramCollocationFinder\\n>>> from nltk.metrics import BigramAssocMeasures\\n>>> words = [w.lower() for w in webtext.words(\\'grail.txt\\')]\\n>>> bcf = BigramCollocationFinder.from_words(words)\\n>>> bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\\n[(\"\\'\", \\'s\\'), (\\'arthur\\', \\':\\'), (\\'#\\', \\'1\\'), (\"\\'\", \\'t\\')]\\nWell that\\'s not very useful! Let\\'s reine it a bit by adding a word ilter to remove punctuation \\nand stopwords.\\n>>> from nltk.corpus import stopwords\\n>>> stopset = set(stopwords.words(\\'english\\'))\\n>>> filter_stops = lambda w: len(w) < 3 or w in stopset\\n>>> bcf.apply_word_filter(filter_stops)\\n>>> bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\\n[(\\'black\\', \\'knight\\'), (\\'clop\\', \\'clop\\'), (\\'head\\', \\'knight\\'), (\\'mumble\\', \\n\\'mumble\\')]\\nMuch better—we can clearly see four of the most common bigrams in Monty Python and the \\nHoly Grail. If you\\'d like to see more than four, simply increase the number to whatever you \\nwant, and the collocation inder will do its best.\\nHow it works...\\nThe BigramCollocationFinder constructs two frequency distributions: one for each \\nword, and another for bigrams. A frequency distribution, or FreqDist in NLTK, is basically \\nan enhanced dictionary where the keys are what\\'s being counted, and the values are the \\ncounts. Any iltering functions that are applied, reduce the size of these two FreqDists by \\neliminating any words that don\\'t pass the ilter. By using a iltering function to eliminate all \\nwords that are one or two characters, and all English stopwords, we can get a much cleaner \\nresult. After iltering, the collocation inder is ready to accept a generic scoring function for \\ninding collocations. Additional scoring functions are covered in the Scoring functions section \\nfurther in this chapter.\\nChapter 1\\n23\\nThere\\'s more...\\nIn addition to BigramCollocationFinder, there\\'s also TrigramCollocationFinder, \\nfor inding triples instead of pairs. This time, we\\'ll look for trigrams in Australian singles ads.\\n>>> from nltk.collocations import TrigramCollocationFinder\\n>>> from nltk.metrics import TrigramAssocMeasures\\n>>> words = [w.lower() for w in webtext.words(\\'singles.txt\\')]\\n>>> tcf = TrigramCollocationFinder.from_words(words)\\n>>> tcf.apply_word_filter(filter_stops)\\n>>> tcf.apply_freq_filter(3)\\n>>> tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)\\n[(\\'long\\', \\'term\\', \\'relationship\\')]\\nNow, we don\\'t know whether people are looking for a long-term relationship or not, but clearly \\nit\\'s an important topic. In addition to the stopword ilter, we also applied a frequency ilter \\nwhich removed any trigrams that occurred less than three times. This is why only one result \\nwas returned when we asked for four—because there was only one result that occurred more \\nthan twice.\\nScoring functions\\nThere are many more scoring functions available besides likelihood_ratio(). But other \\nthan raw_freq(), you may need a bit of a statistics background to understand how they \\nwork. Consult the NLTK API documentation for NgramAssocMeasures in the nltk.metrics \\npackage, to see all the possible scoring functions.\\nScoring ngrams\\nIn addition to the nbest() method, there are two other ways to get ngrams (a generic term \\nfor describing bigrams and trigrams) from a collocation inder.\\n1. above_score(score_fn, min_score) can be used to get all ngrams with scores \\nthat are at least min_score. The min_score that you choose will depend heavily on \\nthe score_fn you use.\\n2. score_ngrams(score_fn) will return a list with tuple pairs of (ngram, score). \\nThis can be used to inform your choice for min_score in the previous step.\\nSee also\\nThe nltk.metrics module will be used again in Chapter 7, Text Classiication.\\n2\\nReplacing and \\nCorrecting Words\\nIn this chapter, we will cover:\\n \\nf\\nStemming words\\n \\nf\\nLemmatizing words with WordNet\\n \\nf\\nTranslating text with Babelish\\n \\nf\\nReplacing words matching regular expressions\\n \\nf\\nRemoving repeating characters\\n \\nf\\nSpelling correction with Enchant\\n \\nf\\nReplacing synonyms\\n \\nf\\nReplacing negations with antonyms\\nIntroduction\\nIn this chapter, we will go over various word replacement and correction techniques. The \\nrecipes cover the gamut of linguistic compression, spelling correction, and text normalization. \\nAll of these methods can be very useful for pre-processing text before search indexing, \\ndocument classiication, and text analysis.\\nStemming words\\nStemming is a technique for removing afixes from a word, ending up with the stem. For \\nexample, the stem of \"cooking\" is \"cook\", and a good stemming algorithm knows that the \\n\"ing\" sufix can be removed. Stemming is most commonly used by search engines for indexing \\nwords. Instead of storing all forms of a word, a search engine can store only the stems, greatly \\nreducing the size of index while increasing retrieval accuracy.\\nReplacing and Correcting Words\\n26\\nOne of the most common stemming algorithms is the Porter Stemming Algorithm, by Martin \\nPorter. It is designed to remove and replace well known sufixes of English words, and its \\nusage in NLTK will be covered next.\\nThe resulting stem is not always a valid word. For example, the \\nstem of \"cookery\" is \"cookeri\". This is a feature, not a bug.\\nHow to do it...\\nNLTK comes with an implementation of the Porter Stemming Algorithm, which is very easy  \\nto use. Simply instantiate the PorterStemmer class and call the stem() method with the \\nword you want to stem.\\n>>> from nltk.stem import PorterStemmer\\n>>> stemmer = PorterStemmer()\\n>>> stemmer.stem(\\'cooking\\')\\n\\'cook\\'\\n>>> stemmer.stem(\\'cookery\\')\\n\\'cookeri\\'\\nHow it works...\\nThe PorterStemmer knows a number of regular word forms and sufixes, and uses \\nthat knowledge to transform your input word to a inal stem through a series of steps. The \\nresulting stem is often a shorter word, or at least a common form of the word, that has the \\nsame root meaning.\\nThere\\'s more...\\nThere are other stemming algorithms out there besides the Porter Stemming Algorithm, such \\nas the Lancaster Stemming Algorithm, developed at Lancaster University. NLTK includes \\nit as the LancasterStemmer class. At the time of writing, there is no deinitive research \\ndemonstrating the superiority of one algorithm over the other. However, Porter Stemming  \\nis generally the default choice.\\nChapter 2\\n27\\nAll the stemmers covered next inherit from the StemmerI interface, which deines the \\nstem() method. The following is an inheritance diagram showing this:\\nLancasterStemmer\\nThe LancasterStemmer functions just like the PorterStemmer, but can produce slightly \\ndifferent results. It is known to be slightly more aggressive than the PorterStemmer.\\n>>> from nltk.stem import LancasterStemmer\\n>>> stemmer = LancasterStemmer()\\n>>> stemmer.stem(\\'cooking\\')\\n\\'cook\\'\\n>>> stemmer.stem(\\'cookery\\')\\n\\'cookery\\'\\nRegexpStemmer\\nYou can also construct your own stemmer using the RegexpStemmer. It takes a single regular \\nexpression (either compiled or as a string) and will remove any preix or sufix that matches.\\n>>> from nltk.stem import RegexpStemmer\\n>>> stemmer = RegexpStemmer(\\'ing\\')\\n>>> stemmer.stem(\\'cooking\\')\\n\\'cook\\'\\n>>> stemmer.stem(\\'cookery\\')\\n\\'cookery\\'\\n>>> stemmer.stem(\\'ingleside\\')\\n\\'leside\\'\\nA RegexpStemmer should only be used in very speciic cases that are not covered by the \\nPorterStemmer or LancasterStemmer.\\nReplacing and Correcting Words\\n28\\nSnowballStemmer\\nNew in NLTK 2.0b9 is the SnowballStemmer, which supports 13 non-English languages. \\nTo use it, you create an instance with the name of the language you are using, and then call \\nthe stem() method. Here is a list of all the supported languages, and an example using the \\nSpanish SnowballStemmer:\\n>>> from nltk.stem import SnowballStemmer\\n>>> SnowballStemmer.languages\\n(\\'danish\\', \\'dutch\\', \\'finnish\\', \\'french\\', \\'german\\', \\'hungarian\\', \\n\\'italian\\', \\'norwegian\\', \\'portuguese\\', \\'romanian\\', \\'russian\\', \\n\\'spanish\\', \\'swedish\\')\\n>>> spanish_stemmer = SnowballStemmer(\\'spanish\\')\\n>>> spanish_stemmer.stem(\\'hola\\')\\nu\\'hol\\'\\nSee also\\nIn the next recipe, we will cover lemmatization, which is quite similar to stemming, but  \\nsubtly different.\\nLemmatizing words with WordNet\\nLemmatization is very similar to stemming, but is more akin to synonym replacement. A \\nlemma is a root word, as opposed to the root stem. So unlike stemming, you are always \\nleft with a valid word which means the same thing. But the word you end up with can be \\ncompletely different. A few examples will explain lemmatization...\\nGetting ready\\nBe sure you have unzipped the wordnet corpus in nltk_data/corpora/wordnet. This will \\nallow the WordNetLemmatizer to access WordNet. You should also be somewhat familiar \\nwith the part-of-speech tags covered in the Looking up synsets for a word in WordNet recipe of \\nChapter 1, Tokenizing Text and WordNet Basics.\\nHow to do it...\\nWe will use the WordNetLemmatizer to ind lemmas:\\n>>> from nltk.stem import WordNetLemmatizer\\n>>> lemmatizer = WordNetLemmatizer()\\n>>> lemmatizer.lemmatize(\\'cooking\\')\\n\\'cooking\\'\\nChapter 2\\n29\\n>>> lemmatizer.lemmatize(\\'cooking\\', pos=\\'v\\')\\n\\'cook\\'\\n>>> lemmatizer.lemmatize(\\'cookbooks\\')\\n\\'cookbook\\'\\nHow it works...\\nThe WordNetLemmatizer is a thin wrapper around the WordNet corpus, and uses the \\nmorphy() function of the WordNetCorpusReader to ind a lemma. If no lemma is found, \\nthe word is returned as it is. Unlike with stemming, knowing the part of speech of the word is \\nimportant. As demonstrated previously, \"cooking\" does not have a lemma unless you specify \\nthat the part of speech (pos) is a verb. This is because the default part of speech is a noun, \\nand since \"cooking\" is not a noun, no lemma is found. \"Cookbooks\", on the other hand, is a \\nnoun, and its lemma is the singular form, \"cookbook\".\\nThere\\'s more...\\nHere\\'s an example that illustrates one of the major differences between stemming  \\nand lemmatization:\\n>>> from nltk.stem import PorterStemmer\\n>>> stemmer = PorterStemmer()\\n>>> stemmer.stem(\\'believes\\')\\n\\'believ\\'\\n>>> lemmatizer.lemmatize(\\'believes\\')\\n\\'belief\\'\\nInstead of just chopping off the \"es\" like the PorterStemmer, the WordNetLemmatizer \\ninds a valid root word. Where a stemmer only looks at the form of the word, the lemmatizer \\nlooks at the meaning of the word. And by returning a lemma, you will always get a valid word.\\nCombining stemming with lemmatization\\nStemming and lemmatization can be combined to compress words more than either process \\ncan by itself. These cases are somewhat rare, but they do exist:\\n>>> stemmer.stem(\\'buses\\')\\n\\'buse\\'\\n>>> lemmatizer.lemmatize(\\'buses\\')\\n\\'bus\\'\\n>>> stemmer.stem(\\'bus\\')\\n\\'bu\\'\\nwww.allitebooks.com\\nReplacing and Correcting Words\\n30\\nIn this example, stemming saves one character, lemmatizing saves two characters, and \\nstemming the lemma saves a total of three characters out of ive characters. That is nearly a \\n60% compression rate! This level of word compression over many thousands of words, while \\nunlikely to always produce such high gains, can still make a huge difference.\\nSee also\\nIn the previous recipe, we covered stemming basics and WordNet was introduced in the \\nLooking up synsets for a word in WordNet and Looking up lemmas and synonyms in WordNet \\nrecipes of Chapter 1, Tokenizing Text and WordNet Basics. Looking forward, we will cover the \\nUsing WordNet for Tagging recipe in Chapter 4, Part-of-Speech Tagging.\\nTranslating text with Babelish\\nBabelish is an online language translation API provided by Yahoo. With it, you can translate \\ntext in a source language to a target language. NLTK comes with a simple interface for \\nusing it.\\nGetting ready\\nBe sure you are connected to the internet irst. The babelfish.translate() function \\nrequires access to Yahoo\\'s online API in order to work.\\nHow to do it...\\nTo translate your text, you irst need to know two things:\\n1. The language of your text or source language.\\n2. The language you want to translate to or target language.\\nLanguage detection is outside the scope of this recipe, so we will assume you already know \\nthe source and target languages.\\n>>> from nltk.misc import babelfish\\n>>> babelfish.translate(\\'cookbook\\', \\'english\\', \\'spanish\\')\\n\\'libro de cocina\\'\\n>>> babelfish.translate(\\'libro de cocina\\', \\'spanish\\', \\'english\\')\\n\\'kitchen book\\'\\n>>> babelfish.translate(\\'cookbook\\', \\'english\\', \\'german\\')\\n\\'Kochbuch\\'\\n>>> babelfish.translate(\\'kochbuch\\', \\'german\\', \\'english\\')\\n\\'cook book\\'\\nChapter 2\\n31\\nYou cannot translate using the same language for both source and target. \\nAttempting to do so will raise a BabelfishChangedError.\\nHow it works...\\nThe translate() function is a small function that sends a urllib request to \\nhttp://babelfish.yahoo.com/translate_txt, and then searches the \\nresponse for the translated text.\\nIf Yahoo, for whatever reason, had changed their HTML response \\nto the point that translate() cannot identify the translated \\ntext, a BabelfishChangedError will be raised. This is unlikely \\nto happen, but if it does, you may need to upgrade to a newer \\nversion of NLTK and/or report the error.\\nThere\\'s more...\\nThere is also a fun function called babelize() that translates back and forth between the \\nsource and target language until there are no more changes.\\n>>> for text in babelfish.babelize(\\'cookbook\\', \\'english\\', \\'spanish\\'):\\n...  print text\\ncookbook\\nlibro de cocina\\nkitchen book\\nlibro de la cocina\\nbook of the kitchen\\nAvailable languages\\nYou can see all the languages available for translation by examining the available_\\nlanguages attribute.\\n>>> babelfish.available_languages\\n[\\'Portuguese\\', \\'Chinese\\', \\'German\\', \\'Japanese\\', \\'French\\', \\'Spanish\\', \\n\\'Russian\\', \\'Greek\\', \\'English\\', \\'Korean\\', \\'Italian\\']\\nThe lowercased version of each of these languages can be used as a source or target \\nlanguage for translation.\\nReplacing and Correcting Words\\n32\\nReplacing words matching regular  \\nexpressions\\nNow we are going to get into the process of replacing words. Where stemming and \\nlemmatization are a kind of linguistic compression, and word replacement can be thought \\nof as error correction, or text normalization.\\nFor this recipe, we will be replacing words based on regular expressions, with a focus on \\nexpanding contractions. Remember when we were tokenizing words in Chapter 1, Tokenizing \\nText and WordNet Basics and it was clear that most tokenizers had trouble with contractions? \\nThis recipe aims to ix that by replacing contractions with their expanded forms, such as by \\nreplacing \"can\\'t\" with \"cannot\", or \"would\\'ve\" with \"would have\".\\nGetting ready\\nUnderstanding how this recipe works will require a basic knowledge of regular expressions and \\nthe re module. The key things to know are matching patterns and the re.subn() function.\\nHow to do it...\\nFirst, we need to deine a number of replacement patterns. This will be a list of tuple pairs, \\nwhere the irst element is the pattern to match on, and the second element is the replacement.\\nNext, we will create a RegexpReplacer class that will compile the patterns, and provide a \\nreplace() method to substitute all found patterns with their replacements.\\nThe following code can be found in the replacers.py module and is meant to be imported, \\nnot typed into the console:\\nimport re\\nreplacement_patterns = [\\n  (r\\'won\\\\\\'t\\', \\'will not\\'),\\n  (r\\'can\\\\\\'t\\', \\'cannot\\'),\\n  (r\\'i\\\\\\'m\\', \\'i am\\'),\\n  (r\\'ain\\\\\\'t\\', \\'is not\\'),\\n  (r\\'(\\\\w+)\\\\\\'ll\\', \\'\\\\g<1> will\\'),\\n  (r\\'(\\\\w+)n\\\\\\'t\\', \\'\\\\g<1> not\\'),\\n  (r\\'(\\\\w+)\\\\\\'ve\\', \\'\\\\g<1> have\\'),\\n  (r\\'(\\\\w+)\\\\\\'s\\', \\'\\\\g<1> is\\'),\\n  (r\\'(\\\\w+)\\\\\\'re\\', \\'\\\\g<1> are\\'),\\n  (r\\'(\\\\w+)\\\\\\'d\\', \\'\\\\g<1> would\\')\\n]\\nclass RegexpReplacer(object):\\nChapter 2\\n33\\n  def __init__(self, patterns=replacement_patterns):\\n    self.patterns = [(re.compile(regex), repl) for (regex, repl) in  \\n      patterns]\\n  def replace(self, text):\\n    s = text\\n    for (pattern, repl) in self.patterns:\\n      (s, count) = re.subn(pattern, repl, s)\\n    return s\\nHow it works...\\nHere is a simple usage example:\\n>>> from replacers import RegexpReplacer\\n>>> replacer = RegexpReplacer()\\n>>> replacer.replace(\"can\\'t is a contraction\")\\n\\'cannot is a contraction\\'\\n>>> replacer.replace(\"I should\\'ve done that thing I didn\\'t do\")\\n\\'I should have done that thing I did not do\\'\\nRegexpReplacer.replace() works by replacing every instance of a replacement pattern \\nwith its corresponding substitution pattern. In replacement_patterns, we have deined \\ntuples such as (r\\'(\\\\w+)\\\\\\'ve\\', \\'\\\\g<1> have\\'). The irst element matches a group of \\nASCII characters followed by \\'ve. By grouping the characters before the \\'ve in parenthesis, \\na match group is found and can be used in the substitution pattern with the \\\\g<1> reference. \\nSo we keep everything before \\'ve, then replace \\'ve with the word have. This is how \\n\"should\\'ve\" can become \"should have\".\\nThere\\'s more...\\nThis replacement technique can work with any kind of regular expression, not just \\ncontractions. So you could replace any occurrence of \"&\" with \"and\", or eliminate all \\noccurrences of \"-\" by replacing it with the empty string. The RegexpReplacer can \\ntake any list of replacement patterns for whatever purpose.\\nReplacement before tokenization\\nLet us try using the RegexpReplacer as a preliminary step before tokenization:\\n>>> from nltk.tokenize import word_tokenize\\n>>> from replacers import RegexpReplacer\\n>>> replacer = RegexpReplacer()\\n>>> word_tokenize(\"can\\'t is a contraction\")\\n[\\'ca\\', \"n\\'t\", \\'is\\', \\'a\\', \\'contraction\\']\\n>>> word_tokenize(replacer.replace(\"can\\'t is a contraction\"))\\n[\\'can\\', \\'not\\', \\'is\\', \\'a\\', \\'contraction\\']\\nReplacing and Correcting Words\\n34\\nMuch better! By eliminating the contractions in the irst place, the tokenizer will produce \\ncleaner results. Cleaning up text before processing is a common pattern in natural  \\nlanguage processing.\\nSee also\\nFor more information on tokenization, see the irst three recipes in Chapter 1, Tokenizing \\nText and WordNet Basics. For more replacement techniques, continue reading the rest of \\nthis chapter.\\nRemoving repeating characters\\nIn everyday language, people are often not strictly grammatical. They will write things like \\n\"I looooooove it\" in order to emphasize the word \"love\". But computers don\\'t know that \\n\"looooooove\" is a variation of \"love\" unless they are told. This recipe presents a method for \\nremoving those annoying repeating characters in order to end up with a \"proper\" English word.\\nGetting ready\\nAs in the previous recipe, we will be making use of the re module, and more speciically, \\nbackreferences. A backreference is a way to refer to a previously matched group in a regular \\nexpression. This is what will allow us to match and remove repeating characters.\\nHow to do it...\\nWe will create a class that has the same form as the RegexpReplacer from the previous \\nrecipe. It will have a replace() method that takes a single word and returns a more correct \\nversion of that word, with dubious repeating characters removed. The following code can be \\nfound in replacers.py and is meant to be imported:\\nimport re\\nclass RepeatReplacer(object):\\n  def __init__(self):\\n    self.repeat_regexp = re.compile(r\\'(\\\\w*)(\\\\w)\\\\2(\\\\w*)\\')\\n    self.repl = r\\'\\\\1\\\\2\\\\3\\'\\n  def replace(self, word):\\n    repl_word = self.repeat_regexp.sub(self.repl, word)\\n    if repl_word != word:\\n      return self.replace(repl_word)\\n    else:\\n      return repl_word\\nChapter 2\\n35\\nAnd now some example use cases:\\n>>> from replacers import RepeatReplacer\\n>>> replacer = RepeatReplacer()\\n>>> replacer.replace(\\'looooove\\')\\n\\'love\\'\\n>>> replacer.replace(\\'oooooh\\')\\n\\'oh\\'\\n>>> replacer.replace(\\'goose\\')\\n\\'gose\\'\\nHow it works...\\nRepeatReplacer starts by compiling a regular expression for matching and deining a \\nreplacement string with backreferences. The repeat_regexp matches three groups:\\n1. Zero or more starting characters (\\\\w*).\\n2. A single character (\\\\w), followed by another instance of that character \\\\2.\\n3. Zero or more ending characters (\\\\w*).\\nThe replacement string is then used to keep all the matched groups, while discarding the \\nbackreference to the second group. So the word \"looooove\" gets split into (l)(o)o(ooove) \\nand then recombined as \"loooove\", discarding the second \"o\". This continues until only one \"o\" \\nremains, when repeat_regexp no longer matches the string, and no more characters \\nare removed.\\nThere\\'s more...\\nIn the preceding examples, you can see that the RepeatReplacer is a bit too greedy and \\nends up changing \"goose\" into \"gose\". To correct this issue, we can augment the replace() \\nfunction with a WordNet lookup. If WordNet recognizes the word, then we can stop replacing \\ncharacters. Here is the WordNet augmented version:\\nimport re\\nfrom nltk.corpus import wordnet\\nclass RepeatReplacer(object):\\n  def __init__(self):\\n    self.repeat_regexp = re.compile(r\\'(\\\\w*)(\\\\w)\\\\2(\\\\w*)\\')\\n    self.repl = r\\'\\\\1\\\\2\\\\3\\'\\n  def replace(self, word):\\n    if wordnet.synsets(word):\\n      return word\\n    repl_word = self.repeat_regexp.sub(self.repl, word)\\n    if repl_word != word:\\n      return self.replace(repl_word)\\n    else:\\n      return repl_word\\nReplacing and Correcting Words\\n36\\nNow, \"goose\" will be found in WordNet, and no character replacement will take place. And \\n\"oooooh\" will become \"ooh\" instead of \"oh\", because \"ooh\" is actually a word in WordNet, \\ndeined as an expression of admiration or pleasure.\\nSee also\\nRead the next recipe to learn how to correct misspellings. And for more on WordNet, refer to \\nthe WordNet recipes in Chapter 1, Tokenizing Text and WordNet Basics. We will also be using \\nWordNet for antonym replacement later in this chapter.\\nSpelling correction with Enchant\\nReplacing repeating characters is actually an extreme form of spelling correction. In this \\nrecipe, we will take on the less extreme case of correcting minor spelling issues using \\nEnchant—a spelling correction API.\\nGetting ready\\nYou will need to install Enchant, and a dictionary for it to use. Enchant is an offshoot  \\nof the \"Abiword\" open source word processor, and more information can be found at  \\nhttp://www.abisource.com/projects/enchant/.\\nFor dictionaries, aspell is a good open source spellchecker and dictionary that can be found \\nat http://aspell.net/.\\nFinally, you will need the pyenchant library, which can be found at http://www.rfk.id.au/\\nsoftware/pyenchant/. You should be able to install it with the easy_install command \\nthat comes with python-setuptools, such as by doing sudo easy_install pyenchant \\non Linux or Unix.\\nHow to do it...\\nWe will create a new class called SpellingReplacer in replacers.py, and this time \\nthe replace() method will check Enchant to see whether the word is valid or not. If not, we \\nwill look up suggested alternatives and return the best match using nltk.metrics.edit_\\ndistance():\\nimport enchant\\nfrom nltk.metrics import edit_distance\\nclass SpellingReplacer(object):\\n  def __init__(self, dict_name=\\'en\\', max_dist=2):\\n    self.spell_dict = enchant.Dict(dict_name)\\n    self.max_dist = 2\\nChapter 2\\n37\\n  def replace(self, word):\\n    if self.spell_dict.check(word):\\n      return word\\n    suggestions = self.spell_dict.suggest(word)\\n    if suggestions and edit_distance(word, suggestions[0]) <=  \\n      self.max_dist:\\n      return suggestions[0]\\n    else:\\n      return word\\nThe preceding class can be used to correct English spellings as follows:\\n>>> from replacers import SpellingReplacer\\n>>> replacer = SpellingReplacer()\\n>>> replacer.replace(\\'cookbok\\')\\n\\'cookbook\\'\\nHow it works...\\nSpellingReplacer starts by creating a reference to an enchant dictionary. Then, in the \\nreplace() method, it irst checks whether the given word is present in the dictionary or \\nnot. If it is, no spelling correction is necessary, and the word is returned. But if the word is \\nnot found, it looks up a list of suggestions and returns the irst suggestion, as long as its edit \\ndistance is less than or equal to max_dist. The edit distance is the number of character \\nchanges necessary to transform the given word into the suggested word. max_dist then acts \\nas a constraint on the Enchant suggest() function to ensure that no unlikely replacement \\nwords are returned. Here is an example showing all the suggestions for \"languege\", a \\nmisspelling of \"language\":\\n>>> import enchant\\n>>> d = enchant.Dict(\\'en\\')\\n>>> d.suggest(\\'languege\\')\\n[\\'language\\', \\'languisher\\', \\'languish\\', \\'languor\\', \\'languid\\']\\nExcept for the correct suggestion, \"language\", all the other words have an edit distance of \\nthree or greater.\\nThere\\'s more...\\nYou can use language dictionaries other than \\'en\\', such as \\'en_GB\\', assuming the \\ndictionary has already been installed. To check which other languages are available, use \\nenchant.list_languages():\\n>>> enchant.list_languages()\\n[\\'en_AU\\', \\'en_GB\\', \\'en_US\\', \\'en_ZA\\', \\'en_CA\\', \\'en\\']\\nReplacing and Correcting Words\\n38\\nIf you try to use a dictionary that doesn\\'t exist, you will get enchant.\\nDictNotFoundError. You can irst check whether the dictionary exists \\nusing enchant.dict_exists(), which will return True if the named \\ndictionary exists, or False otherwise.\\nen_GB dictionary\\nAlways be sure to use the correct dictionary for whichever language you are doing spelling \\ncorrection on. \\'en_US\\' can give you different results than \\'en_GB\\', such as for the word \\n\"theater\". \"Theater\" is the American English spelling, whereas the British English spelling  \\nis \"Theatre\":\\n>>> import enchant\\n>>> dUS = enchant.Dict(\\'en_US\\')\\n>>> dUS.check(\\'theater\\')\\nTrue\\n>>> dGB = enchant.Dict(\\'en_GB\\')\\n>>> dGB.check(\\'theater\\')\\nFalse\\n>>> from replacers import SpellingReplacer\\n>>> us_replacer = SpellingReplacer(\\'en_US\\')\\n>>> us_replacer.replace(\\'theater\\')\\n\\'theater\\'\\n>>> gb_replacer = SpellingReplacer(\\'en_GB\\')\\n>>> gb_replacer.replace(\\'theater\\')\\n\\'theatre\\'\\nPersonal word lists\\nEnchant also supports personal word lists. These can be combined with an existing \\ndictionary, allowing you to augment the dictionary with your own words. So let us say you had \\na ile named mywords.txt that had nltk on one line. You could then create a dictionary \\naugmented with your personal word list as follows:\\n>>> d = enchant.Dict(\\'en_US\\')\\n>>> d.check(\\'nltk\\')\\nFalse\\n>>> d = enchant.DictWithPWL(\\'en_US\\', \\'mywords.txt\\')\\n>>> d.check(\\'nltk\\')\\nTrue'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3:Text Preprocessing"
      ],
      "metadata": {
        "id": "0cLb2w-oa8cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove numbers, punctuations, and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    # Join tokens back to a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "cleaned_text = preprocess_text(text)\n",
        "print(cleaned_text[:500])  # Display first 500 characters of cleaned text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BGoG2M1Ye1o",
        "outputId": "98350be9-b0e5-494f-b32f-880adce69f4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "introduction nltk natural language toolkit comprehensive python library natural language processing text analytics originally designed teaching adopted industry research development due usefulness breadth coverage chapter cover basics tokenizing text using wordnet tokenization method breaking piece text many pieces essential irst step recipes later chapters tokenizing text wordnet basics wordnet dictionary designed programmatic access natural language processing systems nltk includes wordnet cor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Text Segmentation and Vectorization"
      ],
      "metadata": {
        "id": "HSZ1piL8bDQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Tokenization and vectorization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for i in range(1, len(cleaned_text.split())):\n",
        "    n_gram_sequence = cleaned_text.split()[:i+1]\n",
        "    token_list = tokenizer.texts_to_sequences([n_gram_sequence])[0]\n",
        "    input_sequences.append(token_list)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Create predictors and label (Shifted sequence for LSTM prediction)\n",
        "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
      ],
      "metadata": {
        "id": "QW0gVZnqYeyG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step5 Model Selection LSTM"
      ],
      "metadata": {
        "id": "HaEVIIEZbOQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Model definition\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len - 1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y, epochs=10, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKE6Y8FeYev_",
        "outputId": "61d0aae9-e330-4ea3-cb39-8a43e9ef7454"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 140ms/step - accuracy: 0.0145 - loss: 7.0156\n",
            "Epoch 2/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 133ms/step - accuracy: 0.0187 - loss: 6.5896\n",
            "Epoch 3/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 136ms/step - accuracy: 0.0268 - loss: 6.3870\n",
            "Epoch 4/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 134ms/step - accuracy: 0.0294 - loss: 6.2071\n",
            "Epoch 5/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 136ms/step - accuracy: 0.0373 - loss: 5.9584\n",
            "Epoch 6/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 138ms/step - accuracy: 0.0489 - loss: 5.6494\n",
            "Epoch 7/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 141ms/step - accuracy: 0.0680 - loss: 5.3075\n",
            "Epoch 8/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 144ms/step - accuracy: 0.0935 - loss: 4.9117\n",
            "Epoch 9/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 140ms/step - accuracy: 0.1255 - loss: 4.5747\n",
            "Epoch 10/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 140ms/step - accuracy: 0.1804 - loss: 4.1495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-6: Building QA Functionality"
      ],
      "metadata": {
        "id": "sZSJJMvRbYSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate_answer(seed_text, next_words):\n",
        "    for _ in range(next_words):\n",
        "        # Convert the seed text to sequences\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        # Pad the sequences\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "\n",
        "        # Predict the next word probabilities\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "\n",
        "        # Get the index of the highest probability\n",
        "        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n",
        "\n",
        "        # Find the word corresponding to the predicted index\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the seed text\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Example question\n",
        "question = \"What is the \"\n",
        "answer = generate_answer(question, 20)  # Generate 20 words as an answer\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B5gTocZtPOU",
        "outputId": "7200a049-0dc7-42c4-8959-92d69ec13aa6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: What is the  language language language language language language language language language language language language language language language language language language language language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Build the User Interface (UI)"
      ],
      "metadata": {
        "id": "-oaz3QeqbmQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Flask and Flask-Ngrok\n",
        "!pip install flask-ngrok\n",
        "\n",
        "from flask import Flask, render_template, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from threading import Thread\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  # Starts the app on Colab\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return '''\n",
        "        <html>\n",
        "            <body>\n",
        "                <h1>Chatbot</h1>\n",
        "                <input id=\"user_input\" type=\"text\" placeholder=\"Type your message here...\">\n",
        "                <button onclick=\"sendMessage()\">Send</button>\n",
        "                <div id=\"response\"></div>\n",
        "\n",
        "                <script>\n",
        "                    function sendMessage() {\n",
        "                        var userText = document.getElementById(\"user_input\").value;\n",
        "                        fetch(\"/get?msg=\" + userText)\n",
        "                            .then(response => response.text())\n",
        "                            .then(data => {\n",
        "                                document.getElementById(\"response\").innerText = data;\n",
        "                            });\n",
        "                    }\n",
        "                </script>\n",
        "            </body>\n",
        "        </html>\n",
        "    '''\n",
        "\n",
        "@app.route('/get')\n",
        "def get_bot_response():\n",
        "    userText = request.args.get('msg')\n",
        "    answer = generate_answer(userText, 10)  # Generate a response (ensure generate_answer is defined)\n",
        "    return str(answer)\n",
        "# Run the Flask app in a separate thread\n",
        "def run():\n",
        "    app.run()\n",
        "\n",
        "Thread(target=run).start()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE0WDsPUt8nD",
        "outputId": "0b3141db-5d5d-4c9f-9411-0e2a64f36376"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.5)\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}